<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.MA](#cs.MA) [Total: 6]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: 本文提出了Multiverse，一种同时支持无版本与多版本事务的软件事务内存（STM）系统，可显著提升包含长时运行读操作场景下的STM性能。


<details>
  <summary>Details</summary>
Motivation: 当前的软件事务内存（STM）虽然简化了并发数据结构的开发，并在某些情况下性能良好，但难以支持长时运行且频繁读取大量数据的事务，尤其是这些数据地址被频繁更新的场景。多版本化可以应对此类负载，但会导致不必要的开销，影响其他事务性能。

Method: 提出新STM系统Multiverse，支持无版本事务和多版本事务并发执行，使无版本事务保有高性能同时，能有效支持长时读取需求；通过实验与现有多种STM系统对比，论证了性能优势。

Result: Multiverse在常规负载（无长运行读事务）下性能与当前最优STM相当或更优；在包含长运行读和频繁更新的数据集上，多数情况下Multiverse吞吐量比其他STM高出几个数量级。

Conclusion: Multiverse兼顾无版本和多版本STM的优点，为含长时运行读事务且数据频繁更新的工作负载提供了极大性能提升，同时在普通负载下也不逊色于最优现有STM。

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [2] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 虽然FAIR原则提高了数据标准化，但由于标识符和数据模型不统一，资源的实际互操作性较低。该文提出Babel和ORION两工具，有效提升知识库间的互操作性，并推出实用的可下载资源。


<details>
  <summary>Details</summary>
Motivation: 尽管众多资源采用了FAIR原则的标准化词汇与元数据，但因标识符和数据模型差异，实际数据互操作性依然较差。论文致力于解决标准合规但互操作不佳的现象。

Method: 开发了两个工具：Babel和ORION。Babel 通过高质量的标识符映射消除多标识符方案差异，ORION 通过统一数据模型消除多数据模型差异。两者均通过API等方式实现高效的数据协同。

Result: Babel能将多种标识符方案高效映射，形成等价类；ORION将异构知识库转换为统一、社区维护的数据模型。应用两个工具构建的高度互操作知识库已上线并开放获取。

Conclusion: Babel和ORION有效弥合了理论上的数据互操作与实际应用间的鸿沟，显著提升了FAIR数据生态的实际互操作能力。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [3] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: SDP方法高效地发现前k个最有用的功能依赖，通过有效剪枝和优化提升速度与内存效率。


<details>
  <summary>Details</summary>
Motivation: 传统FD发现方法面临计算复杂度高和结果集过大难以筛选有用依赖的问题。因此，需设计高效算法，专注于发掘高价值的依赖约束，缓解大数据下的性能和结果筛选压力。

Method: SDP通过计算冗余计数并设置上界，结合排序、矩阵统计和搜索调度等三项优化，优先挖掘高价值分支、剪去低潜力分支，提升发现效率。

Result: 提出了一种新的发现关系型数据库功能依赖（FD）的选择式发现与剪枝算法SDP，该算法通过冗余计数对FD排序，并利用冗余计数的上界进行剪枝，显著减少候选空间和搜索量。

Conclusion: 实验结果表明，SDP在40多个数据集上比以往的穷举方法更快，且内存消耗更低，有效提升大规模高维数据下FD发现的实用性。

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [4] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 该论文提出通过合并结构上类似的事务来优化应用侧的事务处理性能，并设计了名为TransactionMerger的中间件与静态分析工具，实验证明可显著提升系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 提升应用端事务处理性能，发掘结构类似语句或事务的合并潜力。

Method: 1) 重写事务以合并类似语句，2) 消除冗余读取，3) 预计算聚合效果以合并冲突语句，同时开发TransactionMerger中间件和静态分析工具保证隔离性并实现自动化识别和重写。

Result: 在TPC-C基准测试下吞吐量提升至2.65倍，在真实应用Spree中提升至3.52倍。

Conclusion: 结构性事务合并是一种有效提升应用侧事务处理性能的方式，并在多种场景下取得显著效果。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [5] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 本文提出了一种伪代码算法，可将基础数学数据模型方案转换为关系模型及相关的非关系约束集合，并用于MatBase智能数据库管理系统原型中。作者验证了其算法的快速性、稳健性、完整性以及最优性，并将其应用于族谱树子域的建模，同时还给出了SQL和VBA代码示例以及开发相关约束代码的指南。


<details>
  <summary>Details</summary>
Motivation: 当前数据库建模工具在处理复杂的数学数据结构以及实现非关系约束方面存在局限，因此作者基于MatBase开发该转换算法，以提升智能数据库管理系统的建模能力和约束控制能力。

Method: 作者设计并证明了一个伪代码转换算法，将Elementary Mathematical Data Model方案自动转换为关系数据库结构，同时生成强制非关系约束的辅助代码，并提供相关实现示例。

Result: 算法成功应用于族谱树的数学建模领域，能够快速、完整且最优地实现模型转换，并可通过SQL和VBA代码对非关系约束进行具体实现与控制。

Conclusion: 该算法高效且最优，能够有效地将数学数据模型转化为关系模型及配套约束，实现对数据库非关系约束的支持和控制。

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 本文提出了一个用于系统思考AI生存威胁的通用分析框架，通过分析AI存在主义威胁的两条核心前提，提出人类存续的四类情境分类，并探讨了每类情境面临的具体挑战和应对策略。


<details>
  <summary>Details</summary>
Motivation: 近年来关于AI是否构成人类存在风险存在广泛争议，尤其是在ChatGPT发布后。该文旨在澄清争议，通过逻辑框架和分类全面梳理AI存续威胁及应对措施，为风险评估和政策措施提供理论基础。

Method: 作者采用理论分析方法，围绕‘AI系统是否会变得极其强大’和‘极其强大的AI是否会毁灭人类’两大假设，构建了生存情境分类（taxomony of survival stories），并逐一剖析各情境下人类如何得以存续，最后结合这些情境对P(doom)进行概率估算。

Result: 文章提出了四种人类存续情境，并指出每种情境对应的关键阻碍，同时总结出各情境对AI威胁应对措施的不同激励。通过分类讨论，提供了对于AI引发人类灭绝概率的初步定量分析。

Conclusion: 不同的人类存续情境各自面临独特的挑战，因此应采取相应的差异化策略。同时，基于提出的情境分类，作者对AI导致人类灭绝的可能性进行了粗略概率估算。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [7] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: 本文提出GUI-Eyes RL框架，通过主动视觉感知和逐步感知策略，提高了GUI任务中的信息获取效率和准确性，实现了更强的泛化性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI自动化方法大多依赖静态、一次性的视觉输入和被动感知，导致无法自适应地决定何时、是否及如何观察界面，限制了任务灵活性和数据效率。

Method: 提出两阶段（粗粒探索-细粒定位）主动感知策略，用RL训练智能体自主决策是否及如何使用视觉工具（如裁剪、缩放）以获取最有价值的信息，并设计空间连续奖励函数缓解稀疏奖励问题。

Result: 在ScreenSpot-Pro基准上，GUI-Eyes-3B仅用3k标注样本就达到了44.8%的grounding准确率，显著优于其它监督学习及RL基线方法。

Conclusion: 工具感知的主动感知（结合分阶段策略推理和细粒度奖惩反馈）对构建强健、高效的数据驱动GUI智能体至关重要。

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [8] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 本研究通过在线实验，首次证实使用大型语言模型（LLM）带来的效率提升，会引发来自群体的实际行为惩罚。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被广泛采纳引发社会对其使用者态度的担忧，目前缺乏实际行为层面的证据，研究动机在于检验负面态度是否转化为带有成本的实际惩罚行为。

Method: 采用两阶段在线实验：第一阶段参与者作为“目标”完成任务，部分人使用LLM辅助，第二阶段491名参与者可用自身报酬惩罚第一阶段的目标。通过对实际LLM使用与自报情况的交互，量化惩罚强度。

Result: 平均而言，完全依赖LLM者其报酬被他人销毁了36%；惩罚强度随LLM实际使用递增。自报未用者受到的惩罚高于真实未用者，高度使用者中，真实使用受到更重处罚。这表明LLM效率带来了社会制裁成本。

Conclusion: LLM用户虽然获得了效率提升，但会因此受到他人的经济惩罚，且这种惩罚程度与使用LLM的实际程度正相关。声明未使用LLM的参与者也因不被信任受到更多惩罚。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [9] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 提出非交互式端到端逻辑推理框架，利用“结构化few-shot提示+推断时注意力干预”，显著提升LLM逻辑推理性能，无需外部资源，计算开销低。


<details>
  <summary>Details</summary>
Motivation: 当前LLM逻辑推理多依赖复杂互动框架或外部符号解算器等资源，造成结构复杂、可扩展性受限。希望通过 non-interactive、端到端框架，让推理在模型内部自发涌现，提高泛化并不需外部资源。

Method: 提出了一种非交互式的端到端框架，通过在few-shot提示中引入结构信息激活部分与逻辑推理操作相关的注意力头，并在推断时采用Attention-Aware Intervention（AAI）方法对这些注意力头进行重加权，从而增强模型的逻辑推理能力。

Result: 实验证明AAI方法在不同数据集和模型结构上均能显著提升逻辑推理表现，且计算开销几乎可以忽略。

Conclusion: AAI方法能高效地通过注意力调控增强逻辑推理能力，提升泛化性且易于分析，不依赖外部组件，是提升大模型推理性能的新途径。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [10] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen通过片段检索和强化学习优化，有效提升了多属性精确约束下的分子生成质量，优于当前主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在面向多属性严格约束的分子生成中，受限于精确的多目标控制和数值推理能力，难以直接满足实际需求，因此需要一种更精准且可控的方法。

Method: MolGen是一种两阶段的分子生成框架，第一阶段为原型生成，多智能体检索-锚定通过片段级编辑生成候选分子；第二阶段为基于强化学习的优化，借助Group Relative Policy Optimization（GRPO）算法进行片段细粒度调整，使各属性向目标收敛，同时约束编辑复杂度并减少偏离。两阶段均由大规模自动化标注的片段编辑与属性变化链数据集提供监督。

Result: 实验证明，MolGen在多个属性约束（如QED、LogP、分子量及HOMO、LUMO）的分子生成任务中，在生成有效性和属性满足精度方面均显著优于先进的LLM和图生成算法。

Conclusion: 本文提出的MolGen框架在多属性约束下的分子生成任务中表现优越，能够有效且精确地满足多个理化性质的数值目标，优于主流的LLM与图生成方法。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [11] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek方法可显著提升大模型推理准确率和稳定性，无需调节推理长度，且突破了上下文长度限制，计算高效。


<details>
  <summary>Details</summary>
Motivation: 解决顺序推理扩展导致准确率下降和模型不稳定性的问题，提高大模型在复杂推理任务中的表现，同时减少人工参数调节、提升运算效率。

Method: 方法为在推理时仅保存新思考步的KV对至缓存，并通过自定义KV缓存剥离位置编码，实现KV连续存储及动态扩展，同时具备线性计算复杂度。

Result: 提出了一种新的顺序推理测试时扩展方法Min-Seek，该方法在不需要推理长度微调的情况下，显著提升了大模型在多种推理任务中的准确率和稳定性，且不受模型最大上下文长度限制。Min-Seek仅在KV缓存中保留一个新增推理步的KV对，并通过去除位置嵌入，将KV连续编码进缓存，从而实现高效扩展和线性计算复杂度。

Conclusion: Min-Seek成功解决了现有顺序推理扩展中精度下降与模型不稳定问题，实现了大范围、稳定且高效的推理扩展，并具有广泛推理任务的适应性。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [12] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 论文提出，需用交互主义视角系统研究LLM群体行为与知识、价值与社会环境的交互，并指明了理论、方法、跨学科等四大研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在多智能体系统中的广泛应用，其集体行为可能引发大规模社会影响、风险和机遇，因此需系统探究知识、先验和价值观在群体中的传递与涌现机制。

Method: 通过对LLM特性分析和集体行为综述，提出交互主义理论框架，建议结合多理论、多方法和跨学科手段来研究与部署LLM多智能体系统。

Result: 本文强调了基于大型语言模型（LLM）的多智能体集体行为研究对于评估其风险和收益，以及对社会产生深远影响的重要性。作者主张，LLM具备丰富的预训练知识和隐性社会先验，同时通过上下文学习实现自适应，这使得需要采用交互主义范式，对其理论基础、方法和分析工具进行更新，以系统性地研究知识与价值观在社会环境中的交互作用及其带来的群体涌现现象。作者提出了四个前沿方向，包括理论创新、方法拓展以及跨学科对话，以实现LLM集体系统的更好发展与部署。

Conclusion: 理解和规制LLM集体行为对社会有重大影响，未来需调整理论、方法和跨学科实践，以深入探索知识、价值观与社会环境的交互机制。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [13] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 本文重新定义了人机互补性，将其从单一准确率指标扩展为AI可靠性评价工具，提出应结合各类可靠性指标综合考量。


<details>
  <summary>Details</summary>
Motivation: 传统互补性概念过于狭窄，忽视了决策成本、可靠性及实际操作难题，故需在理论层面深化和重新定位其价值，以更好指导人机协作实践。

Method: 采用理论分析和知识论（epistemology）框架，结合计算可依赖主义，将人机互补性重新定位为可靠性评估的一部分。

Result: 本论文将人机（Human-AI）互补性从原本的决策准确性提升，拓展为对AI辅助决策可靠性的理论分析。通过借助知识论视角，特别是计算可依赖主义，论文认为互补性可作为评价人机决策系统是否为可靠认知过程的证据之一。论文强调人机互补性不仅应关注预测准确率，还需考察其与认知标准及社会技术实践的匹配，从而更科学评估AI参与决策的实际有效性。

Conclusion: 人机互补性价值不在于提供预测准确性的相对比值，而在于辅助评估和校准AI支持决策过程的可靠性，对患者、管理者、监管者等有实际指导意义。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [14] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 现有LLM驱动的多智能体系统往往依赖人工预先设定的工作流规则，存在编码代价高和难以覆盖复杂任务状态空间的问题。本文提出一种基于Agent间通信和信息流协调的新范式，实现了无须预定义工作流的高灵活性多智能体合作。与典型规则驱动系统比较，该方法在通用基准测试上表现更优，准确率提升8.49%。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中人工设定工作流难覆盖复杂任务状态、编码成本高的痛点，引入更自动化、动态的信息流管理机制。

Method: 采用信息流协调器监控任务进度，通过A2A工具包以自然语言动态协调各智能体，无需预设工作流；与主流规则驱动MAS（如OWL）对照实验，角色与模型保持一致，检测准确率和token消耗。

Result: 新方法在GAIA基准测试pass@1下准确率达63.64%，较传统系统提升8.49个百分点，token消耗相近，在个案分析中表现出更优的弹性和鲁棒性。

Conclusion: 信息流协调的A2A多智能体范式能显著提升任务灵活性和边界情况处理能力，在GAIA基准测试中的表现优于传统基于规则的系统。

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [15] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: CMA架构通过引入连续、可变和抽象的记忆机制，改善了RAG方法在长期任务上的记忆管理不足，是面向长时段智能体的必要架构基础。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）方法在大语言模型中广泛应用，但其内存机制仅为无状态查找，缺乏信息更新、时序性和内存持续性，限制了智能体的长期任务表现。

Method: 通过理论定义和架构规范，结合知识更新、时序联想、联想召回和上下文消解等实证探针，对比分析CMA与传统RAG在复杂记忆应用上的行为差异。

Result: 提出了Continuum Memory Architecture（CMA）架构，该系统具备持久存储、选择性保留、联想路由、时序链路和高级抽象整合等能力，有效克服RAG在长期记忆积累、变异与消解方面的结构性局限，并在多个任务上展现持续性行为优势。

Conclusion: CMA能够显著提升智能体在长期、多轮互动中的记忆表现，但同时带来了延迟、漂移及可解释性等新挑战，未来需重点解决这些问题。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [16] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 本文针对计算机使用智能体（CUA）面临的提示注入攻击，提出了一种名为“单次规划”的防护机制，在智能体观测到可能恶意内容之前，预先生成完整带条件分支的执行图，从而显著提升控制流完整性和安全性。


<details>
  <summary>Details</summary>
Motivation: 目前AI智能体易受提示注入攻击，传统防护需架构隔离。但CUA需持续观测UI，难以兼顾安全架构与功能需求，因此亟需兼顾安全与实用的新方法。

Method: 提出架构隔离设计，通过信任的规划器在尚未观测到环境（UI）之前，生成带条件分支的执行图，并在OSWorld环境中进行性能与安全性测试。

Result: 新方案有效抵御指令注入攻击，可兼具实用性和安全性，并在部分情况下提升模型性能。然而尚需针对分支引导攻击补充防护措施。

Conclusion: 单次规划方法能够在保障安全性的同时，保留前沿模型最高57%的性能，并提升小型开源模型性能达19%。但仍需额外措施防止分支引导攻击。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [17] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 本文提出了一种以根因分析和持续改进为核心的AI幻觉管理框架，结合多元检测与分层缓解策略，并在金融数据抽取中验证了其提升模型可靠性和系统信任度的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和大型推理模型（LRMs）在金融与法律等高风险领域展现出变革性潜力，但其幻觉现象（生成事实错误或无根据内容）极大威胁模型可靠性。亟需系统方法来持续管理与减少幻觉，以提升生成式AI在关键场景下的信任度。

Method: 提出了一套全面的幻觉管理运行框架，基于根本原因意识的持续改进循环。将幻觉来源细分为模型、数据和上下文三类，通过多源检测方法（如不确定性估计、推理一致性）和分层缓解策略（如知识锚定、自信度校准）进行有针对性干预。并采用分层架构和金融数据抽取案例实证应用。

Result: 该框架将模型、数据及上下文层次形成闭环反馈，能实现进步式可靠性提升。案例表明，其方法为生成式AI在受监管环境中构建可信系统提供了系统性、可扩展的解决方案。

Conclusion: 通过细致分类和分层干预的幻觉管理框架，有效提升了生成式AI在高风险领域的可靠性，为相关领域广泛应用奠定了基础。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [18] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: 提出并构建了专为中国劳动法打造的大语言模型LabourLawLLM，同时建立了覆盖多任务的综合基准LabourLawBench，实验表明新模型在各项测试中表现优越，方法具有迁移性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在法律专业领域，尤其是子领域如中国劳动法上，存在知识不精确、推理能力不足和上下文理解薄弱的问题。为解决这些限制，研究者希望构建专门针对特定法律子领域的模型。

Method: 开发了专注于中国劳动法的大模型LabourLawLLM，并构建了涵盖多项法律任务的基准测试集LabourLawBench。采用了包含ROUGE-L、accuracy、F1、soft-F1等客观评测指标和基于GPT-4的主观评价框架，对模型进行系统性测试与比较。

Result: 实验结果显示，LabourLawLLM在法律条文引用、知识问答、案例分类、赔偿计算、命名实体识别和案件分析等多个任务类别上，均优于通用型及现有法律专用大模型。

Conclusion: LabourLawLLM不仅提升了中国劳动法律领域AI应用的准确性与可靠性，该研究提出的方法论对于其它法律子领域的大模型定制也具有良好的复制性和扩展性，提升了法律AI服务的社会价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [19] [SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation](https://arxiv.org/abs/2601.09974)
*Seoyeon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: 文章提出SPRInG框架，通过偏好漂移识别和选择性自适应，实现大模型的高效持续个性化，并在实验中优于已有方案。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型个性化方法常假设用户偏好不变，实际中用户兴趣不断变化，偏好漂移给模型适应能力带来挑战，标准持续学习方法容易遗忘旧知识且难以区分真实偏好变动和噪声。

Method: 提出了SPRInG半参数框架。在训练阶段通过基于似然的评分函数检测高度新颖交互，实现偏好漂移驱动的选择性自适应，并用回放缓冲区保存难以学习的残差；推理阶段采用严格的相关门控，并通过logit插值融合模型知识与历史检索内容。

Result: 在长文本个性化生成基准任务上，SPRInG性能超过现有方法，展现了其在真实持续个性化场景下的鲁棒性。

Conclusion: SPRInG能有效适应用户偏好漂移，克服持续学习中遗忘和噪声问题，实现高效可靠的个性化大模型。

Abstract: Personalizing Large Language Models typically relies on static retrieval or one-time adaptation, assuming user preferences remain invariant over time. However, real-world interactions are dynamic, where user interests continuously evolve, posing a challenge for models to adapt to preference drift without catastrophic forgetting. Standard continual learning approaches often struggle in this context, as they indiscriminately update on noisy interaction streams, failing to distinguish genuine preference shifts from transient contexts. To address this, we introduce SPRInG, a novel semi-parametric framework designed for effective continual personalization. During training, SPRInG employs drift-driven selective adaptation, which utilizes a likelihood-based scoring function to identify high-novelty interactions. This allows the model to selectively update the user-specific adapter on drift signals while preserving hard-to-learn residuals in a replay buffer. During inference, we apply strict relevance gating and fuse parametric knowledge with retrieved history via logit interpolation. Experiments on the long-form personalized generation benchmark demonstrate that SPRInG outperforms existing baselines, validating its robustness for real-world continual personalization.

</details>


### [20] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL无需模型微调，通过多样化结构分解和利用历史正确及纠错数据，实现更高准确率和大幅降低计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统只用正确示例，忽略错误-修正对的经验信号，且随意分解问题导致SQL结果相似、集成收益小，同时在准确率与效率间存在明显权衡。论文旨在通过结构化分解和经验驱动纠错，兼顾准确率和效率，实现更强鲁棒性与资源利用优化。

Method: 采用三种结构清晰的分解策略（实体型、层次型、原子顺序型），结合动态记忆库存储成功请求与历史错误修正对，通过检索增强提示在推理时引入相关案例，达到自我纠错与多样化解答。无需微调，无需外部API。

Result: Memo-SQL提出了一种无需训练的新框架，通过结构化分解和基于历史经验的自我纠错，显著提升NL2SQL系统的准确性与效率。其在BIRD数据集上执行准确率达68.5%，刷新了同类无微调方法的新纪录，且资源消耗比现有TTS方法低十倍以上。

Conclusion: 多样化分解和记忆增强纠错无需额外训练即可有效提升NL2SQL系统的性能，可作为无训练高效NL2SQL系统的新范式。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [21] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: 本文提出基于多保真数据的深度学习框架FilDeep，有效解决了大变形问题数据集数量与精度的难题，在制造业相关应用中取得很好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大变形弹塑性固体的科学计算对制造业至关重要，但传统数值方法存在固有限制，深度学习成为替代方案，但受制于高质量、高数量数据集难以获取，导致DL模型表现受限。

Method: 采用多保真训练方案融合低保真与高保真数据，并设计了注意力使能的跨保真模块捕捉长程物理作用，以提升弹塑性大变形计算的深度学习效果。

Result: 提出的FilDeep框架利用多保真数据同时进行训练，实现了数据数量与精度的平衡，显著提升了弹塑性大变形问题的科学计算效果，在一系列实验中表现优异，达到SOTA水平，可有效应用于制造业场景。

Conclusion: FilDeep是首个针对弹塑性大变形问题利用多保真数据的深度学习框架，通过创新的跨保真注意力机制，显著提升了计算精度与应用效率，有望在实际工业制造中广泛应用。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [22] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 本文基于OpenRouter平台，对超过100万亿token的真实大语言模型交互数据进行了实证分析，揭示了LLM实际应用中的多样化行为和留存模式，并探讨了相关影响及未来设计建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力从简单模式生成转向多步推理，实际使用场景迅速扩展，但对LLM真实用法的系统性理解滞后。作者希望通过对大规模实际交互数据的分析，补齐认知空白，指导更科学的模型和系统构建。

Method: 通过OpenRouter平台收集全世界不同时间和任务背景下的真实LLM调用数据，进行大规模实证统计分析，包括模型类别、应用场景分类、用户群体行为、流失率及留存分析等。

Result: （1）开源权重模型获得了广泛应用；（2）创意角色扮演和代码辅助等新兴类别热度显著高于生产力类任务；（3）多步agent推理日益增长；（4）首次发现长期忠实用户群“玻璃鞋效应”，即早期用户留存度明显高于后续新用户。

Conclusion: 开发者和终端用户对LLM的使用比想象中更为复杂多样，多样化场景和用户黏性应引起模型设计者、AI开发者及基础设施提供者的重视。基于实证数据对LLM系统进行优化，可提升用户体验和应用价值。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [23] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: MatrixCoT是一种为增强大语言模型逻辑推理能力而设计的结构化Chain-of-Thought（CoT）方法，采用矩阵化计划与反馈驱动重规划机制，提高了鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统CoT难以处理高度依赖符号表达与严格推理规则的任务，神经符号方法虽能保证形式正确但对输出稳定性高度敏感，基于LLM的方法则缺乏结构化和纠错机制，因此亟需一种兼具结构化表征与稳健纠错的逻辑推理方案。

Method: 提出MatrixCoT方法：包括自然语言规范化、类型化，显式的引用构造，基于矩阵的推理步骤规划来维护全局关联，并将推理路径转化为可验证的计划；此外，引入反馈驱动的检验和重规划机制，实现对缺陷和遗漏的检测、重写及依赖矩阵的压缩优化。

Result: 实验证明，MatrixCoT在五个逻辑推理基准和多个大模型上优于传统方法，表现出更强的输出稳定性与解释性，无需外部符号求解器，维持了高水平推理性能。

Conclusion: MatrixCoT在无需依赖外部求解器的前提下，在多个逻辑推理基准与模型上均显著提升了鲁棒性与可解释性，同时保证了竞争力的性能。

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [24] [Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs](https://arxiv.org/abs/2601.10114)
*Cheng Feng,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.AI

TL;DR: 本文提出了一种新型知识蒸馏方法，通过阶段性蒸馏和样本自适应加权，让小模型在领域任务中达到或超过大模型，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数量大，难以高效部署于特定领域。虽然有知识蒸馏方案，但教师—学生能力差距导致现有方法性能有限，亟需探究何时学生能媲美或超越教师并提出可行技术。

Method: 提出了Scheduled Checkpoint Distillation（SCD）和样本自适应加权（AW）机制。SCD在蒸馏过程中模拟教师微调收敛过程以减少教师偏好子域（TFS）的损失，AW机制通过样本级加权保留学生在学生偏好子域（SFS）上的优势。

Result: 在问答、命名实体识别、文本分类等多语言领域任务中，所提方法显著优于现有蒸馏方案，使学生模型可达到甚至超越经过微调的教师模型效果。

Conclusion: 本文提出的方法能够有效缩小大模型（教师）与小模型（学生）之间在领域任务上的性能差距，甚至使学生模型在部分场景超越教师模型。

Abstract: Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.

</details>


### [25] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 本研究发现，现有LLMs对结构化行为数据的时间规律推理存在局限，未能超越传统机器学习模型。适当的上下文有助于提高准确性，但过多上下文信息会适得其反。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在从结构化行为数据中推断时间规律的能力，特别是在用户重复行为（如复购）时间间隔预测中的表现，以及上下文丰富度对其推理能力的影响。

Method: 在零样本设置下，使用代表性的复购场景对大型语言模型（LLMs）进行基准测试，并将其预测时间间隔的能力与统计和机器学习模型进行比较，同时考察不同上下文信息对其表现的影响。

Result: LLMs在预测行为时间间隔时优于轻量级统计模型，但在捕捉定量时间结构方面逊色于专用机器学习模型。适度增加上下文能提升预测准确性，但进一步增加用户级细节反而降低了模型性能。

Conclusion: 大型语言模型在结构化时间推理任务上有根本性局限，仅靠扩展上下文无法解决其短板。未来需要结合统计方法与语言模型的灵活性，设计兼具定量精度与上下文感知能力的混合模型。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [26] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 本论文提出了一种新颖的、高度自适应的金融数据流系统，能针对市场概念漂移和分布非平稳性问题，通过机器学习驱动的数据管理方法提升模型在真实环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前金融领域数据驱动模型在真实市场难以表现理想，主要受限于概念漂移和分布不稳定，传统静态历史数据训练导致过拟合，泛化能力差。迫切需要一种能够自适应市场变化，提升数据与模型表现的方法。

Method: 采用了由机器学习驱动的自适应控制与数据增广、课程学习和数据流程管理相结合的参数化数据操作模块，并通过梯度双层优化的自适应计划与调度器对系统进行控制。其系统支持单股票变换、多股票混合、数据操作，并具有可追溯重放和数据质量持续监控功能。

Result: 通过大量预测及强化学习任务实验，本文提出的系统有效提升了模型的稳健性和实际风险收益水平，并实现对动态金融数据更加通用、自动化的数据管理与学习流程优化。

Conclusion: 该系统能够通过自适应数据流和差分化框架，实现对金融数据的高效管理，显著增强模型在动态市场中鲁棒性，并获得更好的风险调整收益。该方法在不同的预测与强化学习交易任务中均表现出较强的泛化能力。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [27] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 将LLM与轨迹数据对齐以解决决策任务，提出DecisionLLM，在多个基准任务上显著优于传统方法，性能取决于模型规模、数据体量和质量。


<details>
  <summary>Details</summary>
Motivation: 传统的RL及其改进（如Decision Transformer）在解决长序列决策任务上存在性能瓶颈，而LLM在复杂推理和规划方面表现突出，激发了探索其用于增强长时序决策任务的潜力。同时，LLM对于连续值的解释能力有限，亟需新方法以改进其在决策场景中的表现。

Method: 将大规模语言模型（LLM）应用于离线序列决策任务，通过将轨迹视为一种模态，并学习轨迹数据与自然语言任务描述的对齐，实现整体自回归决策预测，提出了DecisionLLM框架，并研究了其性能的扩展规律。

Result: 提出的DecisionLLM框架能够有效利用LLM的能力，在多个决策任务上超过了基准方法。性能受到模型规模、数据体量和数据质量三者共同影响。实验中，DecisionLLM-3B在Maze2D umaze-v1上相比DT提升69.4分，在AuctionNet上提升0.085分。

Conclusion: LLM通过与轨迹数据的模态对齐可以显著提升离线序列决策问题表现，DecisionLLM验证了该思想的有效性，并为大规模模型在决策领域的进一步应用及在线竞价等场景探索提供了新方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [28] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源容器平台，为医学影像AI模型提供统一、规范、可复现的应用环境，并已验证其在临床场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 医学影像中AI模型多样、部署繁琐且难以复现，制约科研与应用发展。亟需一个统一、可复现且易用的平台来推动医学影像AI模型的落地和对比研究。

Method: 采用开源容器技术，将经过同行评审的AI模型标准化封装，支持医学影像直接处理与统一接口，附带结构化元数据与公开参考数据，并扩展模块化支持与社区协作。通过实际临床数据对比评估展示平台优势。

Result: 本文提出了MHub.ai平台，通过容器化技术，将医学影像AI模型标准化与自动化处理，实现模型的可复现和易访问。平台涵盖分割、预测、特征提取等多领域模型，支持DICOM等主流格式，并通过结构化元数据和公开参考数据提高透明度与可靠性。在肺部分割模型的临床评估案例中演示了其实际效用，所有输出和指标均开放共享，支持结果复现和拓展分析。

Conclusion: MHub.ai简化了医学影像AI模型的使用流程，提升了模型的复现性与透明性，方便科研人员和临床医生进行模型对比评价，有效促进了AI模型的临床应用。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [29] [CtD: Composition through Decomposition in Emergent Communication](https://arxiv.org/abs/2601.10169)
*Boaz Carmeli,Ron Meir,Yonatan Belinkov*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed "Composition through Decomposition", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.

</details>


### [30] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 提出并验证了一种结合形状畸变指标、机器学习分类结果及特征空间分析的降采样评估流程。发现形状感知型降采样优于标准抽取，能有效降低计算负载并保留诊断信息；为nEMG及一般高频时序数据提供了降采样参数选择的实用方法。


<details>
  <summary>Details</summary>
Motivation: nEMG信号采样率高且不均一，导致基于特征的机器学习模型在近实时分析时计算压力大；降采样虽可缓解压力，但对诊断准确性的影响未被系统评估。

Method: 构建了一个系统性评估高频时序信号降采样信息损失的流程，将形状畸变指标、特征空间分析与已有的特征型机器学习模型分类结果相结合。通过三分类NMD任务进行实验验证。

Result: 验证了形状感知型降采样算法能更好地保持信号形态和峰结构，并在显著降低计算负载的同时，确保了模型的分类性能；提出的流程为nEMG和其他高频时序应用的降采样参数筛选提供了通用参考。

Conclusion: 本研究提出的评估流程能够有效筛选兼顾诊断信息保留和计算负载降低的降采样方案。形状感知型降采样算法优于标准抽取法，在保持波形结构和形态上表现更佳。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [31] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG提出了更有效的企业文档混合结构检索生成体系，能很好地处理文本和表格混合数据，效果优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前企业文档中数据结构复杂，既有文本又有结构化表格，现有RAG系统通过线性化简单化处理，导致无法充分捕捉数据的复杂关系，数学上也被证明不足。

Method: 提出Topo-RAG双路结构：文本通过传统的密集检索，表格通过Cell-Aware Late Interaction机制，保留表格空间关系，并在SEC-25数据集上进行了针对性评测。

Result: 在SEC-25数据集上，Topo-RAG在混合查询任务中nDCG@10相较标准线性化方法提升18.4%。

Conclusion: 简单的线性化无法完全利用企业文档的多维结构信息。Topo-RAG通过针对文本与表格分别设计检索机制，显著提升了复杂数据理解与检索效果。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [32] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM按推理步骤难度智能分配大、小模型，提升多步推理准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大模型路由方法未区分推理步骤重要性，往往导致计算资源浪费和级联失败，迫切需要一种更加细粒度的路由机制优化模型调用。

Method: 通过 process reward models 评估各推理步骤的错误概率和不确定性，实现基于阈值、长期收益和预算约束的多种路由策略，仅在关键步骤调用强大模型。

Result: 本文提出了TRIM (Targeted routing in multi-step reasoning tasks) 方法，通过仅在推理过程的关键步骤调用大型模型，而将常规步骤交由小型模型处理，有效防止多步推理任务中的级联错误。实验表明，TRIM 在主流数学推理基准（如MATH-500、AIME）上显著提升了推理成本效率，在保证准确率的同时节省了计算资源。

Conclusion: TRIM 在多步推理任务中通过有针对性的步骤分配，大幅提升了推理效率与成本效益，且具备较强的通用性。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [33] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: 本文提出NoReGeo基准，用于测试大型语言模型对几何知识的原生理解，发现当前主流模型能力有限，仅65%准确率，且无法通过微调习得这类认知。


<details>
  <summary>Details</summary>
Motivation: 现有几何评测主要关注基于推理和代数计算能力，难以衡量大模型是否真正具备原生的几何认知能力。因此，作者希望设计能直接评估模型对空间关系和几何属性的内在理解的基准方法。

Method: 提出了包含2500个、覆盖25类别的几何问题基准，这些问题仅需几何直觉解答而无须推理或代数计算，通过模型测试与消融实验系统评估原生几何理解能力。

Result: 即便是最先进的前沿语言模型（如GPT-4），在NoReGeo的二分类任务中整体最高准确率只有65%；且通过消融实验发现，该类几何认知不是通过微调训练自然获得的。

Conclusion: 当前大型语言模型在原生几何认知方面存在显著能力缺口，需要从训练初期采用专门方法以提升其几何理解，为后续模型发展指明方向。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [34] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: 提出EAPO方法，通过稠密证据奖励和自适应奖赏-策略协同迭代，解决长文本推理中证据检索不足的问题，实验证明有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习提升大模型推理能力，但在长上下文场景下难以获得稠密奖励，导致模型忽略证据检索过程，只关注最终答案，无法有效规避'侥幸猜中'。需要改进奖赏体系，强化证据检索和推理链条的监督。

Method: 提出EAPO（Evidence-Augmented Policy Optimization），一种基于证据的强化学习优化方法。通过Tree-Structured Evidence Sampling，发现提取精确证据是长上下文推理的瓶颈，在此基础上，EAPO利用Group-Relative Evidence Reward为过程提供密集的监督反馈，并引入Adaptive Reward-Policy Co-Evolution联合优化机制。

Result: 在八个基准测试中，EAPO显著优于现有最新基线方法，提升了长上下文推理的准确性和证据质量。

Conclusion: EAPO显著改善了长上下文场景下大模型的推理与证据质量，为证据驱动的强化学习推理提供了新思路。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [35] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: 本论文提出C-GRASP方法，通过个体化和可解释推理步骤，显著提升HRV在情感分类和临床应用中的准确性及透明度，有效解决了传统LLM在HRV分析中的生理偏差问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在理解HRV时存在生理幻觉问题，如RSA污染、非线性指标短数据不稳定、忽略个体基线等，缺乏临床可用性和透明度。

Method: 提出C-GRASP管道，融合了RAG（检索增强生成）和八步推理流程，并引入Z分数优先层级。系统中部署自动化RSA防护机制，以及个体化Delta Z-score模块，评估试验采用DREAMER数据集与高阶推理模型。

Result: C-GRASP在4类情绪分类上达到了37.3%准确率，CRC一致性评分为69.6%。消融实验证实Delta Z-score模块对于减少群体偏差至关重要。

Conclusion: C-GRASP能够显著提高基于HRV的情感分类的准确性，同时保持高一致性的临床推理，实现了个体化和可追溯的生理信号解读。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [36] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: 提出了一种能高效判断用户查询是否可答、并为text-to-SQL系统增加安全防护层的拒绝机制LatentRefusal，在准确性和开销方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对当前LLM驱动的text-to-SQL系统在处理不可答或不明确查询时，可能生成有害或误导性可执行SQL，现有拒绝方法效果有限且复杂，亟需高效安全拒绝机制保障系统部署安全。

Method: 提出LatentRefusal拒绝机制，通过利用LLM中间隐藏激活信息预测查询可答性，并引入Tri-Residual Gated Encoder以提取不符合问题和数据库结构的特征。

Result: LatentRefusal方案在四大基准测试集上，使平均F1分数提升至88.5，并且仅引入约2毫秒的探测计算开销。同时多项消融和可解释性分析验证了方法的有效性。

Conclusion: LatentRefusal拒绝机制凭借高效和可插拔特点，有效增强了text-to-SQL系统在处理不明确与不可答查询时的安全性，有望促进相关系统的安全部署。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [37] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval框架通过显式错误诊断提升了自动问答评价的准确性，改善对低质量问题的识别。


<details>
  <summary>Details</summary>
Motivation: 现有自动问答评价方法难以捕捉与显式建模关键错误，容易高估问答质量，因此亟需更灵敏和可解释的评价体系。

Method: ErrEval采用两阶段流程：第一阶段，轻量级错误识别器检测并分类常见结构、语言及内容错误；第二阶段，将诊断信号作为证据引导LLM评审，实现更细致的评价。

Result: 提出了ErrEval，一个用于自动问答生成评价的错误感知框架，能显式诊断并应对问答中的关键错误，如事实虚构和答案不匹配。ErrEval通过两阶段流程：错误识别与诊断信号引入LLM评估，实现细粒度和有依据的判断。实验证明ErrEval对齐人类评判，优于传统评价方法。

Conclusion: 通过引入错误诊断机制，ErrEval有效减轻了当前方法对低质量问题的高估，并与人类评价更一致。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [38] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: 本文提出并验证了LADFA自动化隐私政策分析框架，利用LLM与RAG技术高效、精准地提取和分析个人数据流，显著提升了隐私政策的大规模智能解析能力。


<details>
  <summary>Details</summary>
Motivation: 现有隐私政策多采用复杂法律语言，结构不统一，导致用户难以理解其个人信息处理方式。过去研究虽有利用大模型提取隐私政策数据流，但效果仍有限，因此有必要开发更精确、可扩展的自动化分析工具。

Method: 提出LADFA框架，结合大语言模型（LLM）、检索增强生成（RAG）以及来自现有研究的定制化知识库。该系统由预处理器、LLM分析处理器和数据流后处理器组成，实现对隐私政策文本的结构化数据流提取与图谱构建。

Result: 通过对汽车行业十份隐私政策的案例分析，验证了LADFA在个人数据流抽取和分析流程中的实用性和高准确性。

Conclusion: LADFA可自动化、高效地分析复杂隐私政策中的个人数据流，支持数据流图的构建，为深入洞察数据处理行为提供技术支持，具有良好扩展性和定制性，适用于多种文本分析场景。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [39] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: 提出LLMdoctor框架，实现以token为粒度的高效推理时对齐，兼顾性能与多样性，效果超过主流对齐和微调方法。


<details>
  <summary>Details</summary>
Motivation: 传统LLM微调对齐方法计算开销大、灵活性差；现有推理时对齐手段存在轨迹级信号失真和采样效率瓶颈，影响模型性能及生成多样性。

Method: 提出LLMdoctor框架，在推理阶段通过“病人-医生”范式实现高效对齐，结合token级奖励采集及token级流引导偏好优化（TFPO），利用小型doctor模型精细引导冻结的大型patient LLM。

Result: LLMdoctor能从patient模型行为变化中提取细粒度token级偏好信号，并用TFPO进行doctor模型训练，实现高效token级对齐，同时保持生成多样性。实验显示LLMdoctor优于现有所有推理时对齐方法，并甚至超越全微调方法DPO。

Conclusion: LLMdoctor通过token级偏好信号和流一致性优化，实现了LLM的精细推理时对齐，解决了计算、灵活性和多样性难题，有望成为推理时对齐的主流新方法。

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [40] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一种用于工业表格数据的非侵入式残差增强系统，通过神经-符号技术提升遗留模型，显著改善难预测区域，部署简单，风险低，效果优于多种主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统工业模型升级面临高昂的再训练成本和部署风险，如何在不重训练的前提下提升模型表现，修复难以预测的区域，是切实的产业痛点。

Method: 方法包括三步：1. 检测遗留GBDT模型的高残差区域（hard regions）；2. 利用大语言模型(LLM)生成易解释的符号“专家”并用贝叶斯优化调整参数；3. 采用轻量级聚合器动态融合专家与原模型输出，实现分区优化。

Result: 本文提出了一种名为NSR-Boost的神经-符号残差提升新框架，用于升级工业级的表格数据模型。该方法无需重新训练原有模型，而是针对预测失效的“难区域”进行修正。此框架已在大型金融风控系统中成功应用，实验证明其在公开与私有数据集上显著优于SOTA基线模型，且在真实线上数据中表现出额外增益。其能够识别传统模型遗漏的长尾风险，为工业应用提供了安全、低成本的模型进化方案。

Conclusion: NSR-Boost有效提升了工业GBDT模型在长尾风险等难题上的表现，部署安全、低成本，能够在不替换原模型的前提下实现大幅性能增益。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [41] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 文中提出ChartComplete数据集，覆盖30类图表，为多模态大模型的图表理解评测提供更全面的基准。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）在图表理解领域表现突出，但现有评测数据集覆盖的图表类型有限，难以全面衡量模型能力。

Method: 基于可视化领域的图表分类体系，系统收集并整理了30种图表类型的已分类图像，构建起多类别、无标注信号的开放数据集。

Result: 提出了ChartComplete数据集，覆盖了30种不同的图表类型，显著扩展了现有数据集的类型广度。

Conclusion: ChartComplete数据集填补了现有数据集覆盖面不足的空白，并向研究社区免费提供，以促进相关领域的发展。

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [42] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 该论文提出了一个新任务——领域知识图谱融合（DKGF），即从通用知识图谱中引入相关事实以丰富领域知识图谱，并提供了首个标准化测试基准。


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱覆盖范围有限，难以满足实际需求，通过融合通用知识图谱信息可提升领域知识图谱的丰富度，但需解决领域相关性和粒度不匹配的问题。

Method: 论文提出了Fact-as-Program范式，通过将通用知识中的事实视为潜在语义程序，并采用粒度感知的算子进行抽象关系映射，再利用程序可执行性验证领域相关性，整体方法建立在联合概率框架之上。

Result: 构建了DKGF(W-I)和DKGF(Y-I)两大基准及21种评测方案，大量实验验证了领域知识图谱融合任务的重要性和所提方法的优越性，并建立了首个领域知识图谱融合标准化测试平台。

Conclusion: 实验结果表明，ExeFuse方法在解决领域相关性与粒度匹配问题方面效果显著，为领域知识图谱融合任务提供了有效支持和标准化测试平台。

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [43] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 本文统一评估了7款前沿多模态大模型的安全性，发现各模型安全表现不一，对抗评估下所有模型安全性均显著下降，呼吁标准化和多维度安全评估以支撑负责任的模型发展和应用。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型取得了推理、感知和生成能力的重大进步，但目前模型安全性提升是否同步尚不明确，且现有评估往往只关注单一模态或威胁模型，缺乏统一、系统的安全评估。

Method: 对7个前沿大模型（如GPT-5.2、Gemini 3 Pro等）开展多模态（文本、视觉-文本、图像生成）统一安全评估，包括基准测试、对抗性评估、多语言评估及合规性评估，形成安全排行榜及模型安全画像。

Result: GPT-5.2在所有评估中表现出稳定且均衡的安全性能，其他模型在基准安全、对抗对齐、多语种泛化和法规合规性等方面存在明显权衡。所有模型在对抗性评估下安全性均有大幅下降，尤其在多模态任务中表现更为脆弱。文本生成图像模型在受监管风险类别对齐较好，但对抗或语义模糊输入下依旧易受影响。

Conclusion: 前沿模型的安全性具有多维属性，依赖于模态、语言和评估方式，强调了评估标准化以准确衡量真实风险和规范模型研发部署的必要性。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [44] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 通过激活LLM解码过程中的内置安全信号，实现高效越狱攻击检测，增强安全性且不损伤正常实用性。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制对复杂的越狱攻击防御能力有限，且往往导致检测过度或模型性能下降，因此需要高效且不损失模型实用性的安全检测手段。

Method: 在LLM生成过程中，显式挖掘和利用模型内部潜在的安全信号，用于实时检测不安全内容。

Result: 该方法能有效提升模型在面对多样越狱攻击时的安全性，同时在正常输入上保持较低的过度拒答率与高回复质量。

Conclusion: 激活和应用模型生成过程中的内置安全感知能力，是对抗越狱攻击的新型且有潜力的补充防御方向。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [45] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent通过多智能体协作提升基因组问答能力，优于GeneGPT，且具备更强泛化与适应性。


<details>
  <summary>Details</summary>
Motivation: 基因组信息在生物医学领域至关重要，但由于数据库结构复杂且分布式，数据提取具有挑战性。现有的大型语言模型应用于基因组问答存在特定领域数据库访问受限的问题。GeneGPT通过API优化提升性能但灵活性有限，无法适应不断变化的数据库环境。

Method: 复现GeneGPT，并设计GenomAgent多智能体协作框架，采用专用agent分工处理复杂基因组任务，通过实验基于GeneTuring基准进行对比评估。

Result: GenomAgent在GeneTuring基准测试的九项任务中平均比GeneGPT高12%。此外，其架构具备在其他科学领域提取专家知识的潜力。

Conclusion: 多智能体框架显著改善了LLM对复杂基因组数据的问答表现，并拓展了应用领域，显示出对需要专业知识提取的科学场景的广泛适用性。

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [46] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 提出了高效符号性LTLf多目标综合算法，比枚举方法快数十至百倍，解决了多个属性不可全部满足时的综合效率瓶颈。


<details>
  <summary>Details</summary>
Motivation: 面对多个LTLf属性难以全部满足的情况，现有方法需枚举所有可能属性子集，计算效率低下且不可扩展。作者希望在不枚举子集的前提下，通过符号性方法提升综合过程的效率和可扩展性。

Method: 本文提出了一种用于LTLf（有限时序逻辑）综合的完全面向符号的方法，采用布尔目标变量和单调性来在一次固定位点计算中高效表示并处理指数级规模的目标组合。相比传统枚举子集的方法，算法能够直接从产品博弈状态推导可实现的目标集合及其最大化策略。

Result: 算法利用布尔目标变量和单调性显著压缩表示规模，在实验中比枚举方法获得最多百倍提速，在多属性综合场景下实现了效能和扩展性的大幅提升。

Conclusion: 文中符号性方法为多目标不可满足时的LTLf综合问题提供了有效解决思路，大幅提升计算效率，对实际复杂系统的合成有较强的应用前景。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [47] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: 论文系统性分析了HRM的推理模式，发现其不像“推理”而更像“猜测”。为克服其失败模式，作者提出三种扩展猜测策略，将推理性能显著提升，并发展了增强版HRM。科学上揭示了推理模型内部推理的本质机制。


<details>
  <summary>Details</summary>
Motivation: 虽然HRM在多种推理任务上远超基于大语言模型（LLM）的推理器，但其具体优势及潜在失败方式尚不清楚，故需对其推理过程机制进行深入剖析，以理解其局限并探索如何进一步增强其性能。

Method: 对HRM（分层推理模型）的推理模式进行了机械性研究，发现其推理存在意外失败、推理动态的“grokking”现象以及存在多个固定点，并据此提出采用数据增强、输入扰动和模型自举等三种策略来提升HRM的推理准确性。最终开发了增强版HRM（Augmented HRM）。

Result: 通过对HRM推理机制的系统分析，发现其在极简谜题下有意外失败、推理过程存在突然“跃进”、以及会被错误固定点卡住。结合三种猜测扩展策略，提出了增强版HRM，实践上将Sudoku-Extreme问题上的准确率从54.5%提升至96.9%。

Conclusion: HRM的推理过程本质上更偏向于“猜测”而非严密推理，其失败多源于固定点假设的破裂、多解的干扰、以及推理动态的不连续。作者通过猜测规模化策略，有效提升其性能，对推理模型本质有重要揭示意义，并实证推动推理系统在复杂任务上的实用价值。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [48] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 本文提出结构知情且受多样性约束的上下文气泡框架，通过谨慎选择和结构引导，优化企业文档检索与生成结果，减少冗余并提升内容覆盖和引用质量。


<details>
  <summary>Details</summary>
Motivation: 针对当前LLM生成内容普遍使用RAG检索机制，存在信息碎片化、内容重复、过度检索及查询上下文不足等问题，包括难以覆盖更高阶的内容关联。作者希望解决这些结构与上下文相关性不足导致的信息检索和生成问题。

Method: 提出了一种结构知情且多样性受限的上下文气泡构建框架，利用严格的token预算，组装内容连贯且可引用的span集合，按文档结构（如章节、表格行）多粒度组织内容，并加入任务相关的结构先验引导检索，从高相关性锚点span出发，以受限选择过程平衡查询相关性、边际覆盖度和冗余惩罚。在检索过程中显示评分和选择轨迹，实现可审计与可调试性。

Result: 在企业文档实验中，该方法显著减少了冗余，上下文更加全面地覆盖了次级内容面，提升了答案质量和引文的真实度；消融实验验证结构先验和多样性约束均为关键，缺失任一将导致覆盖下降和冗余上升。

Conclusion: 结构化和多样性约束的上下文气泡方法有效改善了传统top-k检索的碎片化和冗余问题，提高了企业文档检索质量和生成的引用准确性，关键组件缺失会导致性能明显下降。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [49] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: GenAI对新手建筑设计师有助益，但普遍会降低创意自信，无显著影响认知负荷；使用方式和用户经验影响工具效果。


<details>
  <summary>Details</summary>
Motivation: 探究生成式人工智能（GenAI）在建筑概念设计任务中的应用对于设计表现、创意自我效能感和认知负荷的影响。

Method: 采用两阶段设计任务，36名学生（建筑工程及其它专业）独立完成任务后分别用GenAI或线上建筑项目库协助；专家评分设计成果，并在每阶段后自报创意自我效能和认知负荷。通过差异中的差异进行数据分析和分组比较。

Result: GenAI整体未提升所有参与者的设计表现，但对新手设计师有显著提升；使用GenAI反而降低了学生的创意自我效能感；不同条件下认知负荷无显著差异。迭代式生成与可视化反馈型提示能有效减少认知负荷。

Conclusion: GenAI的效能依赖于用户的既有专业能力以及与工具的互动（如提示使用策略），因此要发挥最大潜力需关注“因人制宜”的使用方式。

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [50] [Multi-Agent Cooperative Learning for Robust Vision-Language Alignment under OOD Concepts](https://arxiv.org/abs/2601.09746)
*Philip Xu,Isabel Wagner,Eerke Boiten*

Main category: cs.MA

TL;DR: 提出了一种多智能体协作学习（MACL）框架，通过多智能体信息传递和自适应动态平衡机制，缓解视觉-语言模型在处理分布外概念时的跨模态失衡问题，并在VISTA-Beyond数据集上获得1-5%的精度提升。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言模型在面对分布外概念时常出现的跨模态对齐能力下降（alignment collapse）问题，提升其泛化能力。

Method: 该方法由图像、文本、名称和协作四个智能体组成，通过结构化消息传递协同工作。框架实现了多智能体特征空间名称学习，集成上下文交换增强少样本学习算法，并采用动态平衡机制调节各智能体贡献。

Result: 在VISTA-Beyond数据集上，MACL在few-shot和zero-shot任务中均带来1-5%的精度提升，覆盖多种视觉领域。

Conclusion: MACL框架有效提升了视觉-语言模型在分布外（OOD）场景下的few-shot和zero-shot学习表现，实现了跨多种视觉领域1-5%的精度增益。

Abstract: This paper introduces a novel Multi-Agent Cooperative Learning (MACL) framework to address cross-modal alignment collapse in vision-language models when handling out-of-distribution (OOD) concepts. Four core agents, including image, text, name, and coordination agents, collaboratively mitigate modality imbalance through structured message passing. The proposed framework enables multi-agent feature space name learning, incorporates a context exchange enhanced few-shot learning algorithm, and adopts an adaptive dynamic balancing mechanism to regulate inter-agent contributions. Experiments on the VISTA-Beyond dataset demonstrate that MACL significantly improves performance in both few-shot and zero-shot settings, achieving 1-5% precision gains across diverse visual domains.

</details>


### [51] [When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making](https://arxiv.org/abs/2601.10102)
*Viswonathan Manoranjan,Snehalkumar `Neil' S. Gaikwad*

Main category: cs.MA

TL;DR: 设计选项（角色、收益显示）深刻影响大模型多智能体推理表现。Qwen模型对这两因子极度敏感，只有同时移除角色和给定明确收益才具备战略推理能力；角色优先驱动社会偏好决策，且各模型之间差异显著。


<details>
  <summary>Details</summary>
Motivation: 探究大模型在多智能体系统中的推理机制：是以战略合理性为主，还是以角色身份（人物设定）为主，尤其考察设计选择如何影响其推理与决策。

Method: 系统对比不同大模型（Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B）在多智能体决策任务中的表现，操控“角色设定”和“收益可见性”等变量，观察纳什均衡的达成情况。

Result: Qwen系列模型在移除角色设置并显示明确收益后，能显著提升纳什均衡达成率，展现战略推理能力；反之，添加角色则显著改变均衡选择方向，倾向“绿色转型”类社会偏好，而当“公地悲剧”最优时则完全无法达成均衡。不同模型对角色设定和收益显示敏感性不同：Qwen极其敏感，Llama与Mistral则更僵化，几乎不受干扰。

Conclusion: 多智能体系统的大模型推理特性深受表示层（角色、收益显示）设计影响，其决定了模型是战略行为者还是身份驱动者，对真实部署有治理层面的重要启示。

Abstract: Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.

</details>


### [52] [TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems](https://arxiv.org/abs/2601.10120)
*Rui Sun,Jie Ding,Chenghua Gong,Tianjun Gu,Yihang Jiang,Juyuan Zhang,Liming Pan,Linyuan Lü*

Main category: cs.MA

TL;DR: TopoDIM是一种针对LLM多智能体系统的通信拓扑优化框架，能够一次性生成多样化的交互方式，实现高效通信。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统的通信多依赖于时空交互，导致高延迟和计算成本。受评估与辩论机制启发，作者希望通过优化通信拓扑提升集体智能。

Method: 提出了TopoDIM框架，该方法无需多轮迭代协调，通过多样化交互模式让智能体自主构建异质的通信拓扑，实现分布式执行以提升隐私和适应性。

Result: Topodim相比主流方法，token消耗下降46.41%，性能提升1.5%，并且在异质智能体环境下适应性强。

Conclusion: TopoDIM框架显著减少了总token消耗，同时略微提升了任务表现，展示了其在异构智能体通信组织中的强适应性。

Abstract: Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/

</details>


### [53] [Fairness Driven Multi-Agent Path Finding Problem](https://arxiv.org/abs/2601.10123)
*Aditi Anand,Dildar Ali,Suman Banerjee*

Main category: cs.MA

TL;DR: 针对MAPF问题，分别对非理性与理性智能体提出了高效而公平的路径分配方法，理论上验证了机制的激励相容性和占优策略性，并在多方法测试下表现优异。


<details>
  <summary>Details</summary>
Motivation: MAPF问题在机器人运动规划与无人机空域分配等实际场景中广泛存在，但因其计算复杂且涉及智能体可能虚报私有信息，亟需兼顾效率与公平性的解决方案。

Method: 本文针对多智能体路径规划（MAPF）问题，分别在非理性与理性智能体条件下提出了解决方法。对于非理性智能体，设计了启发式求解算法；对于理性智能体，开发了一种机制，证明其具备占优策略、激励相容性和个体理性。

Result: 提出的方法在多种求解方法下验证了有效性与高效性，能够在考虑公平性的前提下有效地为多智能体分配非冲突路径。

Conclusion: 该研究在MAPF领域填补了公平性与策略性问题的空白，提出的机制适用于实际多智能体系统，并具备良好的理论性质和应用潜力。

Abstract: The Multi-Agent Path Finding (MAPF) problem aims at finding non-conflicting paths for multiple agents from their respective sources to destinations. This problem arises in multiple real-life situations, including robot motion planning and airspace assignment for unmanned aerial vehicle movement. The problem is computationally expensive, and adding to it, the agents are rational and can misreport their private information. In this paper, we study both variants of the problem under the realm of fairness. For the non-rational agents, we propose a heuristic solution for this problem. Considering the agents are rational, we develop a mechanism and demonstrate that it is a dominant strategy, incentive compatible, and individually rational. We employ various solution methodologies to highlight the effectiveness and efficiency of the proposed solution approaches.

</details>


### [54] [Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems](https://arxiv.org/abs/2601.10560)
*Xi Shi,Mengxin Zheng,Qian Lou*

Main category: cs.MA

TL;DR: 提出了一种并行延迟优化的多智能体系统编排框架（LAMaS），显著降低了关键延迟并保持性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统多假设顺序执行或不显式关注并行延迟，导致高推理延迟和有限的可扩展性。LAMaS致力于解决并行场景下的延迟问题，提升时延敏感场景中的系统实用性。

Method: 采用基于学习的系统编排，在多智能体架构内引入显式延迟监督，构建低延迟的执行拓扑图，实现关键路径并行优化。通过实验证明，LAMaS在多个基准任务上优于现有方法。

Result: 本文提出了一种面向并行执行、多智能体系统的编排方法，通过引入显式延迟监督，优化了多智能体系统的关键路径延迟。在多个基准测试上，所提方法（LAMaS）在保持甚至提升任务性能的同时，显著缩短了关键路径长度，相比现有最优方法降低了38-46%。这一成果展示了在设计高效多智能体系统时，显式优化并行执行下的延迟至关重要。

Conclusion: 优化多智能体系统并行执行时应显式考虑延迟，LAMaS能有效缩短延迟且不损失任务性能。

Abstract: Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS

</details>


### [55] [Procedural Fairness in Multi-Agent Bandits](https://arxiv.org/abs/2601.10600)
*Joshua Caiata,Carter Blair,Kate Larson*

Main category: cs.MA

TL;DR: 本文提出并实践了针对MA-MAB的新公平目标——过程公平。实验证明过程公平策略几乎不损害结果公平，却能显著增强决策过程的平等性与代表性。同时，论文强调不同公平目标间存在本质性冲突，主张程序合法性需成为公平研究重点。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体多臂老虎机(MA-MAB)领域对公平性的研究多关注于结果导向，如最大化社会福利、减少不平等或均衡效用，但心理学、经济学及罗尔斯理论等领域证据表明公平还涉及程序，即决策过程中每个人的发声权。本文受此启发，重新审视公平的多维内涵。

Method: 通过修正多智能体多臂老虎机问题，设定过程公平作为决策目标，并在实验中与传统结果公平方法横向对比，辅以理论分析揭示多种公平目标的本质区别。

Result: 实验显示，单纯优化结果型公平(如平等或效用最大化)会牺牲个体在决策过程中的平等发言权和代表性；而采用过程公平策略时，对基于结果的公平性损失极小。此外，理论证明过程公平与其他公平概念之间存在根本性、不可兼容的价值取向差异。

Conclusion: 过程公平为所有智能体提供平等决策权，在保证结果公平的前提下显著提升代表性。不同公平理论间的不可兼容性要求在公平设计时做出明确规范选择。过程合法性应被纳入公平性主流研究与实现框架中。

Abstract: In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcomes. Empirical results confirm that fairness notions based on optimizing for outcomes sacrifice equal voice and representation, while the sacrifice in outcome-based fairness objectives (like equality and utilitarianism) is minimal under procedurally fair policies. We further prove that different fairness notions prioritize fundamentally different and incompatible values, highlighting that fairness requires explicit normative choices. This paper argues that procedural legitimacy deserves greater focus as a fairness objective, and provides a framework for putting procedural fairness into practice.

</details>
