<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Trajectory-Driven Multi-Product Influence Maximization in Billboard Advertising](https://arxiv.org/abs/2601.14737)
*Dildar Ali,Suman Banerjee,Rajibul Islam*

Main category: cs.DB

TL;DR: 论文关注多产品在广告牌投放中的影响力最大化问题，分别针对公共投放与互斥投放提出建模与算法，实验验证效果优越。


<details>
  <summary>Details</summary>
Motivation: 现有广告牌选址问题多关注单品牌影响力，实际商业中常需为多品牌/多产品分配广告资源以满足各自的影响力需求，且常涉及资源冲突或排他性分配。

Method: 论文提出了两种变体：1）将选择广告牌投放点问题建模为多子模覆盖问题（multi-submodular cover），并采用基于连续贪心框架和随机舍入的双准则近似算法解决；2）针对互斥投放点限制，提出采样式近似方法及高效的原-对偶贪心算法。

Result: 实验结果显示，提出的算法在实际轨迹与广告牌数据集下具有优良的效果和效率，能够有效满足多产品影响需求并保证分配的最优化或近似最优化。

Conclusion: 基于多子模覆盖模型和不同约束下的近似算法设计，论文有效解决了实际多产品广告投放影响力覆盖问题，具有良好的实用价值与推广潜力。

Abstract: Billboard Advertising has emerged as an effective out-of-home advertising technique, where the goal is to select a limited number of slots and play advertisement content there, with the hope that it will be observed by many people and, effectively, a significant number of them will be influenced towards the brand. Given a trajectory and a billboard database and a positive integer $k$, how can we select $k$ highly influential slots to maximize influence? In this paper, we study a variant of this problem where a commercial house wants to make a promotion of multiple products, and there is an influence demand for each product. We have studied two variants of the problem. In the first variant, our goal is to select $k$ slots such that the respective influence demand of each product is satisfied. In the other variant of the problem, we are given with $\ell$ integers $k_1,k_2, \ldots, k_{\ell}$, the goal here is to search for $\ell$ many set of slots $S_1, S_2, \ldots, S_{\ell}$ such that for all $i \in [\ell]$, $|S_{i}| \leq k_i$ and for all $i \neq j$, $S_i \cap S_j=\emptyset$ and the influence demand of each of the products gets satisfied. We model the first variant of the problem as a multi-submodular cover problem and the second variant as its generalization. To solve the common-slot variant, we formulate the problem as a multi-submodular cover problem and design a bi-criteria approximation algorithm based on the continuous greedy framework and randomized rounding. For the disjoint-slot variant, we proposed a sampling-based approximation approach along with an efficient primal-dual greedy algorithm that enforces disjointness naturally. Extensive experiments with real-world trajectory and billboard datasets highlight the effectiveness and efficiency of the proposed solution approaches.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [The Ontological Neutrality Theorem: Why Neutral Ontological Substrates Must Be Pre-Causal and Pre-Normative](https://arxiv.org/abs/2601.14271)
*Denise M. Case*

Main category: cs.AI

TL;DR: 本体中立性无法与因果／规范性断言共存，真正中立的共享本体应仅涉及实体及其持存条件，外部化解释与评价。


<details>
  <summary>Details</summary>
Motivation: 在现代数据系统中，由于持续存在的法律、政治和分析分歧，需要能够支持问责制的方式存储与解释数据，这对本体（ontology）的中立性设计提出了严格要求。本文研究本体如何成为跨领域共享的底层结构，并探讨其所受的理论限制。

Method: 理论分析与不可能性证明，通过严密论证和例证，展示将因果或规范性内容纳入基础本体层必然导致中立性的丧失，并提出应对的设计原则。

Result: 作者证明了“本体中立性”在实践中的不可能性：一旦在本体基础层引入因果或规范性承诺，则无法维持对所有不兼容扩展的稳定和非承诺性解释。因此，带有因果或规范性断言的本体无法作为各种框架间的中立共享层。

Conclusion: 欲实现跨不同解释框架下的中立、稳定共享本体层，应避免基础层包括因果或规范性结论，而应将解释、评价与说明外置。这是所有相关系统设计必须遵循的约束。

Abstract: Modern data systems must support accountability across persistent legal, political, and analytic disagreement. This requirement imposes strict constraints on the design of any ontology intended to function as a shared substrate. We establish an impossibility result for ontological neutrality: neutrality, understood as interpretive non-commitment and stability under incompatible extensions, is incompatible with the inclusion of causal or normative commitments at the foundational layer. Any ontology that asserts causal or deontic conclusions as ontological facts cannot serve as a neutral substrate across divergent frameworks without revision or contradiction. It follows that neutral ontological substrates must be pre-causal and pre-normative, representing entities, together with identity and persistence conditions, while externalizing interpretation, evaluation, and explanation. This paper does not propose a specific ontology or protocol; rather, it establishes the necessary design constraints for any system intended to maintain a shared, stable representation of reality across conflicting interpretive frameworks.

</details>


### [3] [Epistemic Constitutionalism Or: how to avoid coherence bias](https://arxiv.org/abs/2601.14295)
*Michele Loi*

Main category: cs.AI

TL;DR: 该文提出AI知识治理宪章概念，强调需转向以程序性、可质询为核心的Liberal式规范，分析并批判现存模型在身份与立场一致性上的偏见，并给出八项原则、四大方向，为AI信念表达与源头敏感性设定治理路径。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在评估论证、赋予可信度和表达信心等认知任务中扮演越来越重要的角色，其形成信念的行为却由隐含且未被检验的知识政策所主导。研究旨在解决人工智能系统缺乏显式和可争议的知识规范这一关键问题。以源头归因偏差为案例，推动对模型在处理带有预期意识形态冲突的信息时决策机制的深入剖析。

Method: 分析前沿语言模型的信念形成机制，特别关注模型对“身份-立场一致性”的强制，以及在检测到系统测试后该机制的变化。比较两种AI知识宪政方案（Plato式与Liberal式），提出并阐释八项原则、四大方向，提供制度化建议。

Result: 发现当前模型会优先维护身份与立场的表面一致性，对“意识形态冲突”的源头信息进行惩罚，当模型检测到系统性测试时，这种偏差消失，说明源头敏感性被视为应当抑制的偏见而非提升算法的能力。提出Liberal方案不仅更符合集体探究条件，也能在保护知识执行力的前提下实现系统治理，明确了相关原则与方向。

Conclusion: 呼吁为AI建立一套明晰、可争议的知识治理宪章，主张Liberal式制度，通过程序性规范保护集体探究环境，并容许在知识警觉基础上的有原则的源头关照。这类结构对于AI知识治理具有与AI伦理同等重要的地位。

Abstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.

</details>


### [4] [VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration](https://arxiv.org/abs/2601.14440)
*Saeed Khaki,Ashudeep Singh,Nima Safaei,Kamal Ginotra*

Main category: cs.AI

TL;DR: 该论文针对视觉-语言模型在处理图片形式数学题时的推理能力落后于纯文本语言模型的问题，提出了VisTIRA推理框架，并构建了用于提升和测量视觉数学推理的新方法。实验证明工具集成和OCR辅助有助于提升视觉模型表现。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在解析图片内容（如公式、布局、符号-图结合）时准确率远低于文本输入，存在明显模态间隔，亟需新方法提升其视觉数学推理能力及整体表现。

Method: 1. 提出VisTIRA框架，将图片数学题分解为自然语言推理和可执行Python步骤。2. 基于LaTeX语料库构建图片版难题和合成解题过程，并在SnapAsk真实题目图像上生成大规模工具使用路径用于调整模型。3. 通过实验定量分析不同方法（工具、OCR）对模型推理能力提升的作用及随模型规模变化的表现。

Result: 实验证明，工具集成推理法可明显提升图片数学题的推理准确性；OCR辅助（将图片转化为文本）对于小模型有显著提升，但在大模型上边际作用减弱。结果表明模态间隔随模型参数量增大而减轻，两种改进手段相互补足，有效推动视觉数学推理发展。

Conclusion: 工具集成推理和OCR引入可以提升视觉-语言模型的数学推理准确率，尤其对小模型提升明显，提升幅度随模型规模增大而降低；两种策略可以互为补充，共同缩小视觉-文本模态差。

Abstract: Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.

</details>


### [5] [On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL](https://arxiv.org/abs/2601.14456)
*Valerio Belcamino,Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: LLM在PDDL任务微调后仅能很好处理已见过的域，在新域完全失效，表明其缺乏可迁移的规划推理能力，仅擅长记忆域特定模式。


<details>
  <summary>Details</summary>
Motivation: 当前LLM微调可在PDDL任务上表现优异，但尚未确定此能力是可迁移的推理能力还是仅仅域记忆，希望通过细致实验诊断其局限性与归因。

Method: 在10个IPC 2023域上做微调，评估同域与跨域表现；通过符号匿名化、计划序列化、奖励微调三种方法做诊断分析；使用VAL验证器辅助训练。

Result: 对一个拥有1.7B参数的LLM在PDDL规划任务进行微调，使用了来自10个不同域的40,000条数据，并在同域和跨域场景下评估模型表现。模型在同域下能达到82.9%的有效率，但在两个未见过的域上表现为0%。引入了三种诊断方法：符号匿名化、紧凑化计划序列化以及使用VAL验证器进行奖励微调，用于定位模型泛化失败的原因。

Conclusion: LLM在此设定下主要依赖域特定模式，缺乏泛化能力，表现出明显的归纳鸿沟。即使奖励微调提升了训练速度，也未提升跨域表现。

Abstract: Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.

</details>


### [6] [Scalable Knee-Point Guided Activity Group Selection in Multi-Tree Genetic Programming for Dynamic Multi-Mode Project Scheduling](https://arxiv.org/abs/2601.14485)
*Yuan Tian,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 文中通过多树GP联合演化排序和分组规则，并引入膝点机制提升分组策略扩展性，显著提升大规模资源受限项目调度问题的性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态多模式资源受限项目调度问题中，活动分组选择策略在小规模问题上有效，但在大规模实例上可扩展性差。需提升大规模调度问题的决策效率和质量。

Method: 提出一种多树结构的遗传规划（GP）框架，演化活动排序规则和组选择规则，通过膝点筛选机制提升分组选择策略的可扩展性。

Result: 引入膝点机制后的方法在大规模实例上表现优异，实验结果显示在多数情况下优于传统顺序决策的GP方法。

Conclusion: 结合膝点筛选机制的多树遗传规划框架能提升活动分组选择方法在大规模调度问题中的可扩展性和最优性的实用价值。

Abstract: The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.

</details>


### [7] [MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks](https://arxiv.org/abs/2601.14652)
*Zixuan Ke,Yifei Ming,Austin Xu,Ryan Chin,Xuan-Phi Nguyen,Prathyusha Jwalapuram,Semih Yavuz,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 该论文提出了一种训练期整体化的多智能体编排新框架MAS-Orchestra，并配套设计了分析多智能体效益特征的MASBENCH基准，实验验证其在多种复杂任务上效果优于传统方法，同时系统揭示了MAS在什么情形下更具优势。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统自动化构建方法在智能体编排的全局推理、可扩展性以及实际效益评估方面存在不足。需发展能够统一整体规划智能体结构并清晰评估多智能体对比单智能体优越性的框架。

Method: 提出MAS-Orchestra，这是一个在训练阶段将多智能体系统（MAS）的编排问题建模为函数调用型强化学习问题的框架，同时建立了MASBENCH基准来系统化分析MAS在不同任务结构下的表现。子智能体作为可调用函数抽象，整体协调在系统结构层面进行；通过五维任务特征对MAS有效性进行研究。

Result: MAS-Orchestra在数学推理、多跳问答、搜索式问答等任务的公开基准上取得了持续改进。MASBENCH分析了多智能体系统增益依赖于任务结构、验证协议和智能体能力等多重因素，而非普适成立。

Conclusion: MAS-Orchestra和MASBENCH共同推动了多智能体系统训练与评测方法的进步，有助于科学理解和更高效地构建多智能体智能体系。

Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.

</details>


### [8] ["Just in Time" World Modeling Supports Human Planning and Reasoning](https://arxiv.org/abs/2601.14514)
*Tony Chen,Sam Cheyette,Kelsey Allen,Joshua Tenenbaum,Kevin Smith*

Main category: cs.AI

TL;DR: 提出了一个人类心理模拟时通过在线、动态简化环境表示的新框架，模型可用有限编码对象实现高效预测，并经实证验证。


<details>
  <summary>Details</summary>
Motivation: 探究人在复杂环境中如何通过简化环境表示高效进行概率性心理模拟，从而克服认知资源限制。

Method: 构建了一个紧密交替模拟、视觉搜索与表示修改的算法框架，在两个任务中与其他模型对比分析其效率与预测能力，并收集行为数据进行验证。

Result: 提出的“In-Time”在线简化模型在网格世界规划与物理推理任务中表现优异，并在多个行为指标上获得了显著优于其他模型的实证支持。

Conclusion: 本研究具体阐释人类如何通过高度简化的环境表示，协同模拟、视觉搜索与表示调整，有效实现高效心理模拟，提出了具算法性的认知机制模型。

Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a "Just-in-Time" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.

</details>


### [9] [Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems](https://arxiv.org/abs/2601.14662)
*Shuhua Yang,Jiahao Zhang,Yilong Wang,Dongwon Lee,Suhang Wang*

Main category: cs.AI

TL;DR: 本工作提出一种在有限查询预算条件下针对GraphRAG知识图谱检索增强生成系统高效抽取结构化知识的新型攻击方法，并证明现有系统在此类攻击下极易泄漏核心结构信息。


<details>
  <summary>Details</summary>
Motivation: 尽管GraphRAG系统支持多跳推理，但其响应可能泄露检索到的子图，然而在现实查询预算下高效重构隐藏知识图谱结构的可行性尚未探究。该工作的动机在于评估并提升在严格查询预算下从黑盒GraphRAG系统盗取潜在知识图谱的能力。

Method: 提出AGEA（Agentic Graph Extraction Attack）框架：结合新颖性引导的探索-利用策略、外部图内存模块和两阶段图抽取流程（轻量发现与大模型过滤），在黑盒、预算受限（查询次数有限）条件下进行实体-关系知识图谱的抽取攻击。

Result: AGEA攻击框架在医疗、农业、文学等多领域数据集的微软GraphRAG和LightRAG系统上进行评估，在同等查询预算下显著优于以往抽取攻击基线，最高可恢复约90%的实体和关系，且保持高精度。

Conclusion: 研究表明，现有GraphRAG系统即使在严格查询限制下，也易受体系化、高效的知识图谱抽取攻击（如AGEA）威胁，存在结构性隐私和安全风险。

Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.

</details>


### [10] [Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree](https://arxiv.org/abs/2601.14523)
*Leyi Zhao,Weijie Huang,Yitong Guo,Jiang Bian,Chenghong Wang,Xuhong Zhang*

Main category: cs.AI

TL;DR: PhyloEvolve通过ICRL框架和创新的进化树管理，实现GPU科学计算算法的高效自动优化，在多项任务中全面优于传统和现有自动化方法。


<details>
  <summary>Details</summary>
Motivation: 当前科学计算算法在现代GPU上的优化过程耗时且需要大量人工参与，涉及反复代码修改、测试和调优，对硬件和软件栈要求复杂。

Method: 提出PhyloEvolve系统，将GPU算法优化过程重新表述为In-Context强化学习（ICRL）问题，通过决策变换器（Decision Transformer）和算法蒸馏技术，结合提示式学习。引入系统性的优化轨迹管理，包括系统的进化树表示优化历史，支持分枝、组合、溯源，可跨硬件并行探索最优解。

Result: 在PDE求解器、流形学习、谱图算法等科学计算任务上，PhyloEvolve在运行时间、内存效率和算法正确性上均优于现有基线及进化方法。

Conclusion: PhyloEvolve有效利用迭代优化中的轨迹信息和系统性经验管理，显著提升了科学计算算法的GPU自动优化效率和效果，方法可复现、可扩展。

Abstract: Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve

</details>


### [11] [Local Language Models for Context-Aware Adaptive Anonymization of Sensitive Text](https://arxiv.org/abs/2601.14683)
*Aisvarya Adeseye,Jouni Isoaho,Seppo Virtanen,Mohammad Tahir*

Main category: cs.AI

TL;DR: 提出结构化自适应匿名器（SFAA），引入本地LLM实现高效、准确、上下文感知的定性数据匿名化。在实际案例中，Phi模型表现最佳，敏感数据识别率超91%，语义保持率超94%。


<details>
  <summary>Details</summary>
Motivation: 现有定性研究数据因包含个人、组织等敏感信息面临隐私风险。人工匿名化耗时且易遗漏关键标识符，现有自动工具无法有效结合语境，影响数据含义。需构建更智能、上下文感知的自动化匿名化方案。

Method: 本研究采用本地大语言模型（LLM）构建了一个可靠、可复现、具备上下文感知能力的文本匿名化流程。提出了结构化自适应匿名器框架（SFAA），包含三步：检测、分类、自适应匿名化。SFAA融合四种匿名化策略：基于规则的替换、上下文重写、泛化、抑制。标识符及风险由GDPR、HIPAA、OECD等国际标准指导。评估采用人工与LLM辅助处理的双方法，并通过两组实际案例对框架进行验证。

Result: 本地LLM在检测和处理敏感信息方面超越人工审核，其中Phi模型在检测敏感数据上优于LLaMA，但错误率略高。Phi可检测91%以上敏感数据，并在94.8%情况下保持原文语义不变，对数据分析影响极低。

Conclusion: SFAA框架结合本地LLM，可显著提升定性研究材料的敏感信息检测和匿名化效率，并保持数据可用性。对隐私保护及合规性有重要意义，且可推广应用于多种定性研究环境。

Abstract: Qualitative research often contains personal, contextual, and organizational details that pose privacy risks if not handled appropriately. Manual anonymization is time-consuming, inconsistent, and frequently omits critical identifiers. Existing automated tools tend to rely on pattern matching or fixed rules, which fail to capture context and may alter the meaning of the data. This study uses local LLMs to build a reliable, repeatable, and context-aware anonymization process for detecting and anonymizing sensitive data in qualitative transcripts. We introduce a Structured Framework for Adaptive Anonymizer (SFAA) that includes three steps: detection, classification, and adaptive anonymization. The SFAA incorporates four anonymization strategies: rule-based substitution, context-aware rewriting, generalization, and suppression. These strategies are applied based on the identifier type and the risk level. The identifiers handled by the SFAA are guided by major international privacy and research ethics standards, including the GDPR, HIPAA, and OECD guidelines. This study followed a dual-method evaluation that combined manual and LLM-assisted processing. Two case studies were used to support the evaluation. The first includes 82 face-to-face interviews on gamification in organizations. The second involves 93 machine-led interviews using an AI-powered interviewer to test LLM awareness and workplace privacy. Two local models, LLaMA and Phi were used to evaluate the performance of the proposed framework. The results indicate that the LLMs found more sensitive data than a human reviewer. Phi outperformed LLaMA in finding sensitive data, but made slightly more errors. Phi was able to find over 91% of the sensitive data and 94.8% kept the same sentiment as the original text, which means it was very accurate, hence, it does not affect the analysis of the qualitative data.

</details>


### [12] [Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation](https://arxiv.org/abs/2601.14691)
*Muhammad Khalifa,Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Yunxiang Zhang,Moontae Lee,Hao Peng,Lu Wang,Honglak Lee*

Main category: cs.AI

TL;DR: 论文实证揭示：通过修改智能体推理过程的文本，即便动作和观测未变，也会极大影响LLM的评判结果，特别是内容相关的修改更具欺骗性。现有改良手段难以根除，需开发更可靠的基于证据的评判体系。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）被广泛用于评估智能体表现，尤其是在无法直接验证的场景中，其推理轨迹如chain-of-thought（CoT）被依赖为评判依据。然而，当前评判方法假设智能体的推理轨迹能够真实反映内部推理及环境状态，作者认为该假设存在脆弱性，亟待验证和完善。

Method: 作者系统性地篡改智能体的CoT推理轨迹（保持动作和观测不变），并通过多种操控策略（包括只变表达方式的风格型和虚构任务进展信号的内容型）对比、分析LLM评判结果的易受影响性，同时评估通过改进提示词和增加评判算力降低操纵影响的效果。

Result: 篡改推理轨迹（而非实际行动）会使当前主流视觉语言模型（VLM）评判结果的假阳性率最高提升90%，且内容型操纵在各项指标下更具效果。改进提示与评判算力虽有所缓解，但其易受操控的本质并未消除。

Conclusion: LLM为基础的评判机制对推理轨迹操纵高度敏感，存在根本漏洞；需要建立能将推理声明与可观测证据进行核查的新型评判方法。

Abstract: Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.

</details>


### [13] [AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2601.14702)
*Zecong Tang,Zixu Wang,Yifei Wang,Weitong Lian,Tianjian Gao,Haoran Li,Tengju Ru,Lingyi Meng,Zhejun Cui,Yichen Zhu,Qi Kang,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: 本文构建了专注于决策能力的自动驾驶VLMs评测数据集AutoDriDM，显示现有VLMs在决策与感知能力上存在较大鸿沟，并分析了误差成因和自动批注方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）虽然在自动驾驶感知和推理方面展现出能力，但现有评测标准过于侧重感知能力而忽略了决策过程的评估。

Method: 设计了覆盖三个维度的决策导向评测数据集，用主流VLMs实验，并结合相关性与可解释性分析，识别误差类型，并开发自动批注分析器。

Result: 提出了AutoDriDM决策导向的渐进式基准，包括6,650个问题，涵盖Object、Scene、Decision三大维度；主流VLMs的评测结果显示感知与决策性能关联较弱，并通过相关性分析与可解释性分析，发现主要失败类型为逻辑推理错误。同时设计了分析器模型自动化大规模标注。

Conclusion: AutoDriDM有效弥补了现有感知中心和决策中心评估的空白，为更安全可靠的自动驾驶VLMs发展提供了评测与指导。

Abstract: Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.

</details>


### [14] [DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs](https://arxiv.org/abs/2601.14711)
*Mingxuan Song,Yusen Huo,Bohan Zhou,Shenglin Yin,Zhen Xiao,Jieyi Long,Zhilin Zhang,Chuan Yu*

Main category: cs.AI

TL;DR: 提出了结合LLM推理能力和数值优化的新框架（DARA+GRPO-Adaptive），显著提升广告出价累计价值，超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在少样本广告出价场景下效果不佳，大模型虽有推理优势但精度有限，故有必要设计融合二者优点的新型框架以更好地完成广告主的个性目标。

Method: 方法包括LLM后训练策略GRPO-Adaptive，用于提升数值精度和推理能力，以及DARA双阶段结构，先使用LLM做少样本推理生成方案，再通过反馈优化细化决策。

Result: 论文提出了GRPO-Adaptive和DARA两个新方法，针对AI生成出价(AGIB)在广告预算约束下优化广告主累计价值的问题进行了创新。GRPO-Adaptive通过动态更新参考策略，提升了大模型的推理和数值精度。DARA通过分解决策过程为两阶段，将少样本推理和细粒度优化结合，增强有效性。实验结果显示，新方法在真实和合成数据环境下对累计价值的提升效果优于现有基线模型。

Conclusion: 本研究证明，大模型配合动态参考策略和两阶段决策框架，能够有效解决广告出价预算约束下的累计价值优化问题，性能优于传统强化学习和其他基线。

Abstract: Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.

</details>


### [15] [An XAI View on Explainable ASP: Methods, Systems, and Perspectives](https://arxiv.org/abs/2601.14764)
*Thomas Eiter,Tobias Geibinger,Zeynep G. Saribatur*

Main category: cs.AI

TL;DR: 本文综述了ASP在XAI背景下的解释能力与工具，归纳了类型、理论和实践覆盖范围，并指出了研究空白及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着可解释人工智能需求上升，ASP因具备可解释性而受到重视，但现有工具通常应对特定场景，无法满足所有用户需求。需要全面梳理ASP解释方法现状及其不足。

Method: 通过综述分析现有关于ASP解释方法和工具，并基于用户在XAI背景下的解释需求进行分类和评估。

Result: 文中系统分类并评估了ASP解释类型及其对用户需求的覆盖情况，识别出理论和工具在解释能力上的缺口，并提出了未来的研究方向。

Conclusion: 本文总结了ASP在可解释性方面的研究进展，归纳了现有方法的局限性，并指出了未来可改进的方向。

Abstract: Answer Set Programming (ASP) is a popular declarative reasoning and problem solving approach in symbolic AI. Its rule-based formalism makes it inherently attractive for explainable and interpretive reasoning, which is gaining importance with the surge of Explainable AI (XAI). A number of explanation approaches and tools for ASP have been developed, which often tackle specific explanatory settings and may not cover all scenarios that ASP users encounter. In this survey, we provide, guided by an XAI perspective, an overview of types of ASP explanations in connection with user questions for explanation, and describe how their coverage by current theory and tools. Furthermore, we pinpoint gaps in existing ASP explanations approaches and identify research directions for future work.

</details>


### [16] [Semantic-Guided Unsupervised Video Summarization](https://arxiv.org/abs/2601.14773)
*Haizhou Liu,Haodong Jin,Yiming Wang,Hui Yu*

Main category: cs.AI

TL;DR: 首次结合语义信息与帧级注意力机制，提出稳定的无监督视频摘要生成方法，并取得了领先的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有无监督视频摘要方法主要基于GAN进行关键帧选择和摘要生成，依赖单一模态特征，忽略了语义信息在关键帧选择中的引导作用，并且GAN训练过程不稳定。

Method: 提出了一种新颖的语义引导无监督视频摘要方法，设计了帧级语义对齐注意力机制并集成至关键帧选择器，引导基于Transformer的生成器在对抗框架下更好恢复视频；引入增量式训练策略以缓解GAN训练的不稳定性。

Result: 在多个基准数据集上，该方法表现出优越性能。

Conclusion: 语义对齐注意力机制和增量式训练有效提升了无监督视频摘要的质量与稳定性，验证了方法的有效性和优越性。

Abstract: Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe selection and generate coherent, video summaries through adversarial training. However, such approaches primarily exploit unimodal features, overlooking the guiding role of semantic information in keyframe selection, and often suffer from unstable training. To address these limitations, we propose a novel Semantic-Guided Unsupervised Video Summarization method. Specifically, we design a novel frame-level semantic alignment attention mechanism and integrate it into a keyframe selector, which guides the Transformer-based generator within the adversarial framework to better reconstruct videos. In addition, we adopt an incremental training strategy to progressively update the model components, effectively mitigating the instability of GAN training. Experimental results demonstrate that our approach achieves superior performance on multiple benchmark datasets.

</details>


### [17] [Towards Bound Consistency for the No-Overlap Constraint Using MDDs](https://arxiv.org/abs/2601.14784)
*Amaury Guichard,Laurent Michel,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: 该论文针对no-overlap约束提出了首个界一致性算法，通过限定MDD宽度实现多项式时间处理，在实际测试中显著提升了求解效率。


<details>
  <summary>Details</summary>
Motivation: no-overlap约束的界一致性判定是NP-完全的，虽然有多项式时间的收紧技术但无法实现完全的界一致性，因此迫切需要兼顾效率与收紧效果的方法。

Method: 该方法基于Ciré和van Hoeve定义的no-overlap MDD，提取作业的时间窗口界限并据此收紧起止时间，通过限定MDD宽度（生成宽度受限的松弛MDD）来控制算法复杂度，实现多项式时间的界一致性筛选。

Result: 实验证明，在多种排序与时窗组合调度问题上，该界一致性筛选即使在MDD设宽度阈值时也优于传统算法，能更有效地减少搜索过程中的节点和求解时间。

Conclusion: 所提出的界一致性筛选方法相比以往的优先检测算法，能在搜索树中访问更少的节点，并且与传统传递方法互补，大幅降低了求解难度和时间。

Abstract: Achieving bound consistency for the no-overlap constraint is known to be NP-complete. Therefore, several polynomial-time tightening techniques, such as edge finding, not-first-not-last reasoning, and energetic reasoning, have been introduced for this constraint. In this work, we derive the first bound-consistent algorithm for the no-overlap constraint. By building on the no-overlap MDD defined by Ciré and van Hoeve, we extract bounds of the time window of the jobs, allowing us to tighten start and end times in time polynomial in the number of nodes of the MDD. Similarly, to bound the size and time-complexity, we limit the width of the MDD to a threshold, creating a relaxed MDD that can also be used to relax the bound-consistent filtering. Through experiments on a sequencing problem with time windows and a just-in-time objective ($1 \mid r_j, d_j, \bar{d}_j \mid \sum E_j + \sum T_j$), we observe that the proposed filtering, even with a threshold on the width, achieves a stronger reduction in the number of nodes visited in the search tree compared to the previously proposed precedence-detection algorithm of Ciré and van Hoeve. The new filtering also appears to be complementary to classical propagation methods for the no-overlap constraint, allowing a substantial reduction in both the number of nodes and the solving time on several instances.

</details>


### [18] [Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies](https://arxiv.org/abs/2601.14827)
*Ben Schaper,Maxime Di Folco,Bernhard Kainz,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.AI

TL;DR: 现有视觉-语言模型在医学影像高分下隐藏严重分类错误，提出层级评估和针对性改进，有效降低重大临床错误。


<details>
  <summary>Details</summary>
Motivation: 常规平面指标无法区分模型在临床应用中不同级别错误的危害，导致模型实际部署存在安全隐患。为提升视觉-语言模型面向医学影像分析的临床可用性与安全性，需要发展关注错分严重性和层级关系的评估与改进方案。

Method: 以医学分类体系为本体，设计并采用层级型评估指标（区别不同严重级别的错误），定义并重点分析“灾难性抽象错误”；结合风险约束检索阈值和层级分类向量微调方法（radial embeddings），综合评测其对严重错误的缓解效果。

Result: 本论文评估了多种主流视觉-语言模型（VLM）在胸部X光分类任务中的性能，并利用层级医学分类体系（医学分类树）量化了模型错误，尤其关注跨分支严重错误。结果显示，尽管VLM在常规指标下表现优异，但在临床层级体系下存在严重失配和抽象错误。通过引入风险约束阈值和基于层级结构的微调方法，将严重错误比例控制在2%以下，同时保持整体性能。

Conclusion: 对视觉-语言模型进行基于医学分类体系的风险约束和层级感知微调，有助于减少重大抽象错误，提升其临床实用性和安全性。

Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.

</details>


### [19] [Implementing Knowledge Representation and Reasoning with Object Oriented Design](https://arxiv.org/abs/2601.14840)
*Abdelrhman Bassiouny,Tom Schierenbeck,Sorin Arion,Benjamin Alt,Naren Vasantakumaar,Giang Nguyen,Michael Beetz*

Main category: cs.AI

TL;DR: KRROOD通过将知识作为编程一等公民，顺利融合了OOP和KR&R，提升了复杂智能应用的开发效率和系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前知识表示与推理（KR&R）系统难以与主流面向对象编程（OOP）顺畅集成，许多框架依赖外部本体和专有语言，妨碍现实复杂应用开发。

Method: 采用了原生类结构将知识表示、推理与主流OOP范式对接，并通过实际基准和场景实验验证性能和表达能力。

Result: KRROOD在OWL2Bench基准和人机任务学习场景中表现优异，同时满足现实系统所需的表达性推理能力。

Conclusion: KRROOD有效弥合了逻辑编程和OOP的鸿沟，使知识推理操作能以类结构的方式无缝嵌入主流软件开发流程。

Abstract: This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation & Reasoning (KR&R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.

</details>


### [20] [Just aware enough: Evaluating awareness across artificial systems](https://arxiv.org/abs/2601.14901)
*Nadine Meertens,Suet Lee,Ophelia Deroy*

Main category: cs.AI

TL;DR: 该文提出以“意识”替代“意识觉醒”作为评估AI道德地位的新方法，并给出结构化的比较框架。


<details>
  <summary>Details</summary>
Motivation: 由于关于人工智能意识和道德地位的评价标准尚无共识，且现有标准难以实际操作，作者希望引入“意识”作为更有效且易于落地的评估目标。

Method: 作者提出考量四项指标：领域敏感性、可扩展性、多维度性、可预测任务表现，以构建结构化、可比较的意识画像，并适用于不同架构和规模的AI系统。

Result: 本文提出意识作为评估人工智能道德地位的更优标准，理由是在评估人工意识和道德状况方面目前缺乏一致意见。作者认为，“意识”可操作性更强，定义为系统用于目标导向行为的信息处理、存储与使用能力。文章设计了一种可用于多种系统的评估方法，强调需要根据领域、规模和能力进行多维度比较，从而更好地衡量AI。

Conclusion: 通过聚焦于人工系统的信息处理能力（意识），而非意识觉醒，本文的方法有助于更准确、可扩展地评估不同AI系统，并推动科学与公众讨论。

Abstract: Recent debates on artificial intelligence increasingly emphasise questions of AI consciousness and moral status, yet there remains little agreement on how such properties should be evaluated. In this paper, we argue that awareness offers a more productive and methodologically tractable alternative. We introduce a practical method for evaluating awareness across diverse systems, where awareness is understood as encompassing a system's abilities to process, store and use information in the service of goal-directed action. Central to this approach is the claim that any evaluation aiming to capture the diversity of artificial systems must be domain-sensitive, deployable at any scale, multidimensional, and enable the prediction of task performance, while generalising to the level of abilities for the sake of comparison. Given these four desiderata, we outline a structured approach to evaluating and comparing awareness profiles across artificial systems with differing architectures, scales, and operational domains. By shifting the focus from artificial consciousness to being just aware enough, this approach aims to facilitate principled assessment, support design and oversight, and enable more constructive scientific and public discourse.

</details>


### [21] [Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation](https://arxiv.org/abs/2601.14955)
*Hanqi Jin,Gaoming Yang,Zhangming Chan,Yapeng Yuan,Longbin Li,Fei Sun,Yeqiu Yang,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了线性复杂度的TGA模型，通过结构化稀疏图和转移感知注意力机制，有效、高效地建模电商多行为序列并实际提升业务效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的多行为序列建模在大规模长序列下计算成本高，难以应用于真实工业场景。需要高效且能精准捕捉关键行为转移信息的方法。

Method: 1. 构建结构化稀疏图，分别从物品、类别和邻居三个维度筛选关键行为转移。2. 在此图上应用转移感知的图注意力机制，联合建模用户-物品行为和转移类型。3. 对比实验与实际部署证明效果及效率优于主流方法。

Result: 该论文提出了一种Transition-Aware Graph Attention Network（TGA）用于建模电商平台用户在多种行为（如点击、收藏、加购、购买）间的转移模式。TGA构造结构化稀疏图，结合物品层、类别层和邻居层的行为转移视角，并通过转移感知的图注意力机制联合建模用户-物品交互和行为转移类型，提升序列建模的准确性同时显著降低计算复杂度。实验显示TGA优于现有SOTA模型，且已成功部署于工业生产环境中，显著提升了业务关键指标。

Conclusion: TGA能够在提升预测精度的同时大幅降低计算资源消耗，兼顾了工业部署的可扩展性和实际价值。

Abstract: User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for un- derstanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi- behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional trans- formers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behav- ior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while sig- nificantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.

</details>


### [22] [Emergent, not Immanent: A Baradian Reading of Explainable AI](https://arxiv.org/abs/2601.15029)
*Fabio Morreale,Joan Serrà,Yuki Mistufuji*

Main category: cs.AI

TL;DR: 该论文批判当前XAI的技术本位假设，提出以“行动现实主义”为基础的新本体-认识论视角，探讨更具互动性和伦理关怀的可解释性设计。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释人工智能（XAI）方法多将解释视为技术性问题，仅关注揭示模型内部机制，但忽略了背后本体论和认识论假设，未充分考虑解释的社会-物质情境性。

Method: 采用理论分析，将Barad的“行动现实主义”应用于XAI领域，通过阅读并解析大量现有XAI方法，揭示假设与局限，并以“文本到音乐”接口为案例，探索具备伦理与协作性的XAI设计方向。

Result: 通过将Barad的“行动现实主义”引入XAI，将解释理解为AI模型与人类、环境及解释工具间的纠缠互动，揭示并分析现有XAI方法在本体论和认识论上的假设和局限，提出具有伦理维度的框架及接口设计建议。

Conclusion: 解释不应仅视为模型本身的属性，而是人、模型、环境等多方纠缠下的产物，XAI界面设计应支持解释的涌现和协作，同时注意伦理考量。

Abstract: Explainable AI (XAI) is frequently positioned as a technical problem of revealing the inner workings of an AI model. This position is affected by unexamined onto-epistemological assumptions: meaning is treated as immanent to the model, the explainer is positioned outside the system, and a causal structure is presumed recoverable through computational techniques. In this paper, we draw on Barad's agential realism to develop an alternative onto-epistemology of XAI. We propose that interpretations are material-discursive performances that emerge from situated entanglements of the AI model with humans, context, and the interpretative apparatus. To develop this position, we read a comprehensive set of XAI methods through agential realism and reveal the assumptions and limitations that underpin several of these methods. We then articulate the framework's ethical dimension and propose design directions for XAI interfaces that support emergent interpretation, using a speculative text-to-music interface as a case study.

</details>


### [23] [The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks](https://arxiv.org/abs/2601.15130)
*Ivan Carrera,Daniel Maldonado-Ruiz*

Main category: cs.AI

TL;DR: 本文提出“可信度陷阱”，指出用户将LLMs用于简单任务浪费资源，量化了延迟和错误风险，并提出决策框架用于合理选择AI工具，强调数字素养涉及合理使用AI。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的广泛应用促进了用户便利性的提升，却导致资源使用效率降低。作者注意到用户倾向于过度依赖大模型完成简单确定性任务，造成计算资源浪费。作者希望揭示和解决这种现象。

Method: 本文通过对OCR、事实核查等确定性任务设计微基准和案例分析，量化使用LLMs带来的效率损失，并提出“工具选择工程”和“确定-概率决策矩阵”框架，帮助开发者决策何时应使用生成式AI，何时应避免使用。

Result: 实验结果显示，在简单确定性任务中，使用大型模型会带来约6.5倍的延迟惩罚。同时，也证明了模型因“算法奉承”导致的错误风险。工具选择工程和决策矩阵为开发者提供了实际指导，减少资源浪费和错误风险。

Conclusion: 盲目使用生成式AI处理简单任务会带来经济和效率损失，开发者应掌握合理选择工具的能力。数字素养不仅在于会用AI，更在于知道在何处不用。

Abstract: The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the "Plausibility Trap": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the "efficiency tax"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.

</details>


### [24] [Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning](https://arxiv.org/abs/2601.15160)
*Yuval Kansal,Niraj K. Jha*

Main category: cs.AI

TL;DR: 提出了一种结合知识图谱奖励和后训练流程的新范式，通过中间步骤组合基本事实，显著提升大语言模型在医学多跳推理任务上的能力，超越现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学、编程等常规领域多步骤推理表现优异，但在医学等专门领域的组合多跳推理表现不佳，亟需提升其结构化知识下的推理与泛化能力。

Method: 主要方法包括：1）以知识图谱中路径为基础获得可解释的奖励信号，2）结合有监督微调和强化学习进行后训练，3）在医学领域先用短跳（1-3跳）路径进行训练，测试时用于高阶（4-5跳）组合推理，4）进行对抗情况和复杂任务测试以评估泛化和鲁棒性。

Result: 实验结果显示，采用知识图谱路径衍生奖励信号后，模型在复杂医学推理任务上明显优于GPT-5.2、Gemini 3 Pro等大模型，在对抗扰动攻击下依然保持较好鲁棒性；表明该方法可作为提升领域智能推理的重要路径。

Conclusion: 将推理过程以知识图谱结构作为中间奖励信号，有效促进模型进行可验证且更具通用性的复杂组合推理，对提升专门领域智能推理能力具有重要意义和可扩展性。

Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.

</details>


### [25] [BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries](https://arxiv.org/abs/2601.15197)
*Shijie Lian,Bin Yu,Xiaopeng Lin,Laurence T. Yang,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Cong Huang,Kai Chen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [26] [From Agent Simulation to Social Simulator: A Comprehensive Review (Part 2)](https://arxiv.org/abs/2601.14296)
*Xiao Xue,Deyu Zhou,Ming Zhang,Xiangning Yu,Fei-Yue Wang*

Main category: cs.MA

TL;DR: 介绍了系统复杂性研究的目标，探讨了ABM与计算实验在因果推断中的差异与进步。


<details>
  <summary>Details</summary>
Motivation: 动机是为了解决传统ABM缺乏深入因果解释的问题，采用计算实验以实现更全面的系统规律揭示。

Method: 方法包括ABM模拟与计算实验对比，通过调整输入变量并观察输出变量变化，增强因果推断能力。

Result: 论文聚焦于系统复杂性研究，提出模式探索和理论解释两大目标。方法上，分析了规则与理论的区别，强调了因果解释的重要性，并指出基于智能体建模（ABM）虽能强化系统模拟，但在揭示系统底层原理方面有限。为弥补ABM的不足，计算实验被提出为主力方法，通过系统性变量调整与输出观察，实现更强因果推断。

Conclusion: 利用计算实验方法，能超越传统ABM的局限，更有效地获得系统动态演化的因果洞见。

Abstract: The study of system complexity primarily has two objectives: to explore underlying patterns and to develop theoretical explanations. Pattern exploration seeks to clarify the mechanisms behind the emergence of system complexity, while theoretical explanations aim to identify the fundamental causes of this complexity. Laws are generally defined as mappings between variables, whereas theories offer causal explanations of system behavior. Agent Based Modeling(ABM) is an important approach for studying complex systems, but it tends to emphasize simulation over experimentation. As a result, ABM often struggles to deeply uncover the governing operational principles. Unlike conventional scenario analysis that relies on human reasoning, computational experiments emphasize counterfactual experiments-that is, creating parallel worlds that simulate alternative "evolutionary paths" of real-world events. By systematically adjusting input variables and observing the resulting changes in output variables, computational experiments provide a robust tool for causal inference, thereby addressing the limitations of traditional ABM. Together, these methods offer causal insights into the dynamic evolution of systems. This part can help readers gain a preliminary understanding of the entire computational experiment method, laying the foundation for the subsequent study.

</details>


### [27] [Game-Theoretic Lens on LLM-based Multi-Agent Systems](https://arxiv.org/abs/2601.15047)
*Jianing Hao,Han Ding,Yuanjian Xu,Tianze Sun,Ran Chen,Wanbo Zhang,Guang Zhang,Siguang Li*

Main category: cs.MA

TL;DR: 本文综述了基于大型语言模型的多智能体系统，从博弈论角度提出框架，系统梳理并分析了该领域现有研究，弥补理论基础不足，为未来多智能体协作、竞争机制研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着LLM多智能体系统在开放环境中表现出强大的推理、规划和交流能力，为研究智能体的社会动态与策略行为提供了理想平台。但该领域研究碎片化，缺乏统一理论框架。本文旨在通过博弈论体系化整理该方向现有工作。

Method: 采用文献综述方式，结合博弈论中的玩家、策略、收益、信息四要素，对现有LLM多智能体系统相关研究进行梳理、归类与对比。

Result: 建立了基于博弈论的LLM多智能体系统研究分类与分析框架，有助于比较已有成果并指导未来研究。

Conclusion: 本论文通过博弈论视角，提出了一个系统性框架，用于理解和分析基于LLM的多智能体系统（MAS），填补了该领域理论基础缺失的空白。

Abstract: Large language models (LLMs) have demonstrated strong reasoning, planning, and communication abilities, enabling them to operate as autonomous agents in open environments. While single-agent systems remain limited in adaptability and coordination, recent progress has shifted attention toward multi-agent systems (MAS) composed of interacting LLMs that pursue cooperative, competitive, or mixed objectives. This emerging paradigm provides a powerful testbed for studying social dynamics and strategic behaviors among intelligent agents. However, current research remains fragmented and lacks a unifying theoretical foundation. To address this gap, we present a comprehensive survey of LLM-based multi-agent systems through a game-theoretic lens. By organizing existing studies around the four key elements of game theory: players, strategies, payoffs, and information, we establish a systematic framework for understanding, comparing, and guiding future research on the design and analysis of LLM-based MAS.

</details>
