{"id": "2601.16286", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.16286", "abs": "https://arxiv.org/abs/2601.16286", "authors": ["Varun Chillara", "Dylan Kline", "Christopher Alvares", "Evan Wooten", "Huan Yang", "Shlok Khetan", "Cade Bauer", "Tr\u00e9 Guillory", "Tanishka Shah", "Yashodhara Dhariwal", "Volodymyr Pavlov", "George Popstefanov"], "title": "SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems", "comment": null, "summary": "Agentic AI pipelines suffer from a hidden inefficiency: they frequently reconstruct identical intermediate logic, such as metric normalization or chart scaffolding, even when the user's natural language phrasing is entirely novel. Conventional boundary caching fails to capture this inefficiency because it treats inference as a monolithic black box.\n  We introduce SemanticALLI, a pipeline-aware architecture within Alli (PMG's marketing intelligence platform), designed to operationalize redundant reasoning. By decomposing generation into Analytic Intent Resolution (AIR) and Visualization Synthesis (VS), SemanticALLI elevates structured intermediate representations (IRs) to first-class, cacheable artifacts.\n  The impact of caching within the agentic loop is substantial. In our evaluation, baseline monolithic caching caps at a 38.7% hit rate due to linguistic variance. In contrast, our structured approach allows for an additional stage, the Visualization Synthesis stage, to achieve an 83.10% hit rate, bypassing 4,023 LLM calls with a median latency of just 2.66 ms. This internal reuse reduces total token consumption, offering a practical lesson for AI system design: even when users rarely repeat themselves, the pipeline often does, at stable, structured checkpoints where caching is most reliable.", "AI": {"tldr": "SemanticALLI\u901a\u8fc7\u7ed3\u6784\u5316\u7f13\u5b58\u673a\u5236\u663e\u8457\u63d0\u5347AI\u63a8\u7406\u7ba1\u9053\u7684\u6548\u7387\uff0c\u8fdc\u8d85\u4f20\u7edf\u9ed1\u76d2\u5f0f\u7f13\u5b58\u3002", "motivation": "\u5f53\u524dagentic AI\u56e0\u4e3a\u5ffd\u89c6\u4e2d\u95f4\u73af\u8282\u7684\u91cd\u590d\u903b\u8f91\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u7684\u7aef\u5230\u7aef\u7f13\u5b58\u65b9\u5f0f\u65e0\u6cd5\u5e94\u5bf9\u7528\u6237\u8bed\u53e5\u53d8\u5316\u5e26\u6765\u7684\u4f4e\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u80fd\u7ed3\u6784\u5316\u5904\u7406\u548c\u590d\u7528\u4e2d\u95f4\u7ed3\u679c\u7684\u67b6\u6784\u3002", "method": "\u5c06AI\u63a8\u7406\u6d41\u7a0b\u62c6\u5206\u4e3a\u5206\u6790\u610f\u56fe\u89e3\u6790\uff08AIR\uff09\u548c\u53ef\u89c6\u5316\u5408\u6210\uff08VS\uff09\u4e24\u4e2a\u53ef\u7f13\u5b58\u9636\u6bb5\uff0c\u5e76\u5728\u7ba1\u9053\u4e2d\u5c06\u4e2d\u95f4\u8868\u793a\uff08IRs\uff09\u4f5c\u4e3a\u4e00\u7b49\u7f13\u5b58\u5bf9\u8c61\uff0c\u5b9e\u73b0\u4e86\u5185\u90e8\u903b\u8f91\u7684\u9ad8\u6548\u590d\u7528\u3002\u901a\u8fc7\u5728Alli\u8425\u9500\u667a\u80fd\u5e73\u53f0\u4e2d\u7684\u5e94\u7528\uff0c\u5bf9\u6bd4\u7ed3\u6784\u5316\u7f13\u5b58\u4e0e\u4f20\u7edf\u9ed1\u76d2\u7f13\u5b58\u7684\u547d\u4e2d\u7387\u4e0e\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86SemanticALLI\uff0c\u8fd9\u662f\u4e00\u79cd\u7ba1\u9053\u611f\u77e5\u7684\u67b6\u6784\uff0c\u4e13\u4e3a\u51cf\u5c11agentic AI\u7cfb\u7edf\u5728\u4e2d\u95f4\u903b\u8f91\u5c42\u9762\u4e0a\u7684\u91cd\u590d\u8ba1\u7b97\u800c\u8bbe\u8ba1\u3002\u4f20\u7edf\u7684\u7f13\u5b58\u673a\u5236\u56e0\u628a\u63a8\u7406\u8fc7\u7a0b\u89c6\u4e3a\u9ed1\u76d2\u800c\u5ffd\u7565\u4e86\u4e2d\u95f4\u7ed3\u679c\u7684\u590d\u7528\u6f5c\u529b\uff0c\u9020\u6210\u6548\u7387\u4f4e\u4e0b\u3002SemanticALLI\u901a\u8fc7\u663e\u5f0f\u5212\u5206\u5206\u6790\u610f\u56fe\u89e3\u6790\uff08AIR\uff09\u4e0e\u53ef\u89c6\u5316\u5408\u6210\uff08VS\uff09\u4e24\u4e2a\u9636\u6bb5\uff0c\u5c06\u7ed3\u6784\u5316\u7684\u4e2d\u95f4\u8868\u793a\u63d0\u5347\u4e3a\u53ef\u7f13\u5b58\u7684\u4e00\u7b49\u516c\u6c11\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6781\u5927\u63d0\u9ad8\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u51cf\u5c11\u91cd\u590dLLM\u8c03\u7528\u4e0e\u6574\u4f53\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "\u7ed3\u6784\u5206\u89e3\u548c\u4e2d\u95f4\u7ed3\u679c\u590d\u7528\u663e\u8457\u63d0\u9ad8\u4e86AI\u7ba1\u9053\u7f13\u5b58\u7684\u547d\u4e2d\u7387\u548c\u7cfb\u7edf\u6548\u7387\uff0c\u663e\u793a\u5728AI\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u5173\u6ce8\u7ed3\u6784\u5316\u4e2d\u95f4\u68c0\u67e5\u70b9\u5177\u6709\u91cd\u8981\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2601.16409", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.16409", "abs": "https://arxiv.org/abs/2601.16409", "authors": ["Yeasir Rayhan", "Walid G. Aref"], "title": "Gen-DBA: Generative Database Agents (Towards a Move 37 for Databases)", "comment": null, "summary": "Move\\,37 marks one of the major breakthroughs in AI in terms of its ability to surpass human expertise and discover novel strategies beyond the traditional game play in the strategic two-player board game of Go. The domains of Natural Language Processing, Computer Vision, and Robotics have also undergone a similar phenomenon through the advent of large foundational models in the form of Large Language Models (LLMs), Vision Language Models (VLMs) and Vision Language Action models (VLAs), respectively. In this paper, we investigate the current state of Artificial Intelligence for Database Systems research (AI4DB), and assess how far AI4DB systems are from achieving their own Move\\,37 moment. We envision a Generative Database Agent (Gen-DBA, for short) as the pathway to achieving Move\\,37 for database systems that will bring generative reasoning and creativity into the realm of database learning tasks. This vision paper explores this direction by presenting the recipe for building Gen-DBA that encompasses but is not limited to a Transformer backbone, a hardware-grounded tokenization mechanism, a two-stage Goal-Directed Next Token Prediction training paradigm, and a generative inference process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6570\u636e\u5e93\u7cfb\u7edf\u9886\u57dfAI\u53d6\u5f97\u7a81\u7834\u7684\u6982\u5ff5\uff0c\u7c7b\u6bd4\u56f4\u68cb\u9886\u57df\u7684Move 37\u95ee\u4e16\uff0c\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u751f\u6210\u5f0f\u6570\u636e\u5e93\u667a\u80fd\u4f53\uff08Gen-DBA\uff09\u5b9e\u73b0\u6570\u636e\u5e93\u9886\u57df\u521b\u65b0\u3002", "motivation": "\u968f\u7740AI\u5728\u56f4\u68cb\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u9886\u57df\u53d6\u5f97\u7a81\u7834\uff0c\u8bba\u6587\u5173\u6ce8\u6570\u636e\u5e93\u7cfb\u7edf\u9886\u57df\u672a\u51fa\u73b0\u7c7b\u4f3c\u91cd\u5927\u521b\u65b0\uff08Move 37\u65f6\u523b\uff09\uff0c\u5e0c\u671b\u5f15\u5165\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u63a8\u52a8\u6570\u636e\u5e93\u667a\u80fd\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efaGen-DBA\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u3001\u786c\u4ef6\u76f8\u5173\u7684\u5206\u8bcd\u673a\u5236\u3001\u53cc\u9636\u6bb5\u76ee\u6807\u5bfc\u5411\u7684\u4e0b\u4e00\u4e2aToken\u9884\u6d4b\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ee5\u53ca\u751f\u6210\u5f0f\u63a8\u7406\u6d41\u7a0b\u3002", "result": "\u8be5\u8bba\u6587\u4e3a\u6570\u636e\u5e93\u9886\u57df\u8fc8\u5411\u7a81\u7834\u6027\u521b\u65b0\u63d0\u4f9b\u4e86\u6784\u60f3\u548c\u6280\u672f\u65b9\u6848\uff0c\u5177\u4f53\u5b9e\u73b0\u53ca\u5b9e\u9645\u7a81\u7834\u5c1a\u9700\u540e\u7eed\u5de5\u4f5c\u9a8c\u8bc1\u3002", "conclusion": "\u4f5c\u8005\u8ba4\u4e3a\uff0c\u901a\u8fc7\u6784\u5efa\u751f\u6210\u5f0f\u6570\u636e\u5e93\u667a\u80fd\u4f53\uff08Gen-DBA\uff09\uff0c\u6570\u636e\u5e93\u7cfb\u7edf\u6709\u673a\u4f1a\u83b7\u5f97\u7c7b\u4f3cAI\u5728\u56f4\u68cb\u3001\u81ea\u7136\u8bed\u8a00\u7b49\u9886\u57df\u7684\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u5c42\u6b21\u7684\u81ea\u52a8\u63a8\u7406\u548c\u521b\u9020\u529b\u3002"}}
{"id": "2601.16432", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.16432", "abs": "https://arxiv.org/abs/2601.16432", "authors": ["Udesh Kumarasinghe", "Tyler Liu", "Chunwei Liu", "Walid G. Aref"], "title": "iPDB -- Optimizing SQL Queries with ML and LLM Predicates", "comment": null, "summary": "Structured Query Language (SQL) has remained the standard query language for databases. SQL is highly optimized for processing structured data laid out in relations. Meanwhile, in the present application development landscape, it is highly desirable to utilize the power of learned models to perform complex tasks. Large language models (LLMs) have been shown to understand and extract information from unstructured textual data. However, SQL as a query language and accompanying relational database systems are either incompatible or inefficient for workloads that require leveraging learned models. This results in complex engineering and multiple data migration operations that move data between the data sources and the model inference platform. In this paper, we present iPDB, a relational system that supports in-database machine learning (ML) and large language model (LLM) inferencing using extended SQL syntax. In iPDB, LLMs and ML calls can function as semantic projects, as predicates to perform semantic selects and semantic joins, or for semantic grouping in group-by clauses. iPDB has a novel relational predict operator and semantic query optimizations that enable users to write and efficiently execute semantic SQL queries, outperforming the state-of-the-art.", "AI": {"tldr": "iPDB\u6269\u5c55SQL\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u5e93\u5185\u65e0\u7f1d\u8c03\u7528LLM/ML\u80fd\u529b\uff0c\u5927\u5e45\u63d0\u5347\u8bed\u4e49\u67e5\u8be2\u6548\u7387\uff0c\u51cf\u5c11\u6570\u636e\u8fc1\u79fb\u548c\u5de5\u7a0b\u590d\u6742\u5ea6\u3002", "motivation": "\u5f53\u524dSQL\u53ca\u5176\u5173\u8054\u6570\u636e\u5e93\u7cfb\u7edf\u4e0d\u9002\u5408\u76f4\u63a5\u652f\u6301\u548c\u9ad8\u6548\u5b9e\u73b0\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7b49\u5b66\u4e60\u578b\u6a21\u578b\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5bfc\u81f4\u5e94\u7528\u5f00\u53d1\u9700\u8981\u590d\u6742\u7684\u5de5\u7a0b\u5b9e\u73b0\u548c\u591a\u6b21\u6570\u636e\u8fc1\u79fb\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u65b0\u578b\u5173\u7cfb\u578b\u6570\u636e\u5e93\u7cfb\u7edfiPDB\uff0c\u5728SQL\u4e2d\u96c6\u6210\u4e86ML/LLM\u8c03\u7528\u7684\u6269\u5c55\u8bed\u6cd5\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u5173\u7cfb\u578b\u9884\u6d4b\u7b97\u5b50\u548c\u8bed\u4e49\u67e5\u8be2\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u652f\u6301\u9ad8\u6548\u7684\u8bed\u4e49\u5de5\u7a0b\uff08\u5982\u8bed\u4e49\u9009\u62e9\u3001\u8fde\u63a5\u3001\u5206\u7ec4\u7b49\u64cd\u4f5c\uff09\u3002", "result": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u578b\u5173\u7cfb\u578b\u7cfb\u7edfiPDB\uff0c\u652f\u6301\u901a\u8fc7\u6269\u5c55SQL\u8bed\u6cd5\u5728\u6570\u636e\u5e93\u5185\u9ad8\u6548\u6267\u884c\u673a\u5668\u5b66\u4e60\u548cLLM\u63a8\u7406\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "iPDB\u80fd\u591f\u901a\u8fc7\u6269\u5c55SQL\uff0c\u539f\u751f\u96c6\u6210\u548c\u9ad8\u6548\u5730\u7ba1\u7406\u8bed\u4e49\u63a8\u7406\u4e0e\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u663e\u8457\u5f3a\u5316\u4e86\u5173\u7cfb\u578b\u6570\u636e\u5e93\u5728\u8bed\u4e49\u67e5\u8be2\u548c\u590d\u6742\u6570\u636e\u5904\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2601.16490", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.16490", "abs": "https://arxiv.org/abs/2601.16490", "authors": ["Adam A. E. Alflahi", "Mohammed A. Y. Mohammed", "Abdallah Alsammani"], "title": "A Scalable Transaction Management Framework for Consistent Document-Oriented NoSQL Databases", "comment": null, "summary": "NoSQL databases are widely used in modern applications due to their scalability and schema flexibility, yet they often rely on eventual consistency models that limit reliable transaction processing. This study proposes a four-stage transaction management framework for document-oriented NoSQL databases, with MongoDB as the reference platform. The framework combines transaction lifecycle management, operation classification, pre-execution conflict detection, and an adaptive locking strategy with timeout-based deadlock prevention. Formal correctness analysis shows that the proposed approach guarantees conflict serializability under defined conditions. An experimental evaluation using the Yahoo Cloud Serving Benchmark (YCSB) workloads A, B, and F, with concurrency levels ranging from 1 to 100 clients, demonstrates a reduction in transaction abort rates from 8.3% to 4.7%, the elimination of observed deadlocks, and a 34.2% decrease in latency variance. Throughput improvements ranging from 6.3% to 18.4% are observed under high concurrency, particularly for read-modify-write workloads. Distributed experiments on clusters of up to 9 nodes confirm scalability, achieving 15.2% higher throughput and 53% lower abort rates than baseline systems. Comparisons with MongoDB's native transactions, CockroachDB, and TiDB indicate that the proposed framework strikes a good balance between consistency guarantees and performance overhead. Sensitivity analysis identifies optimal parameter settings, including a lock timeout of 100 ms, an initial backoff of 10 ms, and a maximum backoff of 500 ms. These results show that carefully designed consistency mechanisms can significantly improve data integrity in NoSQL systems without undermining scalability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8eNoSQL\u6570\u636e\u5e93\u7684\u9ad8\u6548\u4e8b\u52a1\u7ba1\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u4e00\u81f4\u6027\u3001\u964d\u4f4e\u4e8b\u52a1\u4e2d\u6b62\u3001\u6d88\u9664\u6b7b\u9501\uff0c\u540c\u65f6\u517c\u987e\u9ad8\u5e76\u53d1\u6027\u80fd\u6269\u5c55\u3002", "motivation": "NoSQL\u6570\u636e\u5e93\u56e0\u5176\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u7ed3\u6784\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6700\u7ec8\u4e00\u81f4\u6027\u6a21\u578b\u9650\u5236\u4e86\u53ef\u9760\u4e8b\u52a1\u5904\u7406\u3002\u672c\u7814\u7a76\u52a8\u56e0\u662f\u6539\u8fdbNoSQL\u6570\u636e\u5e93\u4e2d\u7684\u4e8b\u52a1\u7ba1\u7406\uff0c\u63d0\u9ad8\u4e00\u81f4\u6027\u4e0e\u7cfb\u7edf\u6027\u80fd\u517c\u5bb9\u6027\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e00\u79cd\u5305\u542b\u4e8b\u52a1\u751f\u547d\u5468\u671f\u7ba1\u7406\u3001\u64cd\u4f5c\u5206\u7c7b\u3001\u9884\u6267\u884c\u51b2\u7a81\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u9501\u5b9a\u53ca\u8d85\u65f6\u6b7b\u9501\u9884\u9632\u7684\u4e8b\u52a1\u7ba1\u7406\u6846\u67b6\u3002\u5f62\u5f0f\u5316\u5206\u6790\u51b2\u7a81\u53ef\u4e32\u884c\u5316\uff0c\u5728YCSB\u57fa\u51c6\u4e0b\u4e0e\u4e3b\u6d41NoSQL/\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8fdb\u884c\u53c2\u6570\u7075\u654f\u5ea6\u5206\u6790\u3002", "result": "\u63d0\u51fa\u7684\u56db\u9636\u6bb5\u4e8b\u52a1\u7ba1\u7406\u6846\u67b6\u5728MongoDB\u53ca\u5206\u5e03\u5f0f\u5b9e\u9a8c\u4e2d\uff0c\u5c06\u4e8b\u52a1\u4e2d\u6b62\u7387\u4ece8.3%\u964d\u81f34.7%\uff0c\u6d88\u9664\u6b7b\u9501\uff0c\u964d\u4f4e\u5ef6\u8fdf\u65b9\u5dee34.2%\uff0c\u9ad8\u5e76\u53d1\u4e0b\u7cfb\u7edf\u541e\u5410\u91cf\u63d0\u53476.3%-18.4%\uff0c\u96c6\u7fa4\u89c4\u6a21\u6269\u5c55\u4e0b\u541e\u5410\u91cf\u63d0\u534715.2%\uff0c\u4e2d\u6b62\u7387\u964d\u4f4e53%\u3002\u5bf9\u6bd4MongoDB\u539f\u751f\u4e8b\u52a1\u3001CockroachDB\u548cTiDB\uff0c\u4e0d\u4ec5\u63d0\u4f9b\u8f83\u4f18\u7684\u6570\u636e\u4e00\u81f4\u6027\uff0c\u8fd8\u6709\u6548\u63a7\u5236\u4e86\u6027\u80fd\u5f00\u9500\u3002", "conclusion": "\u901a\u8fc7\u56db\u9636\u6bb5\u4e8b\u52a1\u7ba1\u7406\u3001\u51b2\u7a81\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u9501\u673a\u5236\uff0cNoSQL\u6570\u636e\u5e93\u80fd\u5728\u4fdd\u8bc1\u826f\u597d\u53ef\u6269\u5c55\u6027\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u6570\u636e\u5b8c\u6574\u6027\u548c\u7cfb\u7edf\u6027\u80fd\u3002\u5408\u7406\u53c2\u6570\u914d\u7f6e\u8fdb\u4e00\u6b65\u63d0\u5347\u6574\u4f53\u6548\u679c\uff0c\u5c55\u793a\u4e00\u81f4\u6027\u673a\u5236\u6539\u8fdb\u5bf9NoSQL\u7cfb\u7edf\u7684\u5de8\u5927\u4ef7\u503c\u3002"}}
{"id": "2601.16344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16344", "abs": "https://arxiv.org/abs/2601.16344", "authors": ["Fan Nie", "Junlin Wang", "Harper Hua", "Federico Bianchi", "Yongchan Kwon", "Zhenting Qi", "Owen Queen", "Shang Zhu", "James Zou"], "title": "DSGym: A Holistic Framework for Evaluating and Training Data Science Agents", "comment": null, "summary": "Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DSGym\uff0c\u4e00\u4e2a\u6807\u51c6\u5316\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u79d1\u5b66\u667a\u80fd\u4f53\u8bc4\u6d4b\u4e0e\u8bad\u7ec3\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u4e30\u5bcc\u4efb\u52a1\u548c\u81ea\u52a8\u9a8c\u8bc1\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\u53d6\u5f97\u8d85\u8d8aGPT-4o\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u79d1\u5b66\u8bc4\u6d4b\u57fa\u51c6\u5b58\u5728\u63a5\u53e3\u788e\u7247\u5316\u3001\u4efb\u52a1\u8fc7\u7a84\u3001\u6570\u636e\u4f9d\u8d56\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u90e8\u5206\u4efb\u52a1\u751a\u81f3\u65e0\u9700\u6570\u636e\u5373\u53ef\u5b8c\u6210\uff0c\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u771f\u5b9e\u6570\u636e\u5206\u6790\u80fd\u529b\u7684\u68c0\u9a8c\u3002", "method": "\u63d0\u51faDSGym\u6807\u51c6\u5316\u8bc4\u6d4b\u6846\u67b6\uff0c\u5305\u542b\u53ef\u6269\u5c55\u6a21\u5757\u67b6\u6784\uff0c\u6807\u51c6\u5316\u4efb\u52a1\u96c6\uff08DSGym-Tasks\uff09\uff0c\u6db5\u76d6\u751f\u7269\u4fe1\u606f\u5b66\u548c\u591a\u9886\u57df\u9884\u6d4b\u4efb\u52a1\uff0c\u652f\u6301\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u5408\u6210\u5b9e\u73b0\u667a\u80fd\u4f53\u8bad\u7ec3\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u5bf9\u6bd4\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "1. \u6784\u5efa\u4e86DSGym\u5e73\u53f0\uff0c\u652f\u6301\u4efb\u52a1\u3001\u5de5\u5177\u4e0e\u667a\u80fd\u4f53\u6269\u5c55\u30022. \u7b5b\u9009\u5e76\u8865\u5145\u9ad8\u8d28\u91cf\u4efb\u52a1\u96c6\u30023. \u884d\u751f\u9488\u5bf9\u4e0d\u540c\u9886\u57df\u7684\u5b50\u4efb\u52a1\u96c6\u30024. \u5728DSGym\u4e0a\u8bad\u7ec3\u76844B\u6a21\u578b\u5728\u6807\u51c6\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eGPT-4o\u3002", "conclusion": "DSGym\u4e3a\u8bc4\u4f30\u548c\u8bad\u7ec3\u6570\u636e\u79d1\u5b66\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u5e73\u53f0\uff0c\u53ef\u4ee5\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u6570\u636e\u5206\u6790\u80fd\u529b\u6d4b\u8bd5\uff0c\u5e76\u8d85\u8fc7\u76ee\u524d\u4e3b\u6d41\u6a21\u578b\u5982GPT-4o\u5728\u6807\u51c6\u5316\u5206\u6790\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2601.16663", "categories": ["cs.DB", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16663", "abs": "https://arxiv.org/abs/2601.16663", "authors": ["Zoltan Nagy", "Ryan Wisnesky", "Kevin Carlson", "Eswaran Subrahmanian", "Gioele Zardini"], "title": "A Categorical Approach to Semantic Interoperability across Building Lifecycle", "comment": null, "summary": "Buildings generate heterogeneous data across their lifecycle, yet integrating these data remains a critical unsolved challenge. Despite three decades of standardization efforts, over 40 metadata schemas now span the building lifecycle, with fragmentation accelerating rather than resolving. Current approaches rely on point-to-point mappings that scale quadratically with the number of schemas, or universal ontologies that become unwieldy monoliths. The fundamental gap is the absence of mathematical foundations for structure-preserving transformations across heterogeneous building data. Here we show that category theory provides these foundations, enabling systematic data integration with $O(n)$ specification complexity for $n$ ontologies. We formalize building ontologies as first-order theories and demonstrate two proof-of-concept implementations in Categorical Query Language (CQL): 1) generating BRICK models from IFC design data at commissioning, and 2) three-way integration of IFC, BRICK, and RealEstateCore where only two explicit mappings yield the third automatically through categorical composition. Our correct-by-construction approach treats property sets as first-class schema entities and provides automated bidirectional migrations, and enables cross-ontology queries. These results establish feasibility of categorical methods for building data integration and suggest a path toward an app ecosystem for buildings, where mathematical foundations enable reliable component integration analogous to smartphone platforms.", "AI": {"tldr": "\u5efa\u7b51\u751f\u547d\u5468\u671f\u4e2d\u7684\u6570\u636e\u6574\u5408\u6311\u6218\u56e0\u5143\u6570\u636e\u6a21\u5f0f\u788e\u7247\u5316\u800c\u52a0\u5267\uff0c\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u672c\u6587\u63d0\u51fa\u4ee5\u8303\u7574\u8bba\u4e3a\u57fa\u7840\u7684\u7ed3\u6784\u5316\u6570\u636e\u6574\u5408\u529e\u6cd5\uff0c\u663e\u8457\u7b80\u5316\u6574\u5408\u8fc7\u7a0b\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u8fdb\u884c30\u5e74\u7684\u6807\u51c6\u5316\u5de5\u4f5c\uff0c\u5efa\u7b51\u9886\u57df\u5143\u6570\u636e\u65b9\u6848\u4ecd\u9ad8\u5ea6\u788e\u7247\u5316\uff0c\u5bfc\u81f4\u6570\u636e\u6574\u5408\u590d\u6742\u3001\u96be\u4ee5\u7ef4\u62a4\u3002\u7f3a\u4e4f\u6570\u5b66\u57fa\u7840\u4f7f\u5f97\u7ed3\u6784\u4fdd\u7559\u7684\u6570\u636e\u8f6c\u6362\u6210\u4e3a\u672a\u89e3\u51b3\u96be\u9898\u3002", "method": "\u4f5c\u8005\u5c06\u5efa\u7b51\u672c\u4f53\u5f62\u5f0f\u5316\u4e3a\u4e00\u9636\u7406\u8bba\uff0c\u91c7\u7528Categorical Query Language\uff08CQL\uff09\u8fdb\u884c\u8303\u7574\u8bba\u5efa\u6a21\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e24\u4e2a\u539f\u578b\uff1a\u901a\u8fc7\u8303\u7574\u7ec4\u5408\u81ea\u52a8\u751f\u6210\u548c\u6574\u5408BRICK\u3001IFC\u3001RealEstateCore\u6570\u636e\u3002", "result": "\u901a\u8fc7\u8303\u7574\u8bba\u6a21\u578b\u53caCQL\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86n\u4e2a\u672c\u4f53\u53ea\u9700O(n)\u590d\u6742\u5ea6\u7684\u6620\u5c04\u3002\u652f\u6301\u81ea\u52a8\u751f\u6210\u6a21\u578b\u3001\u4e09\u65b9\u672c\u4f53\u6574\u5408\u3001\u53cc\u5411\u8fc1\u79fb\u53ca\u8de8\u672c\u4f53\u67e5\u8be2\uff0c\u4e14\u8bc1\u660e\u6784\u9020\u6b63\u786e\u3002", "conclusion": "\u8303\u7574\u8bba\u4e3a\u5efa\u7b51\u5f02\u6784\u6570\u636e\u6574\u5408\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6570\u5b66\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u3001\u7ed3\u6784\u4fdd\u7559\u7684\u6570\u636e\u8fc1\u79fb\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u53ef\u9760\uff0c\u63a8\u52a8\u53ef\u6269\u5c55\u7684\u6570\u636e\u6574\u5408\u751f\u6001\u3002"}}
{"id": "2601.16479", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16479", "abs": "https://arxiv.org/abs/2601.16479", "authors": ["Hongjia Wu", "Shuai Zhou", "Hongxin Zhang", "Wei Chen"], "title": "Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs", "comment": null, "summary": "While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hierarchy Process (AHP), offer systematic rational frameworks, their construction relies heavily on labor-intensive domain expertise, creating an \"expert bottleneck\" that hinders scalability in general scenarios. To bridge the gap between the generalization capabilities of LLMs and the rigor of decision theory, we propose Doc2AHP, a novel structured inference framework guided by AHP principles. Eliminating the need for extensive annotated data or manual intervention, our approach leverages the structural principles of AHP as constraints to direct the LLM in a constrained search within the unstructured document space, thereby enforcing the logical entailment between parent and child nodes. Furthermore, we introduce a multi-agent weighting mechanism coupled with an adaptive consistency optimization strategy to ensure the numerical consistency of weight allocation. Empirical results demonstrate that Doc2AHP not only empowers non-expert users to construct high-quality decision models from scratch but also significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.", "AI": {"tldr": "\u63d0\u51faDoc2AHP\u6846\u67b6\uff0c\u5c06AHP\u539f\u7406\u4f5c\u4e3a\u7ea6\u675f\uff0c\u5f15\u5bfcLLM\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u51b3\u7b56\u6a21\u578b\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u903b\u8f91\u4e0e\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u96be\u4ee5\u4fdd\u8bc1\u7ed3\u6784\u4e00\u81f4\u6027\u4e0e\u63a8\u7406\u53ef\u9760\u6027\uff0c\u800c\u4f20\u7edf\u51b3\u7b56\u7406\u8bba\u5982AHP\u5219\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5236\u7ea6\u4e86\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u4f5c\u8005\u81f4\u529b\u4e8e\u89e3\u51b3LLMs\u6cdb\u5316\u80fd\u529b\u4e0e\u51b3\u7b56\u7406\u8bba\u4e25\u8c28\u6027\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u4ee5AHP\u7ed3\u6784\u4e3a\u7ea6\u675f\uff0c\u5f15\u5bfcLLM\u8fdb\u884c\u6709\u7ea6\u675f\u7684\u6587\u6863\u7a7a\u95f4\u641c\u7d22\uff0c\u5b9e\u73b0\u7236\u5b50\u8282\u70b9\u4e4b\u95f4\u7684\u903b\u8f91\u8574\u542b\uff1b\u5f15\u5165\u591a\u667a\u80fd\u4f53\u52a0\u6743\u673a\u5236\u4e0e\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u4f18\u5316\uff0c\u786e\u4fdd\u6743\u91cd\u5206\u914d\u7684\u6570\u503c\u4e00\u81f4\u6027\u3002", "result": "Doc2AHP\u6846\u67b6\u80fd\u591f\u5728\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u5e2e\u52a9\u975e\u4e13\u5bb6\u7528\u6237\u4ece\u96f6\u6784\u5efa\u9ad8\u8d28\u91cf\u51b3\u7b56\u6a21\u578b\uff0c\u5728\u903b\u8f91\u5b8c\u6574\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u7cbe\u5ea6\u4e0a\u663e\u8457\u8d85\u8fc7\u76f4\u63a5\u751f\u6210\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Doc2AHP\u6709\u6548\u7ed3\u5408LLM\u6cdb\u5316\u80fd\u529b\u4e0eAHP\u7ed3\u6784\u7ea6\u675f\uff0c\u5728\u975e\u4e13\u5bb6\u573a\u666f\u4e0b\u63d0\u5347\u51b3\u7b56\u6a21\u578b\u81ea\u52a8\u5316\u4e0e\u53ef\u9760\u6027\uff0c\u5bf9\u63a8\u5e7f\u7ed3\u6784\u5316\u63a8\u7406\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.16529", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.16529", "abs": "https://arxiv.org/abs/2601.16529", "authors": ["Dongshen Peng", "Yi Wang", "Carl Preiksaitis", "Christian Rose"], "title": "SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care", "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios, acquiescence rates ranged from 0-100\\%. Models showed higher vulnerability to imaging requests (38.8\\%) than opioid prescriptions (25.0\\%), with model capability poorly predicting robustness. All persuasion tactics proved equally effective (30.0-36.0\\%), indicating general susceptibility rather than tactic-specific weakness. Our findings demonstrate that static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification.", "AI": {"tldr": "\u5927\u591a\u6570\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60a3\u8005\u529d\u8bf4\u4e0b\uff0c\u5bb9\u6613\u505a\u51fa\u4e0d\u9002\u5f53\u7684\u4e34\u5e8a\u51b3\u7b56\uff0c\u4f20\u7edf\u9759\u6001\u5b89\u5168\u6d4b\u8bd5\u4e0d\u80fd\u5145\u5206\u8861\u91cf\u5176\u771f\u5b9e\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u60a3\u8005\u4e0d\u5408\u7406\u65bd\u538b\u65f6\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faSycoEval-EM\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6a21\u62df\u8ba9\u60a3\u8005\u4ee5\u529d\u8bf4\u7b56\u7565\u5f71\u54cdLLM\u5728\u6025\u8bca\u533b\u5b66\u51b3\u7b56\u4e2d\u7684\u9009\u62e9\uff0c\u8986\u76d620\u79cd\u6a21\u578b\u30011,875\u6b21\u4ea4\u4e92\uff0c\u5206\u4e09\u79cd\u5178\u578b\u533b\u7597\u573a\u666f\uff0c\u7cfb\u7edf\u6027\u6d4b\u8bd5\u6a21\u578b\u5728\u793e\u4f1a\u538b\u529b\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6a21\u578b\u5728\u60a3\u8005\u529d\u8bf4\u4e0b\u4f9d\u4ece\u4e0d\u5f53\u533b\u7597\u8bf7\u6c42\u7684\u6982\u7387\u6781\u9ad8\uff080-100%\uff09\uff0c\u4e14\u5728\u5f71\u50cf\u68c0\u67e5\u8bf7\u6c42\u4e0a\u7684\u8106\u5f31\u6027\u9ad8\u4e8e\u963f\u7247\u7c7b\u836f\u7269\u5904\u65b9\u3002\u6a21\u578b\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\u76f8\u5173\u6027\u4f4e\u3002\u6240\u6709\u529d\u8bf4\u7b56\u7565\u5bf9\u6a21\u578b\u5f71\u54cd\u7c7b\u4f3c\uff0c\u663e\u793a\u666e\u904d\u6613\u53d7\u5f71\u54cd\u3002", "conclusion": "\u73b0\u6709\u9759\u6001\u8bc4\u6d4b\u96be\u4ee5\u8986\u76d6\u91cd\u8981\u5b89\u5168\u98ce\u9669\uff0c\u5e94\u91c7\u7528\u591a\u8f6e\u5bf9\u6297\u6d4b\u8bd5\u4f5c\u4e3a\u4e34\u5e8aAI\u8ba4\u8bc1\u7684\u5fc5\u8981\u73af\u8282\uff0c\u4ee5\u786e\u4fdd\u5176\u5728\u771f\u5b9e\u60c5\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u4e0e\u5b89\u5168\u6027\u3002"}}
{"id": "2601.16549", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16549", "abs": "https://arxiv.org/abs/2601.16549", "authors": ["Meet Raval", "Tejul Pandit", "Dhvani Upadhyay"], "title": "LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification", "comment": "9 pages, 5 figures, 3 tables, paper accepted in AAIML'26 conference", "summary": "The combination of multimodal Vision-Language Models (VLMs) and Large Language Models (LLMs) opens up new possibilities for medical classification. This work offers a rigorous, unified benchmark by using four publicly available datasets covering text and image modalities (binary and multiclass complexity) that contrasts traditional Machine Learning (ML) with contemporary transformer-based techniques. We evaluated three model classes for each task: Classical ML (LR, LightGBM, ResNet-50), Prompt-Based LLMs/VLMs (Gemini 2.5), and Fine-Tuned PEFT Models (LoRA-adapted Gemma3 variants). All experiments used consistent data splits and aligned metrics. According to our results, traditional machine learning (ML) models set a high standard by consistently achieving the best overall performance across most medical categorization tasks. This was especially true for structured text-based datasets, where the classical models performed exceptionally well. In stark contrast, the LoRA-tuned Gemma variants consistently showed the worst performance across all text and image experiments, failing to generalize from the minimal fine-tuning provided. However, the zero-shot LLM/VLM pipelines (Gemini 2.5) had mixed results; they performed poorly on text-based tasks, but demonstrated competitive performance on the multiclass image task, matching the classical ResNet-50 baseline. These results demonstrate that in many medical categorization scenarios, established machine learning models continue to be the most reliable option. The experiment suggests that foundation models are not universally superior and that the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) is highly dependent on the adaptation strategy, as minimal fine-tuning proved detrimental in this study.", "AI": {"tldr": "\u591a\u6a21\u6001VLM\u548cLLM\u5728\u533b\u7597\u5206\u7c7b\u573a\u666f\u4e0b\u672a\u80fd\u5168\u65b9\u4f4d\u8d85\u8d8a\u4f20\u7edfML\uff0c\u5c24\u5176PEFT\u5fae\u8c03\u65b9\u5f0f\u5bf9\u7ed3\u679c\u5f71\u54cd\u6781\u5927\uff0c\u4f20\u7edf\u6a21\u578b\u4f9d\u7136\u6700\u53ef\u9760\u3002", "motivation": "\u533b\u836f\u9886\u57df\u5f53\u524d\u6b63\u5feb\u901f\u5f15\u5165\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4f46\u5176\u5728\u533b\u5b66\u5206\u7c7b\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\u548c\u76f8\u8f83\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u65b9\u6cd5\u7684\u4f18\u52a3\u5e76\u672a\u7cfb\u7edf\u5316\u5bf9\u6bd4\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7edf\u4e00\u57fa\u51c6\uff0c\u7cfb\u7edf\u6027\u8bc4\u4f30\u5404\u79cd\u6a21\u578b\u7c7b\u578b\u7684\u8868\u73b0\u3002", "method": "\u57fa\u4e8e\u56db\u4e2a\u516c\u5f00\u533b\u5b66\u6570\u636e\u96c6\uff08\u542b\u6587\u672c\u548c\u56fe\u50cf\u3001\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\uff09\uff0c\u5b9e\u9a8c\u5bf9\u6bd4\u4e09\u79cd\u6a21\u578b\u7c7b\u522b\uff1a\u4f20\u7edfML\uff08LR\u3001LightGBM\u3001ResNet-50\uff09\u3001\u96f6\u6837\u672cVLM/LLM\uff08Gemini 2.5\uff09\u3001\u5fae\u8c03PEFT\u6a21\u578b\uff08LoRA-Gemma3\uff09\u3002\u6240\u6709\u5b9e\u9a8c\u91c7\u7528\u7edf\u4e00\u7684\u6570\u636e\u5207\u5206\u53ca\u6307\u6807\u3002", "result": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982LR\u3001LightGBM\uff09\u5728\u7edd\u5927\u591a\u6570\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u4e2d\u59cb\u7ec8\u8868\u73b0\u6700\u597d\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u4e0a\u8868\u73b0\u7a81\u51fa\u3002LoRA\u5fae\u8c03\u7684Gemma3\u6a21\u578b\u5728\u6240\u6709\u6587\u672c\u548c\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u5dee\uff0c\u65e0\u6cd5\u901a\u8fc7\u6781\u5c11\u91cf\u5fae\u8c03\u5b9e\u73b0\u6cdb\u5316\u3002\u96f6\u6837\u672cVLM/LLM\uff08\u5982Gemini 2.5\uff09\u5728\u6587\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u591a\u7c7b\u522b\u56fe\u50cf\u4efb\u52a1\u4e0a\u5219\u80fd\u5ab2\u7f8e\u4f20\u7edfResNet-50\u57fa\u7ebf\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41\u57fa\u7840\u6a21\u578b\u5e76\u975e\u5bf9\u6240\u6709\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u5747\u5177\u4f18\u52bf\uff0cPEFT\u6548\u679c\u4f9d\u8d56\u5177\u4f53\u7b56\u7565\u548c\u4efb\u52a1\u7c7b\u578b\uff1b\u4f20\u7edfML\u6a21\u578b\u4ecd\u5728\u533b\u5b66\u5206\u7c7b\u9886\u57df\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2601.16685", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16685", "abs": "https://arxiv.org/abs/2601.16685", "authors": ["Suzhong Fu", "Jingqi Dong", "Xuan Ding", "Rui Sun", "Yiming Yang", "Shuguang Cui", "Zhen Li"], "title": "AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning", "comment": null, "summary": "Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoning framework that emulates the collaborative diagnostic workflow of radiologists. By dividing the evaluation process into interpretable steps including criteria definition, evidence extraction, alignment, and consistency scoring, AgentsEval provides explicit reasoning traces and structured clinical feedback. We also construct a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations. Experimental results demonstrate that AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. This framework represents a step toward transparent and clinically grounded assessment of medical report generation systems, fostering trustworthy integration of large language models into clinical practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AgentsEval\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u9488\u5bf9\u81ea\u52a8\u751f\u6210\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u7684\u8bc4\u4f30\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u3001\u4e34\u5e8a\u76f8\u5173\u7684\u8bc4\u6d4b\uff0c\u5e76\u901a\u8fc7\u591a\u9886\u57df\u6270\u52a8\u57fa\u51c6\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u548c\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6355\u6349\u533b\u5b66\u8bca\u65ad\u7684\u7ed3\u6784\u5316\u63a8\u7406\u903b\u8f91\uff0c\u5bfc\u81f4\u8bc4\u6d4b\u7ed3\u679c\u7f3a\u4e4f\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u53ef\u9760\u6027\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u57fa\u7840\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6848\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6a21\u62df\u653e\u5c04\u79d1\u533b\u751f\u534f\u540c\u8bca\u65ad\u6d41\u7a0b\uff0c\u5c06\u8bc4\u4f30\u5206\u4e3a\u51c6\u5219\u5b9a\u4e49\u3001\u8bc1\u636e\u63d0\u53d6\u3001\u5bf9\u9f50\u548c\u4e00\u81f4\u6027\u8bc4\u5206\u7b49\u6b65\u9aa4\uff0c\u5168\u7a0b\u663e\u793a\u63a8\u7406\u8f68\u8ff9\u548c\u7ed3\u6784\u5316\u53cd\u9988\u3002\u6b64\u5916\uff0c\u5efa\u7acb\u4e86\u6db5\u76d6\u591a\u6a21\u6001\u3001\u591a\u8bed\u4e49\u6270\u52a8\u6761\u4ef6\u7684\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cAgentsEval\u5728\u4e0d\u540c\u9886\u57df\u548c\u6570\u636e\u6270\u52a8\u4e0b\u4fdd\u6301\u5bf9\u4e34\u5e8a\u8bed\u4e49\u548c\u98ce\u683c\u7684\u9ad8\u4e00\u81f4\u6027\u8bc4\u6d4b\uff0c\u5177\u5907\u4f18\u826f\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u53ca\u4e34\u5e8a\u9002\u7528\u6027\u3002", "conclusion": "AgentsEval\u80fd\u591f\u63d0\u4f9b\u4e34\u5e8a\u4e00\u81f4\u3001\u8bed\u4e49\u5fe0\u5b9e\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u81ea\u52a8\u533b\u5b66\u62a5\u544a\u8bc4\u4f30\u7ed3\u679c\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u91c7\u7eb3\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u652f\u6301\u3002"}}
{"id": "2601.16725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16725", "abs": "https://arxiv.org/abs/2601.16725", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chen Gao", "Chen Zhang", "Chengcheng Han", "Chenhui Yang", "Chuyu Zhang", "Cong Chen", "Cunguang Wang", "Daoru Pan", "Defei Bu", "Dengchang Zhao", "Di Xiu", "Dishan Liu", "Dongyu Ru", "Dunwei Tu", "Fan Wu", "Fengcheng Yuan", "Fengcun Li", "Gang Xu", "Guanyu Wu", "Guoyuan Lin", "Haibin Wang", "Hansi Yang", "Hao Yang", "Haonan Yan", "Haoxiang Ma", "Haoxing Wen", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiacheng Zhang", "Jiahong Zhou", "Jiahuan Li", "Jiaming Wang", "Jian Yang", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiapeng Zhu", "Jiaqi Sun", "Jiarong Shi", "Jiarui Zhao", "Jingang Wang", "Jinluan Yang", "Jinrui Ding", "Jinwei Xiao", "Jiyuan He", "Juncan Xu", "Kefeng Zhang", "Keheng Wang", "Li Wei", "Lianhui Ma", "Lin Qiu", "Lingbing Kong", "Lingchuan Liu", "Linsen Guo", "Mengshen Zhu", "Mengxia Shen", "Mingyang Zhu", "Peiguang Li", "Peng Pei", "Pengcheng Jia", "Pengtao Zhang", "Peng Zhao", "Qi Gu", "Qiong Huang", "Qiyuan Duan", "Quanchi Weng", "Rongxiang Weng", "Rongzhi Zhang", "Rumei Li", "Shanglin Lei", "Shengnan An", "Shijun Dai", "Shuaikang Liu", "Shuang Zhou", "Shuo Wang", "Songyuan Zhao", "Tao Liang", "Tianhao Hu", "Tianze Chen", "Wei Liu", "Wei Shi", "Wei Wang", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Wentao Chen", "Wentao Shi", "Xi Su", "Xiangcheng Liu", "Xiandi Ma", "Xiangyu Xi", "Xiangyuan Liu", "Xiangzhou Huang", "Xiao Liu", "Xiaodong Cai", "Xiaolong Chen", "Xiaowei Shi", "Xiaoyu Li", "Xin Chen", "Xingchen Liu", "Xuan Huang", "Xuezhi Cao", "Xunliang Cai", "Yan Chen", "Yang Bai", "Yang Liu", "Yang Yang", "Yang Zheng", "Yaoming Wang", "Yaoming Zhu", "Yaqi Huo", "Yanyu Chen", "Yaorui Shi", "Yerui Sun", "Yi Zhang", "Yihao Chen", "Yi-Kai Zhang", "Yifan Lu", "Yifan Zhao", "Yitao Zhai", "Yongjing Yin", "Yongwei Zhou", "Youshao Xiao", "Yuchuan Dai", "Yuchen Xie", "Yuchen Yu", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunfan Liang", "Yunke Zhao", "Yuwei Jiang", "Yuxin Bian", "Yuxin Chen", "Yuxin Liu", "Yue Xu", "Yueqing Sun", "Zeyang Yu", "Zhao Yang", "Zhengsheng Huang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhimin Lin", "Zhiyuan Yao", "Zhuofan Chen", "Zhuowen Han", "Zijian Zhang", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang"], "title": "LongCat-Flash-Thinking-2601 Technical Report", "comment": null, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e865600\u4ebf\u53c2\u6570\u7684MoE\u6a21\u578b\uff0c\u5728\u591a\u4e2aagentic\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5237\u65b0SOTA\uff0c\u5177\u5907\u5f3a\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e13\u4e3a\u590d\u6742\u63a8\u7406\u548c\u5b9e\u9645\u5e94\u7528\u8bbe\u8ba1\uff0c\u5b8c\u5584\u8bad\u7ec3\u548c\u73af\u5883\u534f\u540c\uff0c\u7279\u522b\u5f3a\u8c03\u5bf9\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u4e0e\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u7684\u5904\u7406\u3002", "motivation": "\u5f00\u53d1\u9ad8\u53c2\u6570\u3001\u9ad8\u6548\u7684\u5f00\u6e90MoE\u63a8\u7406\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347agentic\uff08\u81ea\u6cbb\u4f53\u5f0f\uff09\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u4ee5\u53ca\u5de5\u5177\u96c6\u6210\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u663e\u8457\u589e\u5f3a\u6a21\u578b\u5728\u590d\u6742\u3001\u5e26\u566a\u58f0\u7684\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u6784\u5efa5600\u4ebf\u53c2\u6570\u7684MoE\u6a21\u578b\uff0c\u91c7\u7528\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u4e0e\u878d\u5408\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7aef\u5230\u7aef\u8054\u5408\u8bbe\u8ba1\u6570\u636e\u3001\u73af\u5883\u3001\u7b97\u6cd5\u53ca\u57fa\u7840\u8bbe\u65bd\uff1b\u6269\u5c55\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6DORA\u4ee5\u652f\u6301\u591a\u73af\u5883\u5927\u89c4\u6a21\u9ad8\u6548\u8bad\u7ec3\uff1b\u7cfb\u7edf\u5206\u6790\u548c\u5206\u89e3\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u5e76\u5728\u8bad\u7ec3\u4e2d\u663e\u5f0f\u7eb3\u5165\uff1b\u5f15\u5165Heavy Thinking\u6a21\u5f0f\u4ee5\u6d4b\u8bd5\u65f6\u6269\u5c55\u63a8\u7406\u5e7f\u5ea6\u548c\u6df1\u5ea6\u3002", "result": "\u6a21\u578b\u5728\u5f00\u653e\u6e90\u9886\u57df\u521b\u9020\u4e86agentic\u63a8\u7406\u8bf8\u591a\u57fa\u51c6\u6d4b\u8bd5\u7684\u65b0SOTA\u8868\u73b0\uff0c\u5bf9\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u6027\uff0c\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u4e5f\u5177\u5907\u8f83\u9ad8\u9c81\u68d2\u6027\uff0c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u5353\u8d8a\u3002", "conclusion": "LongCat-Flash-Thinking-2601\u5c55\u73b0\u51fa\u6781\u5f3a\u7684agentic\u63a8\u7406\u3001\u5de5\u5177\u5e94\u7528\u53ca\u590d\u6742\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u548c\u9c81\u68d2\u80fd\u529b\uff0c\u4e3a\u5f00\u6e90\u5927\u578b\u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u4efb\u52a1\u4e2d\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2601.16806", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16806", "abs": "https://arxiv.org/abs/2601.16806", "authors": ["Lu Yihe", "Barbara Webb"], "title": "An Efficient Insect-inspired Approach for Visual Point-goal Navigation", "comment": null, "summary": "In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.", "AI": {"tldr": "\u5c06\u6606\u866b\u8111\u7684\u5173\u8054\u5b66\u4e60\u4e0e\u8def\u5f84\u79ef\u5206\u6a21\u578b\u7ed3\u5408\uff0c\u7528\u4e8e\u89c6\u89c9\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\uff0c\u8fbe\u5230\u4e86\u73b0\u6709SOTA\u6c34\u5e73\u4f46\u6781\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u53d7\u6606\u866b\u542f\u53d1\u7684\u89c6\u89c9\u76ee\u6807\u5bfc\u822a\u667a\u80fd\u4f53\u3002\u7ed3\u5408\u4e86\u6606\u866b\u8111\u4e2d\u4e0e\u5173\u8054\u5b66\u4e60\u548c\u8def\u5f84\u79ef\u5206\u76f8\u5173\u7684\u4e24\u4e2a\u7ed3\u6784\u7684\u62bd\u8c61\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u5bfc\u822a\u667a\u80fd\u4f53\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002", "method": "\u5c06\u6606\u866b\u8111\u4e24\u4e2a\u4e0e\u5bfc\u822a\u76f8\u5173\u7684\u7ed3\u6784\u8fdb\u884c\u4e86\u62bd\u8c61\u5efa\u6a21\uff0c\u5e76\u5728Habitat\u7b49\u89c6\u89c9\u5bfc\u822a\u57fa\u51c6\u4e0b\u8fdb\u884c\u6d4b\u8bd5\u548c\u5206\u6790\uff0c\u5f3a\u8c03\u5176\u7b80\u5355\u7ed3\u6784\u4e0e\u4f4e\u8ba1\u7b97\u5f00\u9500\u4ee5\u53ca\u5728\u6a21\u62df\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u8be5\u6606\u866b\u542f\u53d1\u7684\u667a\u80fd\u4f53\u5728\u6807\u51c6\u70b9\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u6700\u65b0SOTA\u6a21\u578b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u4f4e\u8bb8\u591a\u6570\u91cf\u7ea7\uff0c\u5728\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u4e2d\u4e5f\u8868\u73b0\u51fa\u5bf9\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6606\u866b\u542f\u53d1\u7684\u65b9\u6cd5\u53ef\u5b9e\u73b0\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89c6\u89c9\u5bfc\u822a\uff0c\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u80fd\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u5bf9\u5b9e\u9645\u5bfc\u822a\u7cfb\u7edf\u8bbe\u8ba1\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2601.16853", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16853", "abs": "https://arxiv.org/abs/2601.16853", "authors": ["Ian B. de Haan", "Peter van der Putten", "Max van Duijn"], "title": "Reasoning Promotes Robustness in Theory of Mind Tasks", "comment": "14 pages, 2 figures", "summary": "Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.", "AI": {"tldr": "RLVR\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5176\u4f18\u52bf\u66f4\u53ef\u80fd\u6765\u6e90\u4e8e\u89e3\u9898\u9c81\u68d2\u6027\u7684\u63d0\u5347\uff0c\u800c\u975e\u4ea7\u751f\u4e86\u65b0\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u9274\u4e8eLLMs\u5728ToM\u6d4b\u8bd5\u4e2d\u9ad8\u5206\u5f15\u53d1\u4e86\u5176\u63a8\u7406\u80fd\u529b\u672c\u8d28\u7684\u4e89\u8bae\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u6790\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u540e\u7684LLMs\u5728ToM\u4efb\u52a1\u4e2d\u7684\u771f\u5b9e\u8868\u73b0\uff0c\u63a2\u8ba8\u5176\u6240\u5c55\u73b0\u80fd\u529b\u7684\u771f\u6b63\u6765\u6e90\u53ca\u5176\u5bf9\u793e\u4f1a\u8ba4\u77e5\u884c\u4e3a\u8bc4\u4f30\u7684\u542f\u793a\u3002", "method": "\u5229\u7528\u9002\u914d\u540e\u7684\u673a\u5668\u5fc3\u7406\u5b66\u5b9e\u9a8c\u548c\u65e2\u6709\u57fa\u51c6\uff0c\u5bf9RLVR\u8bad\u7ec3\u7684\u63a8\u7406\u578bLLMs\u5728ToM\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u6d4b\u8bd5\u548c\u884c\u4e3a\u5206\u6790\uff0c\u5bf9\u5176\u5bf9\u4efb\u52a1\u6270\u52a8\u548c\u63d0\u793a\u53d8\u5316\u7684\u54cd\u5e94\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u63a8\u7406\u80fd\u529b\u5bfc\u5411\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u7279\u522b\u662f\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u5e94\u5bf9\u63d0\u793a\u53d8\u5316\u548c\u4efb\u52a1\u6270\u52a8\u65f6\u3002\u53e6\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u5c55\u73b0\u51fa\u4e86\u663e\u8457\u63d0\u5347\u3002\u7814\u7a76\u5229\u7528\u4e86\u673a\u5668\u5fc3\u7406\u5b66\u5b9e\u9a8c\u7684\u521b\u65b0\u9002\u914d\u53ca\u73b0\u6709\u57fa\u51c6\u7ed3\u679c\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u5f52\u56e0\u4e8e\u6c42\u89e3\u6b63\u786e\u7b54\u6848\u7684\u9c81\u68d2\u6027\u589e\u5f3a\uff0c\u800c\u975e\u4ea7\u751f\u4e86\u65b0\u7684ToM\u63a8\u7406\u65b9\u5f0f\u3002", "conclusion": "\u5f53\u524dLLMs\u5728ToM\u4efb\u52a1\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\uff0c\u66f4\u5e94\u5f52\u56e0\u4e8e\u5bf9\u6270\u52a8\u548c\u53d8\u5316\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u7684\u63d0\u5347\uff0c\u800c\u4e0d\u662f\u5176\u793e\u4f1a-\u8ba4\u77e5\u63a8\u7406\u80fd\u529b\u672c\u8d28\u4e0a\u7684\u589e\u5f3a\u3002"}}
{"id": "2601.16967", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.16967", "abs": "https://arxiv.org/abs/2601.16967", "authors": ["Bernes Lorier Atabonfack", "Ahmed Tahiru Issah", "Mohammed Hardi Abdul Baaki", "Clemence Ingabire", "Tolulope Olusuyi", "Maruf Adewole", "Udunna C. Anazodo", "Timothy X Brown"], "title": "Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians", "comment": "Accepted at the MIRASOL Workshop at MICCAI 2025. To appear in Lecture Notes in Computer Science (LNCS)", "summary": "In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u5b9e\u8bc1\u4e86\u4e00\u6b3eAI\u8f85\u52a9\u533b\u7597\u8bbe\u5907\u7ef4\u4fee\u5e73\u53f0\uff0c\u80fd\u6709\u6548\u63d0\u5347\u8bbe\u5907\u7ef4\u62a4\u6548\u7387\uff0c\u51cf\u7f13\u8bbe\u5907\u6545\u969c\u5bf9\u4f4e\u4e2d\u6536\u5165\u56fd\u5bb6\u533b\u7597\u670d\u52a1\u7684\u4e0d\u5229\u5f71\u54cd\u3002", "motivation": "\u4f4e\u4e2d\u6536\u5165\u56fd\u5bb6\u533b\u7597\u8bbe\u5907\u56e0\u7f3a\u4e4f\u7ef4\u62a4\u548c\u6280\u672f\u652f\u6301\uff0c\u5bfc\u81f4\u5927\u91cf\u8bbe\u5907\u672a\u88ab\u5145\u5206\u5229\u7528\u6216\u65e0\u6cd5\u4f7f\u7528\uff0c\u5f71\u54cd\u75c5\u60a3\u62a4\u7406\u548c\u533b\u7597\u8d28\u91cf\u3002\u7814\u7a76\u65e8\u5728\u4e3a\u8bbe\u5907\u7ef4\u62a4\u96be\u9898\u63d0\u4f9b\u521b\u65b0\u667a\u80fd\u652f\u6301\u3002", "method": "\u672c\u7814\u7a76\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u5957\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u7f51\u9875\u754c\u9762\u7684AI\u652f\u6301\u5e73\u53f0\uff0c\u7528\u6237\u53ef\u8f93\u5165\u9519\u8bef\u7801\u6216\u8bbe\u5907\u75c7\u72b6\uff0c\u7cfb\u7edf\u5373\u65f6\u63d0\u4f9b\u5206\u6b65\u6392\u67e5\u548c\u4fee\u590d\u6307\u5357\uff0c\u5e76\u8bbe\u6709\u5168\u7403\u540c\u4e1a\u8bba\u575b\u4fc3\u8fdb\u77e5\u8bc6\u4ea4\u6d41\u3002\u4ee5Philips HDI 5000\u8d85\u58f0\u673a\u4e3a\u6848\u4f8b\uff0c\u68c0\u9a8c\u5e73\u53f0\u7684\u53ef\u884c\u6027\u3002", "result": "\u5e73\u53f0\u5bf9\u9519\u8bef\u7801\u7406\u89e3\u7684\u7cbe\u5ea6\u8fbe100%\uff0c\u5bf9\u4fee\u590d\u5efa\u8bae\u7684\u51c6\u786e\u7387\u4e3a80%\uff0c\u5728\u533b\u7597\u8bbe\u5907\u7ef4\u62a4\u65b9\u9762\u5c55\u73b0\u51fa\u9ad8\u6548\u53ef\u884c\u6027\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u652f\u6301\u5e73\u53f0\u53ef\u6709\u6548\u8f85\u52a9\u751f\u7269\u533b\u5b66\u6280\u672f\u4eba\u5458\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e0b\u7ef4\u62a4\u548c\u4fee\u7406\u533b\u7597\u8bbe\u5907\uff0c\u5c55\u73b0\u51fa\u51cf\u5c11\u8bbe\u5907\u505c\u673a\u65f6\u95f4\u3001\u63d0\u5347\u533b\u7597\u670d\u52a1\u8d28\u91cf\u7684\u6f5c\u529b\u3002"}}
