{"id": "2601.11327", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.11327", "abs": "https://arxiv.org/abs/2601.11327", "authors": ["Agata \u017bywot", "Xinyi Chen", "Maarten de Rijke"], "title": "Can Small Agent Collaboration Beat a Single Big LLM?", "comment": null, "summary": "This report studies whether small, tool-augmented agents can match or outperform larger monolithic models on the GAIA benchmark. Using Qwen3 models (4B-32B) within an adapted Agentic-Reasoning framework, we isolate the effects of model scale, explicit thinking (no thinking, planner-only, or full), and tool use (search, code, mind-map). Tool augmentation provides the largest and most consistent gains. Using tools, 4B models can outperform 32B models without tool access on GAIA in our experimental setup. In contrast, explicit thinking is highly configuration- and difficulty-dependent: planner-only thinking can improve decomposition and constraint tracking, while unrestricted full thinking often degrades performance by destabilizing tool orchestration, leading to skipped verification steps, excessive tool calls, non-termination, and output-format drift.", "AI": {"tldr": "\u8be5\u62a5\u544a\u63a2\u8ba8\u4e86\u5c0f\u89c4\u6a21\u3001\u5de5\u5177\u589e\u5f3a\u578b\u667a\u80fd\u4f53\u5728GAIA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u6bd4\u8f83\u4e86\u5176\u4e0e\u66f4\u5927\u89c4\u6a21\u5355\u4f53\u6a21\u578b\u7684\u5dee\u5f02\u3002", "motivation": "\u63a2\u7a76\u5728\u4efb\u52a1\u89e3\u51b3\u4e2d\uff0c\u5c0f\u6a21\u578b\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u80fd\u5426\u5339\u654c\u751a\u81f3\u8d85\u8d8a\u5927\u578b\u5355\u4f53\u6a21\u578b\uff0c\u5e76\u5206\u6790\u5de5\u5177\u4f7f\u7528\u4e0e\u663e\u5f0f\u601d\u7ef4\u5bf9\u667a\u80fd\u4f53\u8868\u73b0\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528Qwen3\u6a21\u578b\uff08\u53c2\u6570\u89c4\u6a214B-32B\uff09\uff0c\u5e76\u5728Agentic-Reasoning\u6846\u67b6\u4e0b\uff0c\u7814\u7a76\u6a21\u578b\u89c4\u6a21\u3001\u663e\u5f0f\u601d\u7ef4\u53ca\u5de5\u5177\u4f7f\u7528\uff08\u68c0\u7d22\u3001\u4ee3\u7801\u3001\u601d\u7ef4\u5bfc\u56fe\uff09\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u6bd4\u8f83\u65e0\u601d\u8003\u3001\u4ec5\u89c4\u5212\u3001\u5b8c\u5168\u601d\u8003\uff0c\u4ee5\u53ca\u4e0d\u540c\u5de5\u5177\u589e\u5f3a\u65b9\u5f0f\u7684\u914d\u7f6e\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5de5\u5177\u589e\u5f3a\u5e26\u6765\u6700\u5927\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f7f\u5f974B\u6a21\u578b\u5728\u4f7f\u7528\u5de5\u5177\u540e\u8d85\u8fc7\u4e8632B\u65e0\u5de5\u5177\u6a21\u578b\u3002\u663e\u5f0f\u601d\u7ef4\u8868\u73b0\u4f9d\u8d56\u914d\u7f6e\uff0c\u89c4\u5212\u578b\u601d\u7ef4\u6709\u52a9\u4e8e\u4efb\u52a1\u5206\u89e3\u4e0e\u7ea6\u675f\u8ddf\u8e2a\uff0c\u800c\u5168\u91cf\u663e\u5f0f\u601d\u7ef4\u53ef\u80fd\u56e0\u5de5\u5177\u7ba1\u63a7\u6df7\u4e71\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u5de5\u5177\u589e\u5f3a\u662f\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u7684\u6700\u6709\u6548\u4e14\u7a33\u5b9a\u7684\u65b9\u6cd5\uff0c\u751a\u81f3\u80fd\u4f7f\u5c0f\u6a21\u578b\u8d85\u8d8a\u65e0\u5de5\u5177\u7684\u5927\u6a21\u578b\u3002\u663e\u5f0f\u601d\u7ef4\u7684\u63d0\u5347\u5177\u6709\u4f9d\u8d56\u6027\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2601.10726", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10726", "abs": "https://arxiv.org/abs/2601.10726", "authors": ["Ross Chu", "Yuting Huang"], "title": "Building AI Agents to Improve Job Referral Requests to Strangers", "comment": null, "summary": "This paper develops AI agents that help job seekers write effective requests for job referrals in a professional online community. The basic workflow consists of an improver agent that rewrites the referral request and an evaluator agent that measures the quality of revisions using a model trained to predict the probability of receiving referrals from other users. Revisions suggested by the LLM (large language model) increase predicted success rates for weaker requests while reducing them for stronger requests. Enhancing the LLM with Retrieval-Augmented Generation (RAG) prevents edits that worsen stronger requests while it amplifies improvements for weaker requests. Overall, using LLM revisions with RAG increases the predicted success rate for weaker requests by 14\\% without degrading performance on stronger requests. Although improvements in model-predicted success do not guarantee more referrals in the real world, they provide low-cost signals for promising features before running higher-stakes experiments on real users.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408RAG\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u81ea\u52a8\u4f18\u5316\u804c\u4e1a\u793e\u4ea4\u5e73\u53f0\u4e0a\u7684\u5185\u63a8\u8bf7\u6c42\uff0c\u5b9e\u73b0\u5f31\u8bf7\u6c42\u9884\u6d4b\u6210\u529f\u7387\u5927\u5e45\u63d0\u5347\uff0c\u5bf9\u5f3a\u8bf7\u6c42\u65e0\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u5728\u804c\u4e1a\u793e\u533a\u4e2d\uff0c\u8bf7\u6c42\u5185\u63a8\u7684\u6210\u6548\u5dee\u5f02\u5927\uff0c\u90e8\u5206\u7528\u6237\u56e0\u63aa\u8f9e\u6216\u8868\u8fbe\u4e0d\u5f53\u5bfc\u81f4\u8bf7\u6c42\u96be\u4ee5\u88ab\u63a5\u53d7\uff0c\u4ece\u800c\u5f71\u54cd\u6c42\u804c\u6548\u7387\u3002\u672c\u6587\u81f4\u529b\u4e8e\u4e3a\u6c42\u804c\u8005\u63d0\u4f9b\u81ea\u52a8\u5316\u4e14\u9ad8\u6548\u7684\u5185\u63a8\u8bf7\u6c42\u4f18\u5316\u5de5\u5177\uff0c\u4ee5\u589e\u52a0\u8bf7\u6c42\u7684\u83b7\u5f97\u5185\u63a8\u6982\u7387\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u5de5\u4f5c\u6d41\uff0c\u5305\u542b\u91cd\u5199\uff08improver\uff09\u548c\u8d28\u91cf\u8bc4\u4f30\uff08evaluator\uff09\u4e24\u7c7b\u667a\u80fd\u4f53\u3002\u91cd\u5199\u667a\u80fd\u4f53\u57fa\u4e8eLLM\u5bf9\u8bf7\u6c42\u8fdb\u884c\u4f18\u5316\uff0c\u8bc4\u4f30\u667a\u80fd\u4f53\u5219\u5229\u7528\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u63a8\u8350\u6210\u529f\u7387\u3002\u6b64\u5916\uff0c\u901a\u8fc7Retrieval-Augmented Generation\uff08RAG\uff09\u673a\u5236\u589e\u5f3aLLM\u8868\u73b0\u3002", "result": "RAG\u589e\u5f3a\u7684LLM\u4fee\u6b63\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u5f3a\u8bf7\u6c42\u7684\u57fa\u7840\u4e0a\uff0c\u5c06\u5f31\u8bf7\u6c42\u7684\u9884\u6d4b\u6210\u529f\u7387\u63d0\u534714%\u3002", "conclusion": "\u7ed3\u5408RAG\u7684LLM\u4fee\u6b63\u65b9\u6cd5\u5728\u4e0d\u964d\u4f4e\u5f3a\u8bf7\u6c42\u8868\u73b0\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5f31\u63a8\u8350\u8bf7\u6c42\u7684\u9884\u6d4b\u6210\u529f\u7387\u3002"}}
{"id": "2601.10738", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10738", "abs": "https://arxiv.org/abs/2601.10738", "authors": ["Percy Jardine"], "title": "CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems", "comment": null, "summary": "Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCTHA\u67b6\u6784\u4ee5\u4fee\u6b63\u591a\u65f6\u95f4\u5c3a\u5ea6\u7cfb\u7edf\u4e2d\u7684\u534f\u8c03\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6548\u7387\u4e0e\u9c81\u68d2\u6027\u4e0a\u5747\u5177\u5907\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u667a\u80fd\u4f53\u67b6\u6784\u867d\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u540c\u65f6\u5bfc\u81f4\u4e86\u534f\u8c03\u7a33\u5b9a\u6027\u4e0b\u964d\uff0c\u51fa\u73b0\u5c42\u95f4\u51b2\u7a81\u3001\u9519\u8bef\u4f20\u64ad\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u53d7\u7ea6\u675f\u7684\u65f6\u5e8f\u5c42\u6b21\u67b6\u6784\uff08CTHA\uff09\uff0c\u901a\u8fc7\u5c06\u5c42\u95f4\u901a\u4fe1\u9650\u5236\u5728\u7ed3\u6784\u5316\u6d41\u5f62\u4e0a\uff0c\u5e76\u8bbe\u8ba1\u88c1\u51b3\u673a\u5236\uff0c\u534f\u8c03\u5404\u5c42\u51b3\u7b56\u3002\u5177\u4f53\u5305\u62ec\u4e09\u5927\u7ea6\u675f\uff1a\u6d88\u606f\u5408\u540c\u7ea6\u675f\uff08\u89c4\u8303\u4fe1\u606f\u6d41\uff09\u3001\u6743\u9650\u6d41\u5f62\u7ea6\u675f\uff08\u9650\u5b9a\u51b3\u7b56\u7a7a\u95f4\uff09\u3001\u88c1\u51b3\u6d88\u89e3\u7ea6\u675f\uff08\u51b2\u7a81\u6d88\u89e3\uff09\u3002", "result": "CTHA\u5728\u590d\u6742\u4efb\u52a1\u6267\u884c\u4e2d\uff0c\u6bd4\u65e0\u7ea6\u675f\u5c42\u7ea7\u57fa\u7ebf\u8868\u73b0\u66f4\u4f18\uff0c\u964d\u4f4e47%\u6545\u969c\u8fde\u9501\uff0c\u63d0\u9ad82.3\u500d\u6837\u672c\u6548\u7387\uff0c\u5e76\u5177\u6709\u66f4\u4f73\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u53d7\u7ea6\u675f\u65f6\u5e8f\u5c42\u6b21\u67b6\u6784\u53ef\u63d0\u5347\u591a\u667a\u80fd\u4f53\u534f\u540c\u4e0e\u7cfb\u7edf\u5065\u58ee\u6027\uff0c\u5bf9\u591a\u5c42\u6b21\u81ea\u4e3b\u7cfb\u7edf\u53d1\u5c55\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2601.10744", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.10744", "abs": "https://arxiv.org/abs/2601.10744", "authors": ["Sen Wang", "Bangwei Liu", "Zhenkun Gao", "Lizhuang Ma", "Xuhong Wang", "Yuan Xie", "Xin Tan"], "title": "Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration", "comment": "Our dataset and code will be released at our \\href{https://wangsen99.github.io/papers/lmee/}{website}", "summary": "An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4ee5\u957f\u671f\u8bb0\u5fc6\u9a71\u52a8\u7684\u5177\u8eab\u63a2\u7d22\u548c\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u5177\u8eab\u4f53\u667a\u80fd\u4f53\u7684\u4e3b\u52a8\u63a2\u7d22\u4e0e\u8ffd\u5fc6\u80fd\u529b\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u5177\u8eab\u4efb\u52a1\u91cd\u7ed3\u679c\uff0c\u5ffd\u7565\u63a2\u7d22\u4e0e\u8bb0\u5fc6\u5229\u7528\u8fc7\u7a0b\uff0c\u9650\u5236\u957f\u671f\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\u3002\u56e0\u6b64\u63d0\u51fa\u7ed3\u5408\u957f\u671f\u8bb0\u5fc6\u548c\u63a2\u7d22\u4ee5\u4fc3\u8fdb\u667a\u80fd\u4f53\u7ec8\u8eab\u5b66\u4e60\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5956\u52b1\u51fd\u6570\uff08\u52a8\u4f5c\u9884\u6d4b\u3001\u524d\u6cbf\u9009\u62e9\u3001\u95ee\u7b54\uff09\uff0c\u63d0\u5347\u667a\u80fd\u4f53\u4e3b\u52a8\u8bb0\u5fc6\u68c0\u7d22\u4e0e\u63a2\u7d22\u80fd\u529b\uff0c\u5e76\u6784\u5efaLMEE-Bench\u7528\u4e8e\u7efc\u5408\u8bc4\u6d4b\u3002", "result": "\u63d0\u51fa\u7684MemoryExplorer\u5728\u5305\u542b\u591a\u76ee\u6807\u5bfc\u822a\u4e0e\u57fa\u4e8e\u8bb0\u5fc6\u7684\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u63a2\u7d22\u6548\u7387\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5MemoryExplorer\u5728\u957f\u65f6\u5e8f\u5177\u8eab\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\u3002"}}
{"id": "2601.10768", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.10768", "abs": "https://arxiv.org/abs/2601.10768", "authors": ["Nina Bo\u010dkov\u00e1", "Barbora Voln\u00e1", "Mirko Dohnal"], "title": "Optimisation of complex product innovation processes based on trend models with three-valued logic", "comment": null, "summary": "This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u4ee5\u8d8b\u52bf\u4e3a\u91cf\u5316\u6307\u6807\uff0c\u63d0\u51fa\u521b\u65b0\u8fc7\u7a0b\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6c\u79fb\u56fe\u5c55\u73b0\u7cfb\u7edf\u7684\u5404\u79cd\u53ef\u80fd\u884c\u4e3a\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u7684\u590d\u6742\u4ea7\u54c1\u521b\u65b0\u8fc7\u7a0b\u96be\u4ee5\u5efa\u6a21\uff0c\u4f20\u7edf\u65b9\u6cd5\u591a\u4f9d\u8d56\u6570\u503c\u6216\u7c97\u7565\u96c6\u5408\uff0c\u4fe1\u606f\u9700\u6c42\u9ad8\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u7b80\u5316\u5bf9\u8fc7\u7a0b\u8d8b\u52bf\u7684\u91cf\u5316\u3002", "method": "\u5229\u7528\u4e00\u7ec4\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5c06\u521b\u65b0\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u53d8\u91cf\u8d8b\u52bf\uff08\u9012\u589e\u3001\u9012\u51cf\u6216\u4fdd\u6301\u4e0d\u53d8\uff09\u4f5c\u4e3a\u5efa\u6a21\u57fa\u7840\u3002\u901a\u8fc7\u6784\u5efa\u8d8b\u52bf\u6a21\u578b\uff0c\u751f\u6210\u5305\u542b\u72b6\u6001\u573a\u666f\u53ca\u76f8\u4e92\u8f6c\u79fb\u5173\u7cfb\u7684\u8f6c\u79fb\u56fe\uff0c\u63cf\u8ff0\u7cfb\u7edf\u6f14\u5316\u8def\u5f84\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u8d56\u8d8b\u52bf\uff08\u9012\u589e\u3001\u9012\u51cf\u3001\u4fdd\u6301\u4e0d\u53d8\uff09\u4f5c\u4e3a\u4fe1\u606f\u91cf\u6781\u4f4e\u91cf\u5316\u6307\u6807\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u521b\u65b0\u8fc7\u7a0b\u7684\u7cfb\u7edf\u884c\u4e3a\u63cf\u8ff0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u4ee5\u8f83\u4f4e\u4fe1\u606f\u8981\u6c42\u6709\u6548\u5efa\u6a21\u590d\u6742\u4ea7\u54c1\u521b\u65b0\u8fc7\u7a0b\uff0c\u5e76\u53ef\u7528\u8f6c\u79fb\u56fe\u8def\u5f84\u5c55\u73b0\u7cfb\u7edf\u5386\u53f2\u548c\u672a\u6765\u7684\u5168\u90e8\u53ef\u80fd\u884c\u4e3a\u3002"}}
{"id": "2601.10922", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10922", "abs": "https://arxiv.org/abs/2601.10922", "authors": ["Yosub Shin", "Michael Buriek", "Boris Sobolev", "Pavel Bushuyeu", "Vikas Kumar", "Haoyang Xu", "Samuel Watson", "Igor Molybog"], "title": "What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge", "comment": null, "summary": "We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.", "AI": {"tldr": "\u5728\u56fa\u5b9a\u6a21\u578b\u548c\u8bad\u7ec3\u534f\u8bae\u4e0b\uff0c\u7cbe\u786e\u7684\u96be\u5ea6\u578b\u6837\u672c\u9009\u62e9\u4f18\u4e8e\u4f20\u7edf\u7684\u591a\u6837\u6027\u6216\u6269\u5145\u7b56\u7565\uff0c\u662f\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u7684\u5173\u952e\u3002", "motivation": "\u5bf9\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u4f18\u5316\uff0c\u672c\u7814\u7a76\u901a\u8fc7NeurIPS DCVLR\u6311\u6218\u4e13\u6ce8\u4e8e\u6570\u636e\u96c6\u9009\u62e9\uff0c\u63a2\u7a76\u5982\u4f55\u901a\u8fc7\u9ad8\u6548\u6570\u636e\u7b56\u5212\u63d0\u5347\u6027\u80fd\u3002", "method": "\u6311\u6218\u8d5b\u56fa\u5b9a\u6a21\u578b\u4e0e\u8bad\u7ec3\u65b9\u6848\uff0c\u4ec5\u805a\u7126\u6570\u636e\u7b56\u5212\u7b56\u7565\u3002\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4ee5\u96be\u5ea6\u4e3a\u4e3b\u7684\u6837\u672c\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u4e0e\u589e\u52a0\u6570\u636e\u91cf\u3001\u591a\u6837\u6027\u3001\u5408\u6210\u589e\u5f3a\u7b49\u4f20\u7edf\u7b56\u7565\u8fdb\u884c\u5bf9\u6bd4\u6d88\u878d\u5206\u6790\u3002", "result": "\u901a\u8fc7\u4ee5Walton Multimodal Cold Start\u4e3a\u4e3b\u6e90\u6784\u5efa\u7cbe\u7b80\u7684\u6570\u636e\u96c6\uff0c\u8bba\u6587\u5728\u6bd4\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u3002\u6d88\u878d\u5b9e\u9a8c\u53d1\u73b0\uff0c\u57fa\u4e8e\u96be\u5ea6\u7684\u6837\u672c\u9009\u62e9\u662f\u6027\u80fd\u63d0\u5347\u7684\u6838\u5fc3\uff1b\u589e\u52a0\u6570\u636e\u91cf\u5e76\u4e0d\u663e\u8457\u63d0\u5347\u5e73\u5747\u51c6\u786e\u7387\uff0c\u53ea\u964d\u4f4e\u4e86\u7ed3\u679c\u7684\u6ce2\u52a8\uff1b\u5e38\u7528\u7684\u591a\u6837\u6027\u548c\u5408\u6210\u589e\u5f3a\u65b9\u6cd5\u65e0\u6548\u751a\u81f3\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "DCVLR\u5c5e\u4e8e\u9971\u548c\u8bc4\u4f30\u573a\u666f\uff0c\u6570\u636e\u5bf9\u9f50\u548c\u6837\u672c\u96be\u5ea6\u4f18\u5148\u4e8e\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u662f\u6570\u636e\u9ad8\u6548\u591a\u6a21\u6001\u63a8\u7406\u7684\u6838\u5fc3\u3002"}}
{"id": "2601.11012", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11012", "abs": "https://arxiv.org/abs/2601.11012", "authors": ["Jiahao Wang", "Shuangjia Zheng"], "title": "Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics", "comment": null, "summary": "The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such a continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties. The code and data are publicly available at https://github.com/GENTEL-lab/HADES.", "code_url": "https://github.com/GENTEL-lab/HADES", "code_stars": 0, "code_last_update": "2026-01-16", "AI": {"tldr": "HADES\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u91c7\u6837\u4e0e\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u86cb\u767d\u5e8f\u5217\u8bbe\u8ba1\u6027\u80fd\uff0c\u5b9e\u73b0\u7ed3\u6784\u4e0e\u529f\u80fd\u7684\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u9a8c\u8868\u73b0\u9886\u5148\uff0c\u4ee3\u7801\u5f00\u653e\u3002", "motivation": "\u73b0\u6709\u86cb\u767d\u5e8f\u5217\u4f18\u5316\u53d7\u9650\u4e8e\u5e8f\u5217\u7a7a\u95f4\u590d\u6742\u5ea6\uff08\u5982\u8868\u578b\u6613\u4f4d\u6548\u5e94\uff09\u548c\u672a\u80fd\u8003\u8651\u4e09\u7ef4\u7ed3\u6784\u7ea6\u675f\uff0c\u4e9f\u9700\u6574\u5408\u7ed3\u6784\u4fe1\u606f\u7684\u65b0\u4f18\u5316\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eHamiltonian\u52a8\u529b\u5b66\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5HADES\uff0c\u7ed3\u5408\u7ed3\u6784\u611f\u77e5\u540e\u9a8c\u91c7\u6837\u53ca\u4f4d\u7f6e\u79bb\u6563\u5316\u8fc7\u7a0b\uff0c\u8f85\u4ee5\u4e24\u9636\u6bb5\u7f16\u7801-\u89e3\u7801\u6846\u67b6\u5b66\u4e60\u7ed3\u6784-\u529f\u80fd\u5173\u7cfb\uff0c\u5e76\u5728\u4f53\u5916\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "HADES\u5728\u591a\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5b9e\u73b0\u7ed3\u6784\u4e0e\u6027\u8d28\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u4fc3\u8fdb\u86cb\u767d\u8bbe\u8ba1\u3002", "conclusion": "HADES\u65b9\u6cd5\u80fd\u9ad8\u6548\u8bbe\u8ba1\u7ed3\u6784\u53d7\u9650\u4e14\u6027\u8d28\u4f18\u5316\u7684\u86cb\u767d\u5e8f\u5217\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.11037", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11037", "abs": "https://arxiv.org/abs/2601.11037", "authors": ["Shiyu Liu", "Yongjing Yin", "Jianhao Yan", "Yunbo Tang", "Qinggang Zhang", "Bei Li", "Xin Chen", "Jingang Wang", "Xunliang Cai", "Jinsong Su"], "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search", "comment": "Code is available at https://github.com/Liushiyu-0709/BAPO-Reliable-Search", "summary": "RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.", "AI": {"tldr": "BAPO\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86RL-agent\u5728\u77e5\u8bc6\u8fb9\u754c\u5904\u627f\u8ba4\"I DON'T KNOW\"\u7684\u80fd\u529b\uff0c\u964d\u4f4e\u4e86\u4e0d\u53ef\u9760\u7b54\u6848\u7684\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u95ee\u9898\u6c42\u89e3\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684agentic search\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u590d\u6742\u95ee\u9898\u6c42\u89e3\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5408\u7406\u7684\u8fb9\u754c\u611f\u77e5\uff0c\u5f80\u5f80\u5728\u7f3a\u4e4f\u5145\u5206\u8bc1\u636e\u6216\u63a8\u7406\u529b\u8fbe\u5230\u6781\u9650\u65f6\u65e0\u6cd5\u627f\u8ba4\"I DON'T KNOW\"\uff0c\u5bfc\u81f4\u6781\u6613\u751f\u6210\u4f3c\u662f\u800c\u975e\u4f46\u4e0d\u53ef\u4fe1\u7684\u7b54\u6848\uff0c\u5e26\u6765\u5b9e\u9645\u5e94\u7528\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86Boundary-Aware Policy Optimization\uff08BAPO\uff09\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5f15\u5165\u57fa\u4e8e\u7fa4\u4f53\u7684\u8fb9\u754c\u611f\u77e5\u5956\u52b1\u4ee5\u53ca\u81ea\u9002\u5e94\u5956\u52b1\u8c03\u8282\u5668\uff0c\u7528\u4ee5\u63d0\u5347RL-agent\u80fd\u5728\u63a8\u7406\u6781\u9650\u65f6\u6070\u5f53\u5730\u8868\u8fbe\"I DON'T KNOW\"\uff0c\u800c\u975e\u5f3a\u884c\u7ed9\u51fa\u4e0d\u53ef\u9760\u7b54\u6848\u3002", "result": "\u5728\u56db\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cBAPO\u660e\u663e\u63d0\u9ad8\u4e86RL-agent\u5728agentic search\u4e2d\u7684\u6574\u4f53\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8fb9\u754c\u611f\u77e5\u5956\u52b1\u548c\u5408\u7406\u8c03\u63a7\uff0cBAPO\u6709\u6548\u63d0\u5347\u4e86agent\u5728\u9762\u5bf9\u672a\u77e5\u6216\u8bc1\u636e\u4e0d\u8db3\u65f6\u7684\u81ea\u6211\u7ea6\u675f\u80fd\u529b\uff0c\u4e3aRL\u9a71\u52a8\u7684\u590d\u6742\u95ee\u9898\u6c42\u89e3\u5e26\u6765\u4e86\u66f4\u9ad8\u7684\u53ef\u9760\u6027\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.11044", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11044", "abs": "https://arxiv.org/abs/2601.11044", "authors": ["Keyu Li", "Junhao Shi", "Yang Xiao", "Mohan Jiang", "Jie Sun", "Yunze Wu", "Shijie Xia", "Xiaojie Cai", "Tianze Xu", "Weiye Si", "Wenjie Li", "Dequan Wang", "Pengfei Liu"], "title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts", "comment": null, "summary": "Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.", "code_url": "https://github.com/GAIR-NLP/AgencyBench", "code_stars": 13, "code_last_update": "2026-01-16", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AgencyBench\uff0c\u4e00\u4e2a\u9488\u5bf9\u591a\u7ef4\u667a\u80fd\u4f53\u80fd\u529b\u7684\u5168\u9762\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u4ec5\u5173\u6ce8\u5355\u4e00\u80fd\u529b\u548c\u4eba\u5de5\u53cd\u9988\u74f6\u9888\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bc4\u4f30\u6807\u51c6\u5c40\u9650\u4e8e\u5355\u4e00\u80fd\u529b\uff0c\u4e14\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u53cd\u9988\uff0c\u65e0\u6cd5\u6269\u5c55\u548c\u81ea\u52a8\u8bc4\u4f30\uff0c\u96be\u4ee5\u53cd\u6620\u5b9e\u9645\u590d\u6742\u573a\u666f\u4e2d\u6a21\u578b\u7684\u7efc\u5408\u8868\u73b0\u3002\u63d0\u51faAgencyBench\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3001\u63a8\u8fdb\u667a\u80fd\u4f53\u8bc4\u4f30\u81ea\u52a8\u5316\u548c\u5168\u9762\u6027\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86AgencyBench\u57fa\u51c6\uff0c\u6db5\u76d66\u4e2a\u6838\u5fc3\u667a\u80fd\u4f53\u80fd\u529b\u300132\u4e2a\u573a\u666f\u3001138\u9879\u4efb\u52a1\uff0c\u5f15\u5165\u7528\u6237\u6a21\u62df\u667a\u80fd\u4f53\u8fdb\u884c\u81ea\u52a8\u5316\u53cd\u9988\u3001\u8bbe\u8ba1Docker\u6c99\u7bb1\u8fdb\u884c\u89c6\u89c9\u4e0e\u529f\u80fd\u8bc4\u6d4b\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u6a21\u578b\u5728\u5176\u4e2d\u7684\u5b9e\u9a8c\u5bf9\u6bd4\u5206\u6790\u6027\u80fd\u4e0e\u8d44\u6e90\u5229\u7528\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u5c01\u95ed\u6e90\u6a21\u578b\u6210\u7ee9\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0848.4% vs 32.1%\uff09\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u8d44\u6e90\u6548\u7387\u3001\u81ea\u6211\u7ea0\u9519\u548c\u5de5\u5177\u4f7f\u7528\u504f\u597d\u65b9\u9762\u5dee\u5f02\u660e\u663e\uff1b\u5c01\u95ed\u6e90\u6a21\u578b\u5728\u5176\u539f\u751f\u751f\u6001\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u5f00\u6e90\u6a21\u578b\u53d7\u6267\u884c\u6846\u67b6\u5f71\u54cd\u8f83\u5927\u3002", "conclusion": "AgencyBench\u663e\u793a\uff0c\u5c01\u95ed\u6e90\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u6307\u51fa\u6a21\u578b\u4e0e\u667a\u80fd\u4f53\u6846\u67b6\u9700\u8981\u534f\u540c\u4f18\u5316\uff0c\u4e3a\u672a\u6765\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2601.11089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11089", "abs": "https://arxiv.org/abs/2601.11089", "authors": ["Suhan Guo", "Jiahong Deng", "Furao Shen"], "title": "MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting", "comment": null, "summary": "Accurate forecasting of infectious disease dynamics is critical for public health planning and intervention. Human mobility plays a central role in shaping the spatial spread of epidemics, but mobility data are noisy, indirect, and difficult to integrate reliably with disease records. Meanwhile, epidemic case time series are typically short and reported at coarse temporal resolution. These conditions limit the effectiveness of parameter-heavy mobility-aware forecasters that rely on clean and abundant data. In this work, we propose the Mobility-Informed Causal Adapter (MiCA), a lightweight and architecture-agnostic module for epidemic forecasting. MiCA infers mobility relations through causal discovery and integrates them into temporal forecasting models via gated residual mixing. This design allows lightweight forecasters to selectively exploit mobility-derived spatial structure while remaining robust under noisy and data-limited conditions, without introducing heavy relational components such as graph neural networks or full attention. Extensive experiments on four real-world epidemic datasets, including COVID-19 incidence, COVID-19 mortality, influenza, and dengue, show that MiCA consistently improves lightweight temporal backbones, achieving an average relative error reduction of 7.5\\% across forecasting horizons. Moreover, MiCA attains performance competitive with SOTA spatio-temporal models while remaining lightweight.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMiCA\u6a21\u5757\uff0c\u6709\u6548\u878d\u5408\u6d41\u52a8\u4fe1\u606f\u63d0\u5347\u6d41\u884c\u75c5\u9884\u6d4b\u6027\u80fd\uff0c\u517c\u987e\u8f7b\u91cf\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u8d8a\u3002", "motivation": "\u6d41\u884c\u75c5\u9884\u6d4b\u9700\u8981\u8003\u8651\u4eba\u7fa4\u6d41\u52a8\u7684\u5f71\u54cd\uff0c\u4f46\u73b0\u6709\u7684\u6d41\u52a8\u6570\u636e\u566a\u58f0\u5927\u3001\u96be\u4e0e\u75c5\u4f8b\u6570\u636e\u7ed3\u5408\uff0c\u4e14\u6570\u636e\u91cf\u6709\u9650\uff0c\u9650\u5236\u4e86\u53c2\u6570\u590d\u6742\u7684\u6d41\u52a8\u611f\u77e5\u6a21\u578b\u7684\u5e94\u7528\u3002\u4e3a\u517c\u987e\u51c6\u786e\u6027\u4e0e\u8f7b\u91cf\u6027\uff0c\u8feb\u5207\u9700\u8981\u65b0\u7684\u6a21\u578b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMobility-Informed Causal Adapter\uff08MiCA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u56e0\u679c\u53d1\u73b0\u63a8\u65ad\u6d41\u52a8\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u6b8b\u5dee\u6df7\u5408\u5c06\u5176\u96c6\u6210\u5230\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\u4e2d\uff0c\u63d0\u5347\u5bf9\u6d41\u52a8\u7ed3\u6784\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u540c\u65f6\u9002\u5e94\u566a\u58f0\u5927\u3001\u6570\u636e\u6709\u9650\u7684\u5b9e\u9645\u60c5\u5f62\u3002", "result": "\u5728COVID-19\u53d1\u75c5\u3001COVID-19\u6b7b\u4ea1\u3001\u6d41\u611f\u548c\u767b\u9769\u70ed\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMiCA\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u964d\u4f4e7.5%\uff0c\u5e76\u4e0e\u5f53\u524d\u4e3b\u6d41\u7684\u65f6\u7a7a\u6a21\u578b\u8fbe\u6210\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "MiCA\u5728\u591a\u4e2a\u771f\u5b9e\u6d41\u884c\u75c5\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u6027\u7684\u540c\u65f6\uff0c\u6027\u80fd\u8fbe\u5230\u4e86\u4e0e\u73b0\u6709\u5148\u8fdb\u65f6\u7a7a\u6a21\u578b\u76f8\u5f53\u7684\u6c34\u5e73\u3002"}}
{"id": "2601.11100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11100", "abs": "https://arxiv.org/abs/2601.11100", "authors": ["Zhezheng Hao", "Hong Wang", "Jian Luo", "Jianqing Zhang", "Yuyan Zhou", "Qiang Lin", "Can Wang", "Hande Dong", "Jiawei Chen"], "title": "ReCreate: Reasoning and Creating Domain Agents Driven by Experience", "comment": null, "summary": "Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86ReCreate\u6846\u67b6\uff0c\u5229\u7528\u5927\u6a21\u578b\u7684\u4ea4\u4e92\u5386\u53f2\u7ecf\u9a8c\u81ea\u52a8\u751f\u6210\u548c\u6539\u8fdb\u9886\u57df\u667a\u80fd\u4f53\uff0c\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e86\u6548\u679c\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u7684\u751f\u6210\u591a\u9700\u4eba\u5de5\u8bbe\u8ba1\uff0c\u81ea\u52a8\u5316\u751f\u6210\u4f9d\u8d56\u9ed1\u76d2\u4f18\u5316\uff0c\u672a\u80fd\u9ad8\u6548\u5229\u7528\u7ecf\u9a8c\u4fe1\u606f\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d88\u8017\u5927\u3001\u6539\u8fdb\u74f6\u9888\u660e\u663e\uff0c\u4e9f\u9700\u66f4\u667a\u80fd\u4e14\u5177\u5907\u89e3\u91ca\u6027\u7684\u81ea\u52a8\u751f\u6210\u6846\u67b6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\u2460\u5b58\u50a8\u4e0e\u68c0\u7d22\u667a\u80fd\u4f53\u4ea4\u4e92\u7ecf\u9a8c\u673a\u5236\uff1b\u2461\u5c06\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u667a\u80fd\u4f53\u7ed3\u6784\u4fee\u6539\u7684\u5173\u8054\u7ba1\u9053\uff08reasoning-creating synergy pipeline\uff09\uff1b\u2462\u901a\u8fc7\u5206\u5c42\u6cdb\u5316\u63d0\u53d6\u53ef\u590d\u7528\u7684\u9886\u57df\u77e5\u8bc6\u6a21\u5f0f\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u5229\u7528\u7ecf\u9a8c\u6570\u636e\uff0c\u6307\u5bfc\u667a\u80fd\u4f53\u6301\u7eed\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cReCreate\u5728\u591a\u4e2a\u771f\u5b9e\u57df\u4efb\u52a1\u4e2d\uff0c\u5747\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u548c\u4e3b\u6d41\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u751f\u6210\u65b9\u6cd5\uff0c\u4e14\u5373\u4fbf\u521d\u59cb\u8d44\u6e90\u6709\u9650\uff08\u4ec5\u6709\u6781\u5c11\u79cd\u5b50\u7ed3\u6784\uff09\uff0c\u4e5f\u80fd\u5feb\u901f\u5b9e\u73b0\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "ReCreate\u6846\u67b6\u53ef\u81ea\u52a8\u4e14\u9ad8\u6548\u5730\u521b\u5efa\u548c\u9002\u5e94\u5404\u9886\u57df\u667a\u80fd\u4f53\uff0c\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u548c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u8f83\u5f3a\u901a\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2601.11147", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11147", "abs": "https://arxiv.org/abs/2601.11147", "authors": ["Zixu Wang", "Bingbing Xu", "Yige Yuan", "Huawei Shen", "Xueqi Cheng"], "title": "Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems", "comment": "17 pages, 4 figures, 3 tables", "summary": "Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \\textbf{SCALE}, which means \\underline{\\textbf{S}}elf prediction of the optimizer with few shot \\underline{\\textbf{CAL}}ibration for \\underline{\\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \\textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\\%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u751f\u6210\u548c\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6d41\u7a0b\u7684\u6210\u672c\u4e0e\u6548\u679c\u95ee\u9898\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4f4e\u6210\u672c\u7684\u4efb\u52a1\u7ea7\u522b\u751f\u6210\u4e0e\u8bc4\u4f30\u6846\u67b6SCALE\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u4f46\u5927\u5e45\u964d\u4f4e\u4e86\u7b97\u529b\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u6d41\u7a0b\u751f\u6210\u4e0e\u8bc4\u4f30\u5b58\u5728\u6781\u9ad8\u7684token\u6210\u672c\u4e0e\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4efb\u52a1\u7ea7\u4e0e\u67e5\u8be2\u7ea7\u6d41\u7a0b\u751f\u6210\u6548\u8d39\u6bd4\u4e0d\u660e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u7701\u8d44\u6e90\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u7b80\u5316\u6d41\u7a0b\u751f\u6210\u8bc4\u4f30\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86SCALE\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9884\u6d4b\u4e0e\u5c11\u91cf\u6821\u51c6\u5b9e\u73b0\u4efb\u52a1\u7ea7\u522b\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\u7684\u751f\u6210\u548c\u8bc4\u4f30\uff0c\u6452\u5f03\u4e86\u9ad8\u6210\u672c\u7684\u5168\u9762\u6267\u884c\u5f0f\u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u53d7\u81ea\u6211\u8fdb\u5316\u4e0e\u751f\u6210\u5f0f\u5956\u52b1\u5efa\u6a21\u542f\u53d1\uff0c\u4e3b\u5f20\u5229\u7528\u6700\u4f73\u82e5\u5e72\u4efb\u52a1\u7ea7\u65b9\u6848\u8986\u76d6\u5927\u90e8\u5206\u67e5\u8be2\uff0c\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u67e5\u8be2\u5355\u72ec\u751f\u6210\u6d41\u7a0b\u3002", "result": "SCALE\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u635f\u5931\u6781\u5c0f\uff080.61%\uff09\uff0c\u4f46\u6574\u4f53\u7b97\u529b\u5f00\u9500\u4e0b\u964d\u53ef\u8fbe83%\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "SCALE\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4e86token\u6d88\u8017\uff08\u6700\u9ad883%\uff09\uff0c\u6027\u80fd\u4ec5\u7565\u6709\u4e0b\u964d\uff08\u5e73\u57470.61%\uff09\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4efb\u52a1\u6d41\u7a0b\u751f\u6210\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11178", "categories": ["cs.AI", "cs.CL", "cs.MM", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.11178", "abs": "https://arxiv.org/abs/2601.11178", "authors": ["Girish A. Koushik", "Helen Treharne", "Diptesh Kanojia"], "title": "TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech", "comment": "Under review at ICWSM 2026", "summary": "Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as \"black boxes\" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TANDEM\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u8de8\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u4e86\u591a\u6a21\u6001\u957f\u5185\u5bb9\u4e2d\u7684\u4ec7\u6068\u68c0\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\uff0c\u5b9e\u73b0\u5728\u76ee\u6807\u8bc6\u522b\u548c\u65f6\u95f4\u5b9a\u4f4d\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u957f\u683c\u5f0f\u591a\u6a21\u6001\u5185\u5bb9\u65e5\u76ca\u589e\u591a\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u7cfb\u7edf\u867d\u7136\u80fd\u9ad8\u6548\u8bc6\u522b\u4ec7\u6068\u8a00\u8bba\uff0c\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u8bc1\u636e\uff0c\u96be\u4ee5\u6ee1\u8db3\u4eba\u5de5\u5ba1\u6838\u9700\u6c42\u3002\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u6027\u80fd\u4f18\u5f02\u7684\u591a\u6a21\u6001\u5185\u5bb9\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0cTANDEM\u5c06\u97f3\u89c6\u9891\u4ec7\u6068\u68c0\u6d4b\u7531\u7b80\u5355\u7684\u4e8c\u5206\u7c7b\u5347\u7ea7\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\uff0c\u5f15\u5165\u4e86\u89c6\u89c9-\u8bed\u8a00\u548c\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u7684\u534f\u540c\u5f3a\u5316\u5b66\u4e60\u673a\u5236\uff0c\u5229\u7528\u81ea\u7ea6\u675f\u7684\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u7a33\u5b9a\u957f\u671f\u5e8f\u5217\u4e2d\u7684\u63a8\u7406\uff0c\u65e0\u9700\u5bc6\u96c6\u5e27\u7ea7\u76d1\u7763\u3002\u901a\u8fc7\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728HateMM\u6570\u636e\u96c6\u7684\u76ee\u6807\u8bc6\u522b\u4efb\u52a1\u4e0a\uff0cTANDEM\u83b7\u5f97\u4e860.73\u7684F1\u5206\u6570\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8630%\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u5b9a\u4f4d\u4e0a\u4e5f\u4fdd\u8bc1\u4e86\u9ad8\u7cbe\u5ea6\u3002\u96f6\u6837\u672c\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\u57fa\u7ebf\u5747\u88ab\u663e\u8457\u8d85\u8d8a\u3002\u4f46\u5728\u591a\u7c7b\u522b\u533a\u5206\uff08\u5982\u4fae\u8fb1\u4e0e\u4ec7\u6068\u5185\u5bb9\uff09\u65b9\u9762\u4ecd\u5b58\u5728\u6807\u7b7e\u6b67\u4e49\u4e0e\u6570\u636e\u4e0d\u5747\u8861\u95ee\u9898\u3002", "conclusion": "TANDEM\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4ec7\u6068\u5185\u5bb9\u7684\u76ee\u6807\u8bc6\u522b\u548c\u65f6\u95f4\u5b9a\u4f4d\u7684\u51c6\u786e\u7387\uff0c\u4e3a\u5b9e\u73b0\u66f4\u900f\u660e\u548c\u53ef\u64cd\u4f5c\u7684\u5185\u5bb9\u5b89\u5168\u5ba1\u6838\u5de5\u5177\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u8be5\u6846\u67b6\u8bc1\u5b9e\u5728\u590d\u6742\u591a\u6a21\u6001\u73af\u5883\u4e0b\u5b9e\u73b0\u53ef\u89e3\u91ca\u5bf9\u9f50\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2601.11252", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11252", "abs": "https://arxiv.org/abs/2601.11252", "authors": ["Qianyue Wang", "Jinwu Hu", "Yufeng Wang", "Huanxiang Lin", "Bolin Chen", "Zhiquan Wen", "Yaofo Chen", "Mingkui Tan"], "title": "Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning", "comment": null, "summary": "Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.", "AI": {"tldr": "\u63d0\u51faThink-with-Me\u4ea4\u4e92\u5f0f\u63a8\u7406\u8303\u5f0f\uff0c\u5728\u5408\u9002\u8282\u70b9\u5f15\u5165\u5916\u90e8\u53cd\u9988\u663e\u8457\u63d0\u5347LRM\u63a8\u7406\u6548\u7387\u4e0e\u51c6\u786e\u7387\uff0c\u83b7\u5f97\u5b9e\u8bc1\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LRMs\u5728\u591a\u6b65\u63a8\u7406\u65f6\u5e38\u56e0\u8fc7\u5ea6\u63a8\u7406\u6216\u65b9\u5411\u504f\u79bb\u800c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u4e0e\u6027\u80fd\u4e0b\u964d\uff0c\u4f20\u7edf\u95ed\u73af\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u7f3a\u4e4f\u5916\u90e8\u5e72\u9884\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6d4b\u8bd5\u65f6\u4ea4\u4e92\u5f0f\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u52a0\u5165\u5916\u90e8\u53cd\u9988\u5e72\u9884\uff0c\u5229\u7528\u8f6c\u6298\u8bcd\u4f5c\u4e3a\u5e72\u9884\u70b9\uff0c\u7ed3\u5408\u591a\u6807\u51c6\u8bc4\u4f30\uff08\u5408\u7406\u6027\u4e0e\u5b8c\u6574\u6027\uff09\uff0c\u5e76\u901a\u8fc7GRPO\u8bad\u7ec3\u6a21\u578b\u4ee5\u9002\u5e94\u8be5\u6a21\u5f0f\u3002", "result": "Think-with-Me\u5728AIME24\u4e0a\u6bd4QwQ-32B\u63d0\u5347\u4e867.19%\u51c6\u786e\u7387\uff0c\u5e76\u57288K\u7a97\u53e3\u4e0b\u5c06\u5e73\u5747\u63a8\u7406\u957f\u5ea6\u51cf\u5c11\u4e8681%\uff1b\u540c\u65f6\u5728\u5b89\u5168\u4e0e\u521b\u65b0\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "conclusion": "Think-with-Me\u663e\u8457\u63d0\u9ad8\u4e86LRMs\u7684\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u7387\uff0c\u5728\u53d7\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0b\u8fbe\u5230\u4e86\u66f4\u4f18\u7684\u63a8\u7406\u957f\u5ea6\u548c\u6027\u80fd\u5e73\u8861\u3002"}}
{"id": "2601.11286", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11286", "abs": "https://arxiv.org/abs/2601.11286", "authors": ["Weihong Qi", "Fan Huang", "Rasika Muralidharan", "Jisun An", "Haewoon Kwak"], "title": "XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making", "comment": null, "summary": "We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.", "AI": {"tldr": "\u63d0\u51faXChoice\u673a\u5236\u6a21\u578b\uff0c\u7528\u4e8eAI\u4e0e\u4eba\u7c7b\u51b3\u7b56\u5bf9\u9f50\u7684\u89e3\u91ca\u6027\u8bc4\u4f30\uff0c\u63ed\u793a\u4e0d\u540c\u7fa4\u4f53\u95f4\u5b58\u5728\u5bf9\u9f50\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7RAG\u6539\u5584\u3002", "motivation": "\u73b0\u6709AI-\u4eba\u7c7b\u5bf9\u9f50\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u7ed3\u679c\u4e00\u81f4\u6027\uff0c\u7f3a\u4e4f\u5bf9\u51b3\u7b56\u673a\u5236\u5c42\u9762\u4e00\u81f4\u6027\u7684\u89e3\u91ca\u6027\u8861\u91cf\u624b\u6bb5\uff0c\u6025\u9700\u66f4\u6df1\u5165\u8bca\u65ad\u548c\u6539\u8fdbAI\u5bf9\u9f50\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u673a\u5236\u6a21\u578b\u62df\u5408AI/\u4eba\u7c7b\u51b3\u7b56\u6570\u636e\uff0c\u53c2\u6570\u5316\u51b3\u7b56\u89c4\u5219\uff0c\u5e76\u5bf9\u6a21\u578b\u3001\u9009\u9879\u548c\u5b50\u7fa4\u4f53\u95f4\u7684\u53c2\u6570\u5411\u91cf\u8fdb\u884c\u6bd4\u8f83\uff0c\u8f85\u4ee5\u4e0d\u53d8\u6027\u5206\u6790\u548cRAG\u5e72\u9884\u8bc4\u4f30\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86XChoice\u6846\u67b6\uff0c\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30AI\u4e0e\u4eba\u7c7b\u5728\u53d7\u9650\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5bf9\u9f50\u65b9\u5f0f\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5173\u6ce8\u51b3\u7b56\u7ed3\u679c\u7684\u4e00\u81f4\u6027\uff08\u5982\u51c6\u786e\u7387\u3001F1\u5206\u6570\uff09\uff0c\u800c\u4e14\u901a\u8fc7\u62df\u5408\u57fa\u4e8e\u673a\u5236\u7684\u51b3\u7b56\u6a21\u578b\uff0c\u63d0\u53d6\u53ef\u89e3\u91ca\u53c2\u6570\uff08\u5982\u51b3\u7b56\u56e0\u7d20\u6743\u91cd\u3001\u5bf9\u7ea6\u675f\u7684\u654f\u611f\u6027\u4e0e\u6743\u8861\uff09\u3002\u8bba\u6587\u91c7\u7528\u7f8e\u56fd\u65f6\u95f4\u4f7f\u7528\u8c03\u67e5\uff08ATUS\uff09\u6570\u636e\u4f5c\u4e3a\u4eba\u7c7b\u5bf9\u7167\u57fa\u51c6\uff0c\u5bf9\u6bcf\u65e5\u65f6\u95f4\u5206\u914d\u7684AI\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u3001\u6d3b\u52a8\u53ca\u5b50\u7fa4\u4f53\u95f4\u5bf9\u9f50\u7a0b\u5ea6\u5f02\u8d28\uff0c\u9ed1\u4eba\u53ca\u5df2\u5a5a\u7fa4\u4f53\u4e2d\u5b58\u5728\u663e\u8457\u9519\u914d\u3002\u6846\u67b6\u8fd8\u5305\u62ec\u7a33\u5065\u6027\u9a8c\u8bc1\u548cRAG\u5e72\u9884\u7684\u6548\u679c\u8bc4\u4f30\uff0c\u7efc\u5408\u63d0\u4f9b\u673a\u5236\u5c42\u9762\u7684\u5bf9\u9f50\u8861\u91cf\u4e0e\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "XChoice\u80fd\u591f\u8bca\u65adAI-\u4eba\u7c7b\u51b3\u7b56\u673a\u5236\u7684\u9519\u914d\uff0c\u4e3a\u8d85\u8d8a\u7ed3\u679c\u4e00\u81f4\u6027\u3001\u5b9e\u73b0\u673a\u5236\u4e00\u81f4\u63d0\u4f9b\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u53d1\u73b0\u548c\u5b9a\u5411\u6539\u8fdbAI\u6a21\u578b\u7684\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2601.11354", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11354", "abs": "https://arxiv.org/abs/2601.11354", "authors": ["Weiyi Wang", "Xinchi Chen", "Jingjing Gong", "Xuanjing Huang", "Xipeng Qiu"], "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems", "comment": null, "summary": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u7a7a\u95f4\u89c4\u5212\u95ee\u9898\u7684\u65b0\u57fa\u51c6 AstroReason-Bench\uff0c\u7ed3\u679c\u53d1\u73b0\uff0c\u4e3b\u6d41\u901a\u7528 agentic LLM \u5728\u73b0\u5b9e\u7269\u7406\u7ea6\u675f\u573a\u666f\u4e0b\u8fdc\u900a\u4e8e\u4e13\u4e1a\u5de5\u5177\uff0c\u8fd9\u4e3a\u672a\u6765\u76f8\u5173\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u8bca\u65ad\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9 agentic LLM \u7684\u57fa\u51c6\u6d4b\u8bd5\u591a\u805a\u7126\u4e8e\u7b26\u53f7\u6216\u5f31\u7269\u7406\u7ea6\u675f\u73af\u5883\uff0c\u7f3a\u4e4f\u5728\u4e25\u683c\u7269\u7406\u7ea6\u675f\u3001\u73b0\u5b9e\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u7684\u8bc4\u6d4b\uff0c\u4e9f\u9700\u62d3\u5c55\u81f3\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4ee5\u8bca\u65ad\u5176\u5b9e\u9645\u80fd\u529b\u4e0e\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u4e86 AstroReason-Bench \u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u79cd\u8c03\u5ea6\u60c5\u5883\uff0c\u4e3a\u7a7a\u95f4\u89c4\u5212\u95ee\u9898\u8bbe\u5b9a\u7edf\u4e00\u7684 agent \u4ea4\u4e92\u534f\u8bae\uff0c\u5e76\u5bf9\u591a\u79cd\u4e3b\u6d41 agentic LLM \u7cfb\u7edf\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5728 AstroReason-Bench \u4e0a\uff0c\u591a\u79cd\u5f00\u6e90\u548c\u95ed\u6e90 agentic LLM \u7cfb\u7edf\u5747\u8868\u73b0\u4e0d\u4f73\uff0c\u8fdc\u4f4e\u4e8e\u4e13\u4e1a\u89e3\u7b97\u5668\u6c34\u5e73\u3002\u8be5\u57fa\u51c6\u63ed\u793a\u4e86\u901a\u7528 LLM \u5728\u957f\u5468\u671f\u3001\u7269\u7406\u7ea6\u675f\u590d\u6742\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u4e0d\u8db3\u3002", "conclusion": "\u73b0\u6709\u7684\u901a\u7528\u578b agentic LLM \u5728\u9762\u5bf9\u5177\u6709\u4e25\u683c\u7269\u7406\u7ea6\u675f\u7684\u7a7a\u95f4\u89c4\u5212\u95ee\u9898\u65f6\uff0c\u6027\u80fd\u660e\u663e\u900a\u4e8e\u4e13\u4e1a\u89e3\u7b97\u5668\uff0c\u66b4\u9732\u4e86\u5176\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u7684\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2601.11389", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11389", "abs": "https://arxiv.org/abs/2601.11389", "authors": ["Hedieh Haddad", "Thibault Falque", "Pierre Talbot", "Pascal Bouvry"], "title": "Hyperparameter Optimization of Constraint Programming Solvers", "comment": "28 pages, 3 figures. Submitted to Journal of Combinatorial Optimization. Special Issue: Recent applications, models and algorithms in Combinatorial Optimization", "summary": "The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.\n  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.\n  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8d85\u53c2\u6570\u4f18\u5316\u673a\u5236\uff08probe and solve algorithm\uff09\uff0c\u5728\u591a\u5b9e\u4f8b\u6d4b\u8bd5\u4e2d\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u663e\u8457\u4f18\u4e8e\u9ed8\u8ba4\u914d\u7f6e\u4e0e\u5c40\u90e8\u641c\u7d22\uff0c\u63d0\u5347\u4e86\u591a\u6cdb\u5316\u573a\u666f\u4e0b\u7684\u89e3\u8d28\u91cf\u3002", "motivation": "\u7ea6\u675f\u89c4\u5212\u6c42\u89e3\u5668\u7684\u6027\u80fd\u5bf9\u8d85\u53c2\u6570\u9009\u62e9\u9ad8\u5ea6\u654f\u611f\uff0c\u624b\u52a8\u8c03\u53c2\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u9ad8\u6548\u7684\u4f18\u5316\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86probe and solve\u7b97\u6cd5\uff0c\u5728CPMpy\u5e93\u4e2d\u5b9e\u73b0\uff0c\u5206\u4e3a\u63a2\u6d4b\uff08\u8d85\u53c2\u6570\u4f18\u5316\uff09\u548c\u6c42\u89e3\u4e24\u4e2a\u9636\u6bb5\u3002\u63a2\u6d4b\u9636\u6bb5\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u6c49\u660e\u8ddd\u79bb\u641c\u7d22\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u8d1d\u53f6\u65af\u4f18\u5316\u5728ACE\u6c42\u89e3\u5668\u4e0a\u63d0\u5347\u4e8625.4%\u7684\u5b9e\u4f8b\u7ed3\u679c\uff0c\u5e76\u572857.9%\u7684\u5b9e\u4f8b\u4e0a\u4fdd\u6301\u6027\u80fd\uff1b\u5728Choco\u4e0a\u63d0\u5347\u4e8638.6%\u7684\u5b9e\u4f8b\u3002\u8d1d\u53f6\u65af\u4f18\u5316\u660e\u663e\u4f18\u4e8e\u6c49\u660e\u8ddd\u79bb\u641c\u7d22\u3002", "conclusion": "probe and solve\u7b97\u6cd5\u4e3a\u7ea6\u675f\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u3001\u8d44\u6e90\u611f\u77e5\u7684\u8d85\u53c2\u6570\u4f18\u5316\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u95ee\u9898\u4e2d\u53d6\u5f97\u4e86\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2601.11468", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.11468", "abs": "https://arxiv.org/abs/2601.11468", "authors": ["Alessandro Padella", "Massimiliano de Leoni", "Marlon Dumas"], "title": "Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs", "comment": "19 pages, 4 figure, TMIS journal submission", "summary": "Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u4e86LLM\u5728\u8fc7\u7a0b\u76d1\u63a7\u4e2d\u5bf9\u4e0d\u540cKPI\u7684\u4f18\u8d8a\u9884\u6d4b\u80fd\u529b\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u5176\u63a8\u7406\u80fd\u529b\u8d85\u8d8a\u4e86\u666e\u901a\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u63d0\u5347\u8fc7\u7a0b\u6316\u6398\u4e2d\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u9884\u6d4b\u6548\u679c\uff0c\u5e76\u63a2\u7a76LLM\u80fd\u5426\u901a\u8fc7\u5176\u5148\u9a8c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u8fc7\u7a0b\u76d1\u63a7\u7684\u6cdb\u5316\u6027\u4e0e\u89e3\u91ca\u6027\u3002", "method": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7prompting\u8fdb\u884c\u9884\u6d4b\uff0c\u5c06\u65b9\u6cd5\u62d3\u5c55\u81f3\u5bf9\u591a\u79cdKPI\uff08\u5982\u603b\u65f6\u95f4\u3001\u6d3b\u52a8\u53d1\u751f\uff09\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u7ed3\u5408\u591a\u7ec4\u4e8b\u4ef6\u65e5\u5fd7\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u6d4b\uff0c\u5e76\u6df1\u5165\u5256\u6790\u6a21\u578b\u7684\u63a8\u7406\u673a\u5236\u3002", "result": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\u65b9\u6cd5\uff0c\u91cd\u70b9\u5728\u4e8e\u603b\u65f6\u95f4\u9884\u6d4b\u7684\u57fa\u7840\u4e0a\uff0c\u62d3\u5c55\u8bc4\u4f30\u5176\u901a\u7528\u6027\u3001\u8bed\u4e49\u5229\u7528\u80fd\u529b\u3001\u63a8\u7406\u673a\u5236\uff0c\u5e76\u6269\u5c55\u5230\u591a\u4e2a\u5173\u952e\u7ee9\u6548\u6307\u6807\uff08KPI\uff09\u9886\u57df\u3002\u5b9e\u8bc1\u5206\u6790\u57fa\u4e8e\u4e09\u7ec4\u4e0d\u540c\u7684\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u6db5\u76d6\u603b\u65f6\u95f4\u548c\u6d3b\u52a8\u53d1\u751f\u7b49KPI\u4efb\u52a1\uff0c\u7ed3\u679c\u663e\u793a\u5728\u6570\u636e\u7a00\u7f3a\uff08\u4ec5100\u6761\u8f68\u8ff9\uff09\u60c5\u51b5\u4e0b\uff0cLLM\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\uff0cLLM\u4e0d\u4ec5\u5229\u7528\u4e86\u5176\u5185\u5728\u7684\u5148\u9a8c\u77e5\u8bc6\u4e0e\u8bad\u7ec3\u6570\u636e\u95f4\u7684\u76f8\u5173\u6027\uff0c\u8fd8\u5c55\u73b0\u51fa\u9ad8\u9636\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u7684\u7b80\u5355\u590d\u73b0\u3002", "conclusion": "LLM\u5728\u9884\u6d4b\u8fc7\u7a0b\u76d1\u63a7\u4efb\u52a1\u4e2d\u5177\u6709\u8f83\u5f3a\u7684\u901a\u7528\u6027\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u4e0d\u8db3\u65f6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u80fd\u5229\u7528\u81ea\u8eab\u4e30\u5bcc\u7684\u8bed\u4e49\u77e5\u8bc6\u4e0e\u9ad8\u9636\u63a8\u7406\u8fdb\u884c\u7cbe\u51c6\u9884\u6d4b\u3002"}}
{"id": "2601.11479", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11479", "abs": "https://arxiv.org/abs/2601.11479", "authors": ["Yohai Trabelsi", "Guojun Xiong", "Fentabil Getnet", "St\u00e9phane Verguet", "Milind Tambe"], "title": "Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning", "comment": null, "summary": "Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.", "AI": {"tldr": "\u63d0\u51fa\u4eba\u673a\u878d\u5408LEG\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u6a21\u578b\u7406\u89e3\u4e13\u5bb6\u610f\u56fe\u4e0e\u4f18\u5316\u7b97\u6cd5\uff0c\u4f18\u9009\u536b\u751f\u7ad9\u5347\u7ea7\u540d\u5355\uff0c\u517c\u987e\u7406\u8bba\u8986\u76d6\u4e0e\u5b9e\u9645\u9700\u6c42\uff0c\u5b9e\u6d4b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u65e2\u4fdd\u8bc1\u5347\u7ea7\u536b\u751f\u8bbe\u65bd\u8986\u76d6\u6700\u5927\u4eba\u53e3\uff0c\u540c\u65f6\u5145\u5206\u8003\u8651\u6765\u81ea\u4e0d\u540c\u4e13\u5bb6\u4e0e\u5229\u76ca\u76f8\u5173\u65b9\u7684\u591a\u6837\u5316\u4e3b\u89c2\u504f\u597d\u3002\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u91cf\u5316\u76ee\u6807\uff0c\u96be\u4ee5\u6574\u5408\u4eba\u7c7b\u4e13\u4e1a\u76f4\u89c9\u3002", "method": "\u63d0\u51fa\u4e86LEG\uff08Large language model and Extended Greedy\uff09\u6846\u67b6\uff0c\u5c06\u4f18\u5316\u7b97\u6cd5\u4e0e\u5927\u6a21\u578b\u9a71\u52a8\u7684\u4eba\u673a\u8fed\u4ee3\u7ed3\u5408\uff0c\u5b9e\u73b0\u57ce\u4e61\u536b\u751f\u7ad9\u9009\u5740\u5347\u7ea7\u7684\u6700\u4f18\u8986\u76d6\u4e0e\u4e13\u5bb6\u77e5\u8bc6\u878d\u5408\u51b3\u7b56\u3002", "result": "\u5728\u57c3\u585e\u4fc4\u6bd4\u4e9a\u4e09\u4e2a\u533a\u57df\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u65e2\u80fd\u4fdd\u8bc1\u6700\u4f18\u6216\u8fd1\u4f3c\u6700\u4f18\u7684\u4eba\u53e3\u8986\u76d6\uff0c\u53c8\u80fd\u6709\u6548\u53cd\u6620\u4e13\u5bb6\u548c\u5229\u76ca\u76f8\u5173\u65b9\u7684\u610f\u613f\uff0c\u5b9e\u73b0\u4eba\u673a\u8054\u5408\u4f18\u5316\u9009\u5740\u65b9\u6848\u3002", "conclusion": "LEG\u6846\u67b6\u4e3a\u516c\u5171\u536b\u751f\u57fa\u7840\u8bbe\u65bd\u63d0\u5347\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u3001\u53ef\u63a8\u5e7f\u7684\u79d1\u5b66\u51b3\u7b56\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u7406\u8bba\u6700\u4f18\u4e0e\u5b9e\u9645\u504f\u597d\u6709\u673a\u7ed3\u5408\uff0c\u52a9\u529b\u5b9e\u73b0\u66f4\u516c\u5e73\u3001\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u536b\u751f\u5065\u5eb7\u89c4\u5212\u3002"}}
{"id": "2601.11492", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11492", "abs": "https://arxiv.org/abs/2601.11492", "authors": ["Kaiwen Wang", "Kaili Zheng", "Rongrong Deng", "Qingmin Fan", "Milin Zhang", "Zongrui Li", "Xuesi Zhou", "Bo Han", "Liren Chen", "Chenyi Guo", "Ji Wu"], "title": "BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics", "comment": null, "summary": "Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.", "AI": {"tldr": "BoxMind\u901a\u8fc7\u7cbe\u7ec6\u5206\u89e3\u62f3\u51fb\u52a8\u4f5c\u3001\u878d\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u6218\u672f\u7279\u5f81\uff0c\u4ee5\u56fe\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u548c\u63a8\u8350\uff0c\u5b9e\u73b0\u4e86\u540c\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u7684\u6218\u672f\u5efa\u8bae\uff0c\u5e76\u52a9\u529b\u4e2d\u56fd\u961f\u5728\u5965\u8fd0\u53d6\u5f97\u7a81\u7834\u3002", "motivation": "\u73b0\u6709\u7684\u7ade\u6280\u4f53\u80b2\u6218\u672f\u5206\u6790\u4e2d\uff0c\u683c\u6597\u7c7b\u9879\u76ee\uff08\u5982\u62f3\u51fb\uff09\u7684AI\u5206\u6790\u53d7\u9650\u4e8e\u52a8\u4f5c\u590d\u6742\u3001\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u6218\u672f\u8868\u793a\uff0c\u53d1\u5c55\u6ede\u540e\u3002", "method": "\u5b9a\u4e49\u539f\u5b50\u7ea7\u62f3\u51fb\u4e8b\u4ef6\u53ca18\u9879\u5206\u5c42\u6280\u6218\u672f\u6307\u6807\uff0c\u6784\u5efa\u56fe\u7ed3\u6784\u9884\u6d4b\u6a21\u578b\uff0c\u878d\u5408\u663e\u5f0f\u6280\u6218\u672f\u7279\u5f81\u4e0e\u6df1\u5ea6\u53ef\u5b66\u4e60\u65f6\u53d8\u9690\u53d8\u91cf\uff1b\u5c06\u6bd4\u8d5b\u7ed3\u679c\u5efa\u6a21\u4e3a\u6280\u6218\u672f\u6307\u6807\u7684\u53ef\u5fae\u51fd\u6570\uff0c\u901a\u8fc7\u68af\u5ea6\u6307\u5bfc\u6218\u672f\u8c03\u6574\u3002", "result": "\u63d0\u51fa\u7684BoxMind\u7cfb\u7edf\u5728BoxerGraph\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e8669.8%\u51c6\u786e\u7387\uff0c\u5728\u5965\u8fd0\u6bd4\u8d5b\u4e0a\u8fbe\u5230\u4e8687.5%\u51c6\u786e\u7387\u3002\u7cfb\u7edf\u57282024\u5e74\u5df4\u9ece\u5965\u8fd0\u4f1a\u671f\u95f4\u7684\u95ed\u73af\u90e8\u7f72\uff0c\u76f4\u63a5\u63a8\u52a8\u4e2d\u56fd\u56fd\u5bb6\u961f\u53d6\u5f973\u91d12\u94f6\u7684\u5386\u53f2\u6027\u6210\u7ee9\u3002", "conclusion": "BoxMind\u53ef\u5b9e\u73b0\u4ece\u975e\u7ed3\u6784\u5316\u89c6\u9891\u5230\u53ef\u6267\u884c\u6218\u672f\u7b56\u7565\u7684\u8f6c\u5316\uff0c\u5960\u5b9a\u4e86\u683c\u6597\u7c7b\u8fd0\u52a8AI\u5206\u6790\u548c\u51b3\u7b56\u652f\u6301\u7684\u65b0\u8303\u5f0f\u3002"}}
