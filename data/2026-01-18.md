<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 该论文提出了一个分析AI生存风险的通用框架，构建了基于两个前提出发的生存故事分类法，并探讨了每种故事面临的挑战与应对策略，进而对AI导致人类灭亡的概率进行了估算。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI是否对人类构成生存威胁的争论愈演愈烈。该研究旨在梳理对AI生存威胁的主要思路，系统分析各种可能的未来情境，并为应对AI风险提供理论依据。

Method: 论文通过分析关于AI生存威胁的典型两前提论证，构建生存故事的分类法，考察各自前提失效的不同情境，探讨每种情境下的应对挑战，并对P(doom)进行了定性或粗略量化分析。

Result: 通过提出生存故事分类法，论文明确了可能避免人类因AI灭亡的不同路径，分析了每种路径下应对挑战的差异，并提出这些分析能够帮助估算人类因AI灭亡的概率。

Conclusion: 不同的生存故事面临不同挑战，并需采取不同的应对措施。本文提出的分类法为理解和管理AI带来的生存风险提供了新视角，并对AI导致人类灭亡的概率做出粗略估算。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [2] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: 提出主动视觉感知RL方法GUI-Eyes，实现较少标注高精度，显著超越传统基线。


<details>
  <summary>Details</summary>
Motivation: 现有GUI自动化方法以被动、单次观察为主，缺乏针对复杂GUI环境适应性观察和奖励密度，需要有主动感知和更密集监督的新方法。

Method: 采用两阶段推理（粗略探索+细粒度定位）的分层策略，结合空间连续奖励函数，智能体可主动决策和优化视觉工具使用。

Result: 该论文提出了一种用于图形用户界面（GUI）任务的主动视觉感知强化学习框架 GUI-Eyes。利用两阶段推理过程和分层策略，智能体可以主动决策是否以及如何使用视觉工具（如裁剪、放大）来获取更多信息性观察。为解决GUI环境下奖励稀疏的问题，论文设计了空间连续的奖励函数，将位置接近性和区域重叠性结合，为工具使用提供密集反馈。在ScreenSpot-Pro基准上，GUI-Eyes-3B模型仅利用3000个标注样本便达到44.8%的定位精度，显著优于现有监督式和RL方法。实验证明工具感知的主动视觉感知及细粒度奖励反馈对鲁棒且数据高效的GUI智能体至关重要。

Conclusion: 工具感知的主动视觉策略和细粒度奖励机制能显著提升GUI任务中智能体的数据效率和鲁棒性。

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [3] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 实验发现，公开或实际使用大语言模型完成任务的人会受到他人收入上的实际惩罚，且惩罚程度与使用程度相关，显示LLM所带来的效率提升伴随着社会制裁成本。


<details>
  <summary>Details</summary>
Motivation: 前期研究聚焦AI用户的社会标签，未明确负面态度是否会导致实际、具成本的惩罚行为。该文旨在提供直接的行为学证据，测量对基于LLM完成实际任务的惩罚意愿。

Method: 采用两阶段在线实验，第一阶段确定“任务目标”组，第二阶段491名参与者可用自有奖金削减他人奖励，依据对方是否、以及自报或实际使用LLM的情形。

Result: 该论文通过两阶段线上实验，分析了他人对使用大语言模型（LLMs）完成任务者的实际惩罚行为。实验中，参与者可用自己的部分奖金，主动削减依赖或未依赖LLM的同伴收入。结果发现，独立依赖模型者的收入平均被削减36%，且LLM使用越多的参与者受到的惩罚越严重。特别地，如果参与者自称“未使用”LLM但实际被怀疑时，比真实未使用者受到更强惩罚。相反，高水平实际使用者又比自称使用者受惩罚更重。这揭示了效率提升的同时伴随显著社会惩罚。

Conclusion: LLM使用带来的效率优势会以社会惩罚为代价，社会成员对试图隐藏LLM使用情况表示怀疑，并愿意实际为惩罚行为付出成本。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [4] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 该论文提出了一种非交互式的端到端逻辑推理框架，通过在few-shot提示中引入结构信息，并通过注意力引导推理，无需依赖外部资源或复杂流程。核心方法Attention-Aware Intervention (AAI)能有效提升LLM的逻辑推理能力且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 当前LLM逻辑推理方案依赖复杂的交互式框架或外部组件，存在计算负担或依赖性强、扩展性差的问题。作者希望实现无需外部资源的端到端推理，简化流程的同时提升泛化能力和可分析性。

Method: 作者提出了一种Attention-Aware Intervention (AAI)方法，在推理阶段对经过识别的包含逻辑模式的注意力头进行重新加权，从而引导模型在推理中的信息流，提升模型的逻辑推理能力。该方法基于在few-shot提示中加入结构性信息，激活与逻辑算子对齐的注意力头。

Result: 实验证明AAI方法在多种主流推理基准和模型中均可提升逻辑推理表现，计算负担微乎其微。

Conclusion: AAI方法可有效提升现有大语言模型在多种推理基准中的逻辑推理表现，同时只带来极小计算成本，无需外部组件，利于实际部署和扩展。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [5] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek显著提升了大型推理模型在不同推理长度下的准确性和稳定性，效率高，突破了传统方法的精调和上下文长度限制。


<details>
  <summary>Details</summary>
Motivation: 现有的序列测试时缩放方法虽然能提升大型推理模型的准确率，但在推理长度过长时准确率反而下降，模型不稳定，需要对推理长度精调，因此存在明显局限。

Method: 仅保留一个额外推理步骤产生的KV对至KV缓存，同时采用自定义KV缓存，存储去除位置嵌入的keys，并在生成新思路前动态连续编码，从而支持超长上下文推理和高效计算。

Result: 提出了一种新方法Min-Seek，可以在广泛的推理长度下稳定提升模型准确率，无需推理长度精调，且计算效率高。

Conclusion: 所提出Min-Seek能够在不用精调推理长度的条件下，大幅提升各种推理任务中的模型准确率和稳定性，且在一定条件下具有线性复杂度。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [6] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 本研究用认识论和计算可靠主义重塑人-AI互补性，将其从预测准确性指标转向人-AI团队可靠性的衡量。互补性有助于校准决策流程可靠性，为实际应用（医疗、管理、监管）提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 人-AI互补性提出，支持AI系统的人在决策过程中能超过人类或AI单独表现。该概念在人-AI交互领域受关注，但缺乏理论基础，仅作为预测精度的后验指标，忽略了其他需求，也未考虑性能提升的成本。导致互补性在实际应用中难以获得，亟需理论完善。

Method: 作者采用认识论，将人-AI互补性融入关于可证实性AI的讨论，借助计算可靠主义理论，将历史互补性案例视为人-AI交互是否为可靠的认知过程的证据，并结合团队是否符合同步标准和实践的可靠性指标进行分析。

Result: 互补性不仅提升预测准确性，更用于校准人-AI团队的可靠性，对实际决策有指导意义（如患者、管理者、监管者）。强调互补性的价值体现在可靠的AI支持流程而非简单准确性提升。

Conclusion: 通过认识论重塑互补性，提升其理论基础，使其在实际人-AI决策中成为可靠性的关键衡量指标，而不仅是预测精度的参考。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [7] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: 提出分两阶段、碎片检索增强的分子生成框架MolGen，在多属性约束下有效提升生成分子的精确度和属性满足性，实验优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多目标精确控制和数值推理方面表现有限，尤其在分子生成任务中难以满足高精度、多属性约束。作者旨在解决分子生成时的多属性、数值精确约束难题。

Method: 框架分为两阶段：第一阶段利用多智能体推理器进行碎片检索与局部编辑生成原型分子，第二阶段采用基于GRPO的强化学习优化器进行细粒度调整，使分子更加符合目标属性。整个流程由自动化整理的碎片编辑及属性变化链式数据集支持。

Result: 在多个物化属性约束（如QED、LogP、分子量以及HOMO、LUMO）实验上，框架在合法性和属性满足度上均明显优于现有主流LLM与图生成算法。

Conclusion: 提出了一种基于碎片检索和分两阶段的分子生成框架MolGen，能够在多属性约束下更精准地产生目标分子，并且优于现有LLMs与图生成方法。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [8] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 本文提出一种无需预置工作流的多智能体系统，利用信息流协调器和自然语言A2A通信实现动态协调，大幅减少手动工作量，并在基准测试中提升准确率和鲁棒性，超越传统方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型（LLM）的多智能体系统（MAS）普遍依赖预设的工作流设计，即人工工程师需提前枚举任务状态并规定路由及上下文注入规则。这种以工作流驱动的设计本质上属于基于规则的决策树，面临手工工作量大与难以覆盖复杂真实任务状态空间两大核心问题。

Method: 提出了一种基于Agent间（A2A）通信和信息流协调器的信息流编排多智能体范式。通过专门的信息流协调器实时监控任务进展，动态地以自然语言通过A2A工具调度其他智能体，无需依赖预设的工作流。

Result: 在GAIA通用基准测试上，以经典的基于工作流的MAS系统OWL为对照（其余变量一致），在pass@1设置下新方法准确率为63.64%，比OWL高8.49个百分点（OWL为55.15%），且消耗token量相当。

Conclusion: 新范式在无需手动编排复杂任务状态的前提下，展现了更高的准确率和灵活性以及对边缘案例的更强鲁棒性。实现代码已开源，便于复现实验。

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [9] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 文章提出理解和规范大语言模型集体行为至关重要，并提出了四个关键研究方向，为未来理论与方法发展提供参考。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）驱动的多智能体系统在社会各层面带来重要影响，对于其集体行为的理解变得至关重要。本文旨在强调从交互主义视角出发，系统性研究LLM集体行为的必要性。

Method: 文章采用理论分析和综述方法，通过探讨LLM的特殊性及其影响，引申出交互主义范式下研究路径，并提出四个未来研究方向作为方法论指导。

Result: 作者提出了四个关键研究方向，涵盖理论基础、研究方法和跨学科对话，旨在为多智能体生成式AI系统的系统性考察提供方向。这为社会科学与AI结合开辟了新路径。

Conclusion: 集体行为中，LLM的先验知识与社会语境交互，塑造出新的涌现现象。通过提出新的理论与方法框架，能够更有效地分析和引导多智能体AI系统的社会影响。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [10] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 提出了CMA架构，用以解决RAG在长期记忆和动态知识处理上的不足，经实验验证效果优越，但存在新挑战。


<details>
  <summary>Details</summary>
Motivation: 当前RAG架构无法有效管理与动态更新知识，对长期智能体行为形成严重制约，因而需要更先进且动态的记忆系统架构。

Method: 采用多种行为任务设计（如知识更新、时序联想、联想回忆、上下文消歧）对CMA与RAG做性能对比，验证CMA在动态记忆任务上的优势。

Result: 文章提出了“连续记忆架构（Continuum Memory Architecture，CMA）”的概念，并指出目前主流的检索增强生成（RAG）方法对于大模型的知识记忆和利用存在重大限制，如仅支持无状态查找、信息不可变以及缺少时间连续性。CMA 作为一种新架构，能够跨交互维护并更新内部状态，包括持久存储、选择性保留、联想式路由、时间链式关联以及向更高阶抽象的整合。作者通过实验证明 CMA 在需要持续、变化和判别记忆的任务上优于传统的 RAG，并指出了如延迟、漂移、可解释性等新挑战。

Conclusion: CMA是长期智能体不可或缺的架构原语，在需要记忆不断累积、变更或判别的任务中优于RAG，但实施中需要关注性能与可解释性等问题。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [11] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 提出分层、持续改进的大模型幻觉管理框架，通过模型、数据、上下文三维闭环及多策略结合，提升金融等高风险场景下生成AI的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大模型在金融、法律等敏感领域虽具变革潜力，但幻觉风险严重阻碍落地，急需可操作、可扩展的方法体系来识别并缓解幻觉，提升模型在受监管环境下的信任度。

Method: 采用根因驱动、持续反馈改进的操作周期，将幻觉来源分层（模型、数据、上下文），结合多元检测与缓解策略，组建三层体系架构，并以金融数据抽取为案例应用验证。

Result: 本文提出了一套针对大模型（LLMs、LRMs）幻觉（生成事实错误或无依据内容）问题的全面操作框架。作者将幻觉来源细分为模型、数据和上下文三类因素，分别针对性干预，而非采用单一通用修正措施。该框架集成不确定性估计、推理一致性等多维度检测方法，并配合分层缓解策略（如知识嵌入、置信度校准），形成反馈闭环，实现持续优化。在金融数据抽取案例中，展示了模型、上下文与数据三层架构的协同及其对提升模型可靠性的作用。该体系为高风险领域的生成式AI系统提供了系统化、可扩展的信任保障途径。

Conclusion: 提出的操作框架能够系统且持续地识别与缓解大模型幻觉，显著提升其在金融等受监管高风险领域的可靠性和可用性。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [12] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: LabourLawLLM聚焦中国劳动法，结合LabourLawBench多任务专用评测集，在多项法律应用上优于现有模型，并为其他细分法律领域 LLM 构建提供了通用方法。


<details>
  <summary>Details</summary>
Motivation: 现有通用型LLM（如GPT-4）在需要高专业性、复杂推理与上下文敏感性的法律细分领域表现不足，因此需开发面向具体法律子领域的专用模型，以提升法律AI应用的准确性与可靠性。

Method: 提出LabourLawLLM并制作LabourLawBench基准，对模型进行客观与主观多维评测（ROUGE-L、准确率、F1、soft-F1及基于GPT-4的人工评分），涵盖条文引用、问答、分类、计算、实体识别及分析任务。

Result: LabourLawLLM是一款专为中国劳动法打造的法律领域大型语言模型。该模型在多个劳动法相关任务（如法律条文引用、基于知识的问答、案例分类、赔偿计算、命名实体识别及案件分析）上进行测试，结果显示LabourLawLLM在各项任务上均超越了通用型及现有法律专用LLM，表现更佳。

Conclusion: 专用法律LLM（如LabourLawLLM）在细分领域具备更高的准确性、稳定性与应用价值，显示出超越通用型和现有法律模型的能力；这种针对性开发模式可推广至其它法律子领域。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [13] [SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation](https://arxiv.org/abs/2601.09974)
*Seoyeon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: SPRInG利用选择性适应和历史融合，有效解决了用户兴趣漂移下个性化大模型灾难性遗忘的问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型个性化方法通常假设用户偏好不变，而现实中用户兴趣会不断变化，导致模型难以自适应和避免灾难性遗忘。标准的持续学习方法在处理动态变化和噪声干扰的用户数据时表现不佳。

Method: 提出了SPRInG半参数化框架。训练阶段，SPRInG采用基于漂移检测的选择性适应法，通过似然评分函数筛选具有高新颖性的交互，实现有针对性的模型更新，并将难学的残差存于重放缓冲区。推理时，则通过严格的相关性门控，将参数化知识与检索历史以logit插值方式融合。

Result: 在长文本个性化生成基准测试上，SPRInG显著优于现有主流方法，显示出对真实场景中持续个性化任务的强鲁棒性。

Conclusion: SPRInG能够有效应对用户偏好的持续变化，通过有选择性的学习和历史信息整合，提升了大语言模型的持续个性化能力。

Abstract: Personalizing Large Language Models typically relies on static retrieval or one-time adaptation, assuming user preferences remain invariant over time. However, real-world interactions are dynamic, where user interests continuously evolve, posing a challenge for models to adapt to preference drift without catastrophic forgetting. Standard continual learning approaches often struggle in this context, as they indiscriminately update on noisy interaction streams, failing to distinguish genuine preference shifts from transient contexts. To address this, we introduce SPRInG, a novel semi-parametric framework designed for effective continual personalization. During training, SPRInG employs drift-driven selective adaptation, which utilizes a likelihood-based scoring function to identify high-novelty interactions. This allows the model to selectively update the user-specific adapter on drift signals while preserving hard-to-learn residuals in a replay buffer. During inference, we apply strict relevance gating and fuse parametric knowledge with retrieved history via logit interpolation. Experiments on the long-form personalized generation benchmark demonstrate that SPRInG outperforms existing baselines, validating its robustness for real-world continual personalization.

</details>


### [14] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL 是一种无训练新框架，通过结构化分解和经验自我修正，提升 NL2SQL 系统的准确性和效率，不依赖微调，在 BIRD 数据集上实现新 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有 NL2SQL 方法只用正确例，引导有限，自我纠错能力弱；测试时问题分解随意，导致候选 SQL 单调、集成效果有限，且精度-效率权衡明显。

Method: 提出三种结构化分解策略（按实体、分层、原子顺序），并构建动态记忆库，结合历史错误修正对和成功案例，通过检索增强的提示实现自我修正，全流程不需要微调或外部 API。

Result: Memo-SQL 大幅提升 NL2SQL 执行准确率（68.5%），用低十倍以上计算资源，优于以往同类 TTS 方法。

Conclusion: Memo-SQL 在无需微调和外部 API 的前提下，通过检索增强的提示和动态记忆机制，将 BIRD 执行准确率提升至 68.5%，同时资源消耗大幅低于现有方法，刷新了开放式零微调方法的最好水平。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [15] [PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization](https://arxiv.org/abs/2601.10029)
*Tingyue Pan,Jie Ouyang,Mingyue Cheng,Qingchuan Li,Zirui Liu,Mingfan Pan,Shuo Yu,Qi Liu*

Main category: cs.AI

TL;DR: 该工作提出了PaperScout自主体系统并采用PSPO序列级优化，实现了对复杂论文检索查询的高效与高相关性返回，优于传统和强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有学术论文检索方法多依赖固定预设流程，难以处理复杂、有条件的查询问题。该研究为解决这一局限提出新方法。

Method: 提出了PaperScout自主体系统，将论文检索建模为顺序决策过程，并依据累积检索上下文动态调用搜索和扩展工具。此外，引入一种新序列级决策优化方法：Proximal Sequence Policy Optimization（PSPO），对多轮agent任务进行过程感知优化。

Result: 在合成和真实基准测试中，PaperScout在召回率和相关性方面均明显优于现有基于流程和强化学习的强基线方法，验证了其自适应主体框架和优化策略的有效性。

Conclusion: PaperScout将论文检索任务转化为更灵活的多轮决策过程，结合PSPO优化策略，大幅提升复杂查询的检索性能，解决了流程僵化和噪声分配等核心难题。

Abstract: Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.

</details>


### [16] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: FilDeep通过结合高低保真数据和注意力机制模块，实现了大变形弹塑性问题的高效高精深度学习建模，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大变形弹塑性固体计算中因高质量数据难以获得、数据量与精度之间权衡导致的深度学习方法性能受限问题。

Method: 提出了一种面向大变形弹塑性固体的基于保真度的深度学习框架FilDeep。该方法融合了高保真小样本和低保真大样本数据，通过注意力机制的跨保真度模块，实现长距离物理关联信息的有效捕获。

Result: FilDeep是首个针对大变形弹塑性问题，基于多保真（MF）数据的深度学习框架。实验证明FilDeep在相关任务中达到了当前最优性能，并可高效集成于制造流程中。

Conclusion: FilDeep成功解决了大变形弹塑性固体数值模拟中数据精度与数量之间的矛盾，极大提升了深度学习模型在复杂工程场景下的适用性和性能。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [17] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 本文大规模分析了真实世界LLM数据，发现了实际应用中的多样化场景、模型选择偏好及用户留存特征，为LLM设计和部署提供了数据驱动的洞见。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在现实世界应用中的迅速发展，尤其是多步推理模型的出现，实际用户对LLM的使用方式发生了重大变化，但对这些实际应用方式的经验性理解仍然缺乏。

Method: 利用OpenRouter平台汇聚的多型号LLM大规模交互数据，对不同任务、地域、时间段的LLM使用情况进行经验分析，包括模型偏好、使用场景、用户留存分析等。

Result: 通过分析OpenRouter平台上超过100万亿token的真实LLM交互数据，本文发现了开放权重模型的大规模采用、创造性角色扮演和编程辅助类应用的突出流行，以及agentic inference（具备自主推理与决策能力的推理方式）的兴起。此外，文章提出了“Glass Slipper”效应，指出早期用户的留存显著优于后续用户，并认为LLM的真实使用方式比预期更复杂和多元。

Conclusion: 研究表明，开发者和终端用户对LLM的应用方式极为复杂且多样；基于大规模真实数据的洞察有助于更合理地优化模型设计、开发和基础设施建设。作者建议数据驱动的分析应成为未来LLM系统设计部署的重要依据。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [18] [Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs](https://arxiv.org/abs/2601.10114)
*Cheng Feng,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.AI

TL;DR: 论文提出按教师训练收敛过程分阶段蒸馏，并对数据分样本赋权，显著提升了小模型在领域任务上的表现，部分情形下超越了原始大模型。


<details>
  <summary>Details</summary>
Motivation: 当前领域任务部署大语言模型成本高昂，将其蒸馏为小模型是主流思路，但巨大的模型容量差距导致效果损失明显。论文关注学生模型何时能追平甚至超越教师，并提出理论分析和新解法来改善这一难题。

Method: 提出了Scheduled Checkpoint Distillation（SCD）方法，模仿教师模型在下游任务微调过程中的收敛曲线以减少学生在教师优势子域（TFS）上的劣势，并设计了样本级自适应加权机制（AW）以保留学生模型在学生优势子域（SFS）上的能力。进行了多领域（问答、命名实体识别、文本分类）、多语言的实验证明。

Result: 实验表明该方法在多个领域任务上均优于现有蒸馏方法，学生模型性能可达到或超过微调后的教师模型水平。

Conclusion: 提出了一种新的知识蒸馏方法，能够让小模型在特定条件下追平甚至超越领域下的教师大模型，并在多个实际任务中验证了其有效性。

Abstract: Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.

</details>


### [19] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLM能粗略预测重复行为的时间间隔，但在复杂定量时序推断上不如专用机器学习模型，且过多细节反而可能拉低性能。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否能从结构化行为数据中推理时间规律，以及上下文信息的丰富程度如何影响其对时序数据的推断能力。

Method: 采用经典的重复购买模拟场景，在零样本条件下，系统评估LLM与统计、机器学习方法对时间间隔预测的效果，通过不同级别的上下文环境分析其表现。

Result: 本研究发现，在预测用户行为中重复事件的时间间隔时，当前大型语言模型（LLM）在零样本任务中虽优于轻量级统计模型，但劣于专门的机器学习方法，表明LLM在定量时序结构识别上的能力有限。此外，提供适度上下文信息确实提升了模型的准确率，但过多的用户级细节反而会导致性能下降。

Conclusion: 当前LLM在结构化时序推断方面存在明显短板，仅适度丰富上下文有利于提升效果，盲目加细节则适得其反，未来模型需结合统计精度和语言灵活性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [20] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 提出一种融合自适应数据生成和流程自动化的漂移感知系统，显著提升量化金融模型在动态市场中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于历史数据训练的金融模型面对市场变化时表现较差，容易过拟合。市场的动态和分布漂移等特性，需求一种能自适应市场演化且自动优化的数据流系统弥补“历史并不足够”的问题。

Method: 系统包含参数化的数据操作模块（单股票变换、多股票混合、数据筛选）和自适应的规划调度器，后者采用基于梯度的双层优化进行控制。整体结构统一了数据增强、课程学习和数据流程管理，实现了溯源回播和持续数据质量监测。

Result: 该论文提出了一种关注概念漂移和分布非平稳性的量化金融数据流系统。通过将机器学习自适应控制集成到数据管理流程中，能够提升模型在实际交易中的稳健性和泛化能力。实验在预测与强化学习交易任务上展现了模型的稳健性提升和风险调整后收益的提升。

Conclusion: 该系统为金融领域自适应数据管理和流程自动化提供了一个通用、可扩展的解决方案，有效提升了模型在动态市场条件下的稳健性和收益表现。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [21] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 提出将轨迹视为独特模态，通过LLM与自然语言任务描述对齐，实现创新的决策LLM系统，在多个离线决策与竞价任务中超越传统RL与DT方法。


<details>
  <summary>Details</summary>
Motivation: 考察能否利用与Transformer架构相同、但规模更大的大型语言模型（LLM）提升长序列决策任务的表现，解决传统RL中对连续值表征的局限。

Method: 提出将决策轨迹作为独立模态，通过与自然语言任务描述对齐训练LLM，使其能在统一框架下自回归预测未来动作，并推导出其性能的缩放定律。

Result: DecisionLLM在离线实验基准和真实竞价场景中取得突出成绩，具体如：在Maze2D umaze-v1任务上比传统Decision Transformer高69.4分，在AuctionNet上高0.085。

Conclusion: 验证了LLM在长序列决策领域的大规模应用潜力，强调模型规模、数据量与数据质量为性能关键要素，为线上竞价等场景的进一步探索指明方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [22] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个用容器封装、多模型支持的开源医学影像AI平台，强调模型标准化、易复现和便于临床应用。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域的AI模型实现多样、不一致文档和难以复现的困境制约了其研究与临床应用，因此亟需一个标准化、易获取且便于复现的通用平台来提升医学影像AI的实际价值和应用推广。

Method: 采用了开放源码、容器化部署，将来自同行评审论文的医学影像AI模型打包、标准化，支持DICOM等多种数据格式，并集成结构化元数据和参考数据；设有可视化仪表板用于展示评估结果和辅助复现分析。

Result: MHub.ai平台实现了对医学影像AI模型的标准化与容器化封装，显著提升了模型的复现性和易用性。平台收集并统一了多种分割、预测和特征提取模型，支持直接处理DICOM等格式，且每个模型都附有公开的参考数据以验证正确运行。平台还提供了可视化仪表板及相关评估数据，以便用户深入分析和复现结果。通过实际临床用例展示了平台在肺部分割模型中的比较测试能力，有效支持AI模型的快速临床转化。

Conclusion: MHub.ai通过标准化、容器化和开放数据，简化了AI医学影像模型的使用流程，提升了模型的可访问性和复现性，有助于推动AI技术在医疗领域的临床落地。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [23] [MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning](https://arxiv.org/abs/2601.10157)
*Yusong Wang,Jialun Shen,Zhihao Wu,Yicheng Xu,Shiyin Tan,Mingkun Xu,Changshuo Wang,Zixing Song,Prayag Tiwari*

Main category: cs.AI

TL;DR: MMPG通过多视角图构建结合专家混合方法，提升了蛋白质表示的完整性和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于GNN的蛋白质表示学习仅采用单一视角的图构建策略，无法全面捕捉蛋白质残基间多层次的互作特性，导致表示信息不全。

Method: 提出MMPG框架，从多视角（物理、化学、几何）构建蛋白质残基互作图，并通过专家混合（MoE）自适应融合信息，捕捉各视角特征及其协同作用。

Result: MMPG能够自动将专家分工于不同层级的信息（单一视角、视角间协同、全局共识），并在四项蛋白质下游任务中性能领先，证明方法有效提升了蛋白质表示能力。

Conclusion: 多视角结合与专家混合机制能有效提升蛋白质图表征学习能力，为蛋白质结构及功能预测任务带来更优表现。

Abstract: Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.

</details>


### [24] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 提出了一套评估降采样对nEMG信号诊断性能影响的流程，实验显示形状感知降采样方案效果更优，可用于实现高效、实时的神经肌肉疾病辅助诊断，对其他高频时序数据也具参考意义。


<details>
  <summary>Details</summary>
Motivation: 高异构、高采样率的nEMG信号，在基于特征的机器学习模型上导致计算负担过重，尤其在需要实时分析时更为突出。虽然降采样有望缓解该问题，但其对诊断信息与分类性能的影响尚不明确。

Method: 提出了一套系统流程，用形状失真度量、机器学习分类结果和特征空间分析，评估不同降采样算法/参数对nEMG信号信息保留及分类性能的影响。通过三类NMDs分类实验，验证了该流程的有效性。

Result: 工作流能有效筛选出兼顾信息保存和计算效率的降采样配置；形状感知降采样算法相对于传统抽取法对诊断关键信息和信号形态保护更优，为其他高频时序信号的降采样和实时分析也具有推广价值。

Conclusion: 本研究证明，形状感知的降采样算法相较于标准抽取法更好地保持了nEMG信号的峰值结构和整体形态，可在显著降低计算负载的同时，依然保留足够的诊断信息，适用于近实时分析。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [25] [GFM4GA: Graph Foundation Model for Group Anomaly Detection](https://arxiv.org/abs/2601.10193)
*Jiujiu Chen,Weijun Zeng,Shaofeng Hu,Sihong Xie,Hui Xiong*

Main category: cs.AI

TL;DR: 作者提出GFM4GA，用于少样本和标签稀缺场景下的群组异常检测，通过双层对比预训练和微调实现对复杂群组异常的检出，实验效果领先。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型虽然能检测个体异常，但难以泛化到群组异常，因群组内部个体可能表现正常，且现有模型需大量标注数据，检测群组异常困难较大。

Method: 提出GFM4GA模型，在预训练阶段采用基于特征估计和群组提取的双层对比学习机制，下游任务中引入参数约束和群组异常比例加权微调，并利用已标注邻居确定群组上下文增强对未见异常类别的泛化能力。

Result: 本文提出了一种用于群组异常检测的新型图基础模型（GFM4GA）。通过双层对比学习进行预训练，结合特征估计和群组提取，实现特征一致性及群组异常结构的捕捉。在下游任务中，通过参数约束及基于群组异常比例的加权机制进行微调，并利用已标注异常邻居确定群组上下文以扩展对未知群组异常的自适应能力。实验结果表明，GFM4GA在AUROC和AUPRC指标上分别平均提升2.85%和2.55%，优于现有方法。

Conclusion: GFM4GA能够有效提升群组异常检测性能，尤其在复杂、少样本和标签不足的场景下明显优于现有方法。

Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.

</details>


### [26] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: 本文提出Topo-RAG框架，通过差异化处理文本和表格结构，实现对企业级复杂文档数据的更高效检索与理解。Topo-RAG在合成企业数据集上，比现有方法在混合查询任务上提高了18.4%的nDCG@10。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统常将企业文档丰富的结构内容线性化为文本输入，无法充分捕捉表格等结构信息的几何关系，导致检索效果受限。因此需要更好地利用文档的结构特性。

Method: 该方法采用双分路架构，针对文本内容使用传统的密集型检索器（dense retrievers），针对表格结构使用Cell-Aware Late Interaction机制以保持单元格间空间关系，从而分别处理叙述性与结构化信息。

Result: Topo-RAG框架在SEC-25合成企业数据集的混合查询实验中，相较主流线性化方案，nDCG@10提升了18.4%，展现了有效性。

Conclusion: Topo-RAG能更有效地捕捉和利用企业文档数据的结构与叙事内容，显著提升混合类型查询的检索效果，优于现有的线性化嵌入方法。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [27] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: 论文提出TRIM方法，在多步推理任务中仅将关键步骤交由大模型处理，其他步骤交由小模型，实现高成本效率和准确率，显著优于现有模型分配策略。


<details>
  <summary>Details</summary>
Motivation: 现有多步推理任务（如数学问题求解）易受级联失败影响，一步出错可能导致整个解答失败。目前的大语言模型（LLM）分配方法将整体任务交由单一模型，未能区分各推理步骤的重要性，导致资源使用不精准。

Method: 提出TRIM（多步推理任务中的目标化路由）方法：仅将关键性步骤（即可能造成整个解答失败的步骤）分配给高性能大模型，普通步骤交由小模型处理。TRIM在推理过程中使用奖励模型识别关键错误步骤，并结合步骤的不确定性与预算约束实现动态路由，具体包括阈值策略以及结合长远准确率与成本权衡的高级策略。

Result: 在MATH-500数据集上，TRIM的最简单路由策略其成本效率达到了现有方法的5倍，更复杂策略能在极大降低大模型token消耗（减少80%）的情况下达到与昂贵强模型相同的表现。在AIME等更具挑战性的数据集上，TRIM效率可达6倍提升。所有策略均在多种数学推理任务中泛化良好。

Conclusion: TRIM能够通过针对关键性推理步骤的目标化路由，有效防止推理错误级联提升推理效率，在保证准确性的同时显著减少计算成本。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [28] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: 针对长上下文推理奖励稀疏与证据提取难题，提出EAPO方法，通过密集证据奖赏与奖励-策略协同进化，大幅提升长上下文场景下大模型推理能力，在多项基准测试中效果突出。


<details>
  <summary>Details</summary>
Motivation: 当前RL在提升大模型推理方面取得进步，但在长上下文场景下，因结果奖励稀疏，难以有效约束无依据的“蒙猜”行为，证据搜索过程缺乏监督，成为提升性能的瓶颈。

Method: 提出EAPO（Evidence-Augmented Policy Optimization）方法：首先建立Evidence-Augmented Reasoning范式，通过树状证据采样验证证据提取是长上下文推理的决定性瓶颈。EAPO设计专用RL算法，以组相对证据奖励（Group-Relative Evidence Reward）提供密集过程监督，显式提升证据质量。为保证训练期间奖励模型监督效果，引入自适应奖励-策略协同进化机制（Adaptive Reward-Policy Co-Evolution），通过与结果一致的采样迭代优化奖励模型，提高其区分能力，精确指导推理过程。

Result: 在八个基准测试中，EAPO方法明显优于当前最优基线(SOTA)，显著提升了长上下文推理的表现。

Conclusion: 证据提取是长上下文推理的核心难点，通过EAPO引入密集过程监督与奖励-策略协同进化机制，能够有效提升证据质量与整体推理性能。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [29] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: 本文提出C-GRASP系统，用于提升LLM解读HRV的可靠性与透明度，通过个体基线与自动RSA防控，有效提升情感识别准确性并降低生理假象风险。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对HRV解读易产生生理性幻觉，如呼吸性窦性心律不齐（RSA）干扰、非线性指标对短时数据不稳定、只用人口均值忽略个体本底，限制了其在临床及生物医学工程中的合理性与可用性。

Method: 提出C-GRASP（一种基于临床推理的HRV解读管道），结合守护机制与RAG（Retrieval-Augmented Generation），通过八步可追溯推理流程分解心率变异性解读，重点引入Z-score优先层级，将个体化基线位移优先于群体统计。自动化实现RSA污染防控和指标纠正。

Result: 在DREAMER数据集414组实验中，结合高阶推理模型的C-GRASP在四分类情感识别中达37.3%准确率、临床推理一致性CRC为69.6%。消融实验表明个体化Delta Z-score模块是防止群体偏置的逻辑核心。

Conclusion: C-GRASP实现了将情感计算从黑箱分类转向可追溯的临床决策支持，为安全集成AI于生物医学工程、医学信号解读领域奠定了新基础。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [30] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: 论文针对text-to-SQL系统面对不可回答性问题提出了LatentRefusal安全拒答机制，通过利用LLM隐层信号及时准确拒绝无解查询，有效提升系统安全并保持高效率。


<details>
  <summary>Details</summary>
Motivation: 现有text-to-SQL系统在处理不可回答或未明确查询时，会生成错误/误导性甚至不安全的执行程序，现有拒答策略要么依赖输出指令遵循，容易被幻觉影响，要么依赖输出不确定性，计算复杂度高，部署负担重。论文旨在解决这一安全部署的重大瓶颈。

Method: 将安全拒答建模为'可回答性门控'问题，提出LatentRefusal机制——基于大语言模型中间隐层激活信号的预测方法。为此设计了Tri-Residual Gated Encoder，以抑制schema噪声、放大问题与schema不匹配的信号，从而精准判别不可回答查询。主要通过消融和解释性实验进行方法验证。

Result: LatentRefusal在四个主流基准上，均提升平均F1至88.5％，在主干模型上都表现出一致提升，同时附加的推理开销仅约2ms，兼具高效和部署便利性，验证了其实用价值和通用性。

Conclusion: 提出的LatentRefusal机制能高效且有效地提升LLM驱动的text-to-SQL系统的安全性，通过在模型中增加一个高效的、可插拔的拒答层，对不可回答或未充分指明的查询进行筛查。实证结果表明，在多个基准下，LatentRefusal显著提高了系统的性能并显著控制了拒答相关风险。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [31] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: 该文提出ErrEval，通过错误诊断促进自动问答生成评价，提升与人为评判对齐度，有效减少不良问题被高估。


<details>
  <summary>Details</summary>
Motivation: 常规评测忽视QG中事实幻觉与答案不匹配等关键缺陷，容易高估低质量问题，且多为黑箱评价，缺乏透明、细粒度的错误建模。

Method: ErrEval 框架：将自动问答生成（QG）评价流程分为错误诊断和评分两阶段。先用可插拔的错误识别器检测结构、语言和内容等常见错误，再引入这些诊断信号作为显式证据，辅助大模型（LLM）评分。

Result: 在三套基准数据上实验证明，显式引入错误诊断信号可提升评价结果与人工一致性，降低对问题质量的过度乐观判断。

Conclusion: ErrEval 能显著提升QG评价的细致性和客观性，在自动检测与修正关键缺陷方面优于现有评价体系。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [32] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: 本文提出基于LLM和RAG的LADFA框架，可自动从隐私政策中抽取并分析个人数据流，经汽车行业案例验证效果优良，并具有良好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 隐私政策通常用复杂的法律语言描述个人数据的收集、存储与共享方式，难以被普通用户理解，同时各机构做法不一，给隐私分析带来挑战。需要有效的自动化工具来大规模分析隐私政策内容。

Method: 提出了LADFA端到端计算框架，将大语言模型（LLM）与检索增强生成（RAG）技术相结合，并基于已有研究自定义知识库。框架包括预处理器、LLM处理器和数据流后处理器，实现了隐私政策中个人数据流的抽取和数据流图的构建与分析。

Result: 通过对汽车行业十份隐私政策的案例分析，验证了所提方法在有效性和准确性上的表现。系统具备灵活性和可定制性，可应用于隐私政策之外的各类文本分析任务。

Conclusion: LADFA框架能够有效自动化提取和分析隐私政策中的个人数据流，并对其他文本分析场景具有潜在应用价值。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [33] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: 提出LLMdoctor框架，通过token级别的奖励信号和TFPO，有效提升LLM的测试时对齐能力，性能优于现有方法和完全微调。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型偏好对齐方法成本高且不灵活，测试时对齐方法存在性能上限和文本多样性损失。为解决这些问题，作者提出了新框架，提升对齐效率和模型输出多样性。

Method: 采用“患者-医生”模型架构，将原有冻结的大模型（patient）与小型doctor专用模型结合，并以token级别信号进行TFPO优化，确保子序列流一致性，实现精细化对齐。

Result: 本文提出了一种新的大型语言模型（LLM）对齐方法——LLMdoctor。该方法通过“患者-医生”范式，实现了高效的测试时对齐，能使冻结的大模型（patient）在小型专用模型（doctor）的引导下，获得细粒度、token级别的偏好信号，并用TFPO进行优化。实验表明，该框架优于现有的测试时对齐方法，甚至超过了完全微调（如DPO）的效果。

Conclusion: LLMdoctor在无需昂贵完全微调的前提下，实现了高效且多样性的测试时偏好对齐，是当前对齐领域的新突破。

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [34] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: 提出了NSR-Boost新框架，无需重训练即可针对旧GBDT模型失效区域进行高效修正，已在金融风控实战验证优越性；实现了可解释、低风险、低成本的模型演进。


<details>
  <summary>Details</summary>
Motivation: 在高并发生产环境下，传统GBDT遗留模型的升级存在高昂的重训练成本与系统风险，需要非侵入式且可控的增强手段，在不修改原有模型前提下弥补其在“难区”表现不足。

Method: 提出了NSR-Boost神经符号残差提升框架，包含三阶段：1）通过残差发现难以预测的区域；2）利用大模型生成可解释的专家符号结构并用贝叶斯优化微调参数；3）通过轻量级聚合器动态整合专家与遗留模型输出。原始遗留GBDT模型被冻结，不做变动。

Result: NSR-Boost在六个公共数据集及一个真实金融数据集上效果优于SOTA基线，在金融风控实战环境中展现大幅性能提升。能够发现并修正传统模型难以识别的长尾风险。

Conclusion: NSR-Boost有效捕捉了传统模型遗漏的长尾风险，并为工业应用提供了一种安全、低成本的模型演进范式。其在多个公开及真实金融数据集上均显著优于当前最优基线方法。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [35] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 提出了包含30种类型图表图片的ChartComplete数据集，填补现有数据集图表类型单一的空白。


<details>
  <summary>Details</summary>
Motivation: 目前图表理解领域用于评估多模态大语言模型的数据集覆盖的图表类型有限，制约算法的评测与发展。

Method: 作者借鉴可视化领域的图表分类体系，系统收集并分类整理成30类图表图片，形成ChartComplete数据集。该数据集未包含具体任务标注，仅提供结构化图表样本。

Result: 本文提出了ChartComplete数据集以提升和评估多模态大语言模型（MLLMs）在图表理解领域的性能。该数据集来源于可视化社区的图表分类法，覆盖了30种不同类型的图表，解决现有基准只覆盖少量类型的问题。ChartComplete数据集目前仅包含经过分类的图表图片，不包含学习信号，便于社区进一步扩展和研究。

Conclusion: ChartComplete数据集丰富了图表类型，为图表理解任务提供了更全面的评测与研究基础，促进MLLMs能力的提升。

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [36] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种领域知识图谱融合（DKGF）任务，通过融合通用知识图谱的信息来丰富领域知识图谱，并提出了ExeFuse方法解决相关性和粒度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 针对领域知识图谱覆盖度不足问题，利用通用知识图谱进行融合，提高领域知识图谱的丰富性和实用性。

Method: 提出了Fact-as-Program范式，将GKG事实视为潜在语义程序，并将抽象关系映射为粒度感知的操作符，通过程序在目标DKG上的可执行性验证领域相关性，在统一概率框架下联合解决相关性和粒度问题。并构建了两个基准数据集和多种评价配置。

Result: 在两个基准及21种配置下进行了大量实验，验证任务的重要性和模型优势，首次为DKGF任务提供了标准化测试平台。

Conclusion: 首次建立了DKGF标准化基准，充分证明了所提模型和任务的有效性，为促进领域知识图谱发展提供了新路径。

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [37] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 论文利用多层次诊断框架与可解释性方法分析LLM微调的泛化失效，发现架构与数据多样性为泛化关键，不同模型表现出显著差异，并为AI系统可靠性分析提供了工具与路径。


<details>
  <summary>Details</summary>
Motivation: 论文关注于微调大型语言模型（LLM）在特定任务取得高性能的同时，易出现脆弱性与泛化失败现象，动机在于揭示泛化失败的根本原因并提升模型可靠性。

Method: 提出并应用了多层次诊断框架，包括对跨架构的Llama 3.1 8B、Gemma 2 9B、Mistral模型，在高风险钓鱼检测任务上的微调；采用SHAP分析与机理可解释性方法，挖掘泛化失效的根源。

Result: 发现：1）架构与数据多样性协同驱动泛化，Gemma 2 9B在多样化数据集上能达到>91% F1表现；2）泛化高度依赖架构，Llama 3.1 8B在窄域表现好，难集成多样数据，性能明显下降；3）有架构天生更具泛化能力，如Mistral在多个训练范式中表现稳定。

Conclusion: 通过诊断模型的失效模式和不健全启发式，提出一套具体方法协助解析泛化问题，强调可靠AI需深度验证架构、数据、训练三者的互动关系。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [38] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 评估7个多模态大模型的安全性，发现模型在不同测试下表现不一，对抗测试尤其薄弱，呼吁统一、标准的安全评估体系。


<details>
  <summary>Details</summary>
Motivation: 随着大模型能力的提升，尚不清楚安全性是否同步进步，现有评测分散且不足以展现模型真实风险，需要全面、统一的安全评估体系。

Method: 统一实验协议，系统评估7个前沿大模型在语言、视觉-语言和图像生成模式下的安全性，涵盖基准测试、对抗测试、多语言测试和合规性测试，量化并归纳为安全排行榜。

Result: 本文报告了对7个前沿大模型（包括GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro、Seedream 4.5）在语言、视觉-语言、图像生成等多模态下的安全性综合评测。采用统一协议，融合基准测试、对抗测试、多语言和合规性测试，形成了安全排行榜和模型安全画像。结果显示安全性表现高度异质，GPT-5.2在各项评测中表现最均衡，其他模型则在不同维度间存在明显取舍。所有模型在对抗测试下均显著退化。

Conclusion: 前沿大模型安全性表现多维异质，不同评估方式和输入模态影响大，强调建立标准化安全评估以准确衡量实际风险并引导负责任的模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [39] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 以调动LLMs生成过程中的隐藏安全信号为核心，提出早期识别危险内容的方法，有效提升模型抗越狱攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有防御手段对复杂越狱攻击常效果欠佳且影响模型实际用途，因此急需一种兼顾安全性和实用性的新防御机制。

Method: 在模型生成输出时，实时检测解码过程中的潜在安全相关信号，并将这些信号显性化，作为判断和拦截危险内容的依据。通过在多种越狱攻击场景下实验，评估该方法的安全增益及误拒绝情况。

Result: 本文针对大语言模型（LLMs）在面对越狱攻击时，现有安全防护方法有效性不足的问题，提出了一种新颖的实用方法。该方法利用模型生成过程中内在的安全信号，实现对危险内容的早期检测。通过丰富的实验，验证了该方法在阻挡多样化越狱攻击的同时，能有效降低误拒绝率并保持回答质量，提升了模型整体的安全性。

Conclusion: 激活和利用内在安全信号是在解码过程中提升越狱防御的新方向。本文提出的方法不仅显著增强了模型安全性，还避免了对正常内容的过度筛查，兼顾了检测准确率与模型实用性。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [40] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent利用多智能体机制，提升了大语言模型在基因组问答任务中的表现和泛化能力，平均超越GeneGPT 12%。


<details>
  <summary>Details</summary>
Motivation: 面对越来越复杂的分布式基因组数据库，生物医学研究人员需要高效提取和理解相关信息。然而，现有大语言模型（LLMs）由于无法灵活访问特定领域数据库，难以胜任基因组问答任务。主流方法如GeneGPT虽通过API调用提升了LLM能力，但受限于API僵化和适应性不足。

Method: 本文复现了GeneGPT系统，并提出GenomAgent：通过多智能体框架，协调多个专用智能体以解答复杂的基因组查询，从而克服单一API依赖和适应性差的问题。

Result: 在GeneTuring基准的九项任务上，GenomAgent平均性能超越GeneGPT 12%，并展现出更具扩展性，适用于更广泛的科学领域知识提取需求。

Conclusion: GenomAgent突破了单一API依赖的局限，实现了更高效和灵活的领域知识抽取，对于拓展大模型在生物信息及其他科学领域应用具有重要意义。

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [41] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 本研究提出了一种利用布尔变量与单调性的全符号LTLf合成算法，可大幅缩短多目标合成的计算时间，性能优于现有的枚举法。


<details>
  <summary>Details</summary>
Motivation: 在LTLf合成中，涉及多个性质时，往往不可能全部满足所有性质，因此需要一种方法能高效找出最大可实现的目标集及其策略。

Method: 引入布尔目标变量，利用单调性实现目标组合的紧凑表达，通过全符号算法进行固定点计算，无需枚举所有性质子集。

Result: 算法能够对目标组合实现指数级压缩，显著提升计算效率，并在实验中大幅超越枚举法基线。

Conclusion: 所提出的方法能够在一次固定点计算中高效地找出可实现的目标集关联关系，比传统的枚举法在性能上有显著优势。实验证明，该方法能够比以往的基于枚举的基线快达百倍。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [42] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: 论文分析了HRM推理模型的失败模式和推理动态，发现其行为更像“猜测”而非“推理”，并提出新策略显著提升其准确率。


<details>
  <summary>Details</summary>
Motivation: 分层推理模型近期在推理任务上表现突出，且常超越大规模语言模型，但其工作机制和局限性尚不明确。为提升其性能并深入理解模型推理本质，作者对HRM的推理行为进行机制剖析，针对其失败模式提出改进策略。

Method: 作者采用机制性分析，研究模型在具体任务（如极简数独等）中的表现，解析其推理步骤和固定点性质。通过引入数据增强、输入扰动和模型自举等三种策略，并将这些方法融合实验于Sudoku-Extreme任务，对比前后性能变化。

Result: 该论文对分层推理模型（HRM）的推理过程进行机制性研究，揭示了其在某些极为简单的推理任务上易失败、“突发式”推理过程以及多个固定点等特性，并据此提出了三种策略（数据增强、输入扰动、模型自举）提升HRM推理表现。最终，组合这些策略大幅提升了HRM在Sudoku-Extreme任务上的准确性，展示了机制分析辅助模型能力提升的有效性。

Conclusion: 论文结论是，通过认识和利用HRM本质为“猜测”的特性，结合数据增强、输入扰动和模型自举等方法可以极大提升HRM模型的推理准确性与稳定性，对推理模型本质也有新认识。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [43] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 本文提出了一种结合结构感知和多样性约束的上下文气泡（context bubble）方法，在文档片段检索和构建过程中，实现了冗余降低、覆盖提升和可审计性的优化，显著优于传统top-k策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成（RAG）的LLM上下文构建方法，通常采用top-k条目选择，导致信息碎片化、内容冗余、二三阶语义片段覆盖不足等问题。

Method: 该方法利用结构先验和任务条件，基于高相关锚点片段，通过受限选择机制，在相关性、边际覆盖和冗余三者间平衡，约束多样性和总长度，生成多层次、独立且可审计的context集合，同时输出完整记录检索和评分决策。

Result: 提出的结构感知与多样性约束的context bubble方法，在有限token预算内，显著降低了冗余、提升了多阶覆盖能力，提升了答案质量和引用准确性。在实际企业文档场景中，取得了比top-k检索更优的表现。消融实验显示，缺少结构先验或多样性约束都会导致覆盖下降、冗余或不完整。

Conclusion: 结构化约束和多样性选择能够更合理地组织文档信息，满足strict token预算下的高效、可追溯的上下文构建需求，改善了答案质量和引用信度。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [44] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: 本文研究了生成式人工智能（GenAI）在建筑概念设计任务中的影响，涵盖设计表现、创造自我效能和认知负担。结果显示：GenAI对整体表现无显著优势，但对新手设计者表现提升明显，同时会降低学生的创造自我效能。认知负担总体无差异，但迭代性提问和可视化反馈可有效减轻负担。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI在复杂跨专业设计任务中的优势与局限，分析其对用户创造力感知、认知负担的实际影响，为未来AI辅助设计工具开发与推广提供数据支撑。

Method: 36名多学科学生完成两阶段建筑设计任务：第一阶段独立完成，第二阶段分别用GenAI辅助与传统在线资料辅助。专家评分设计成果，学生自评效能感和认知负担。采用双重差分分析对数据进行统计。

Result: GenAI对新手组设计表现有显著提升，但对整体没有优势。使用GenAI后学生创造自我效能普遍下降。不同工具下认知负担无显著差异，但提示策略优化可有效缓解负担。

Conclusion: GenAI在建筑设计领域是否有效取决于用户的经验基础及其交互策略，对新手有明显帮助，但可能削弱创造自信；合理的提示使用可缓解认知负担。

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [45] [Multi-Agent Cooperative Learning for Robust Vision-Language Alignment under OOD Concepts](https://arxiv.org/abs/2601.09746)
*Philip Xu,Isabel Wagner,Eerke Boiten*

Main category: cs.MA

TL;DR: 提出了多智能体协作学习（MACL）框架，有效缓解跨模态视觉-语言模型在处理分布外概念时的对齐崩溃问题，实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型无法有效处理分布外概念，易出现模态对齐崩溃，亟需新的方法增强模型对OOD概念的对齐与学习能力。

Method: 设计了包括图像、文本、名称和协调智能体在内的多智能体，通过结构化信息传递协同学习，集成多智能体特征空间名称学习、上下文交换增强的小样本学习算法，以及动态自适应平衡机制对智能体贡献进行调节。

Result: 在VISTA-Beyond数据集上，MACL在多视觉域中few-shot和zero-shot任务下均获得了1-5%的精度提升，显示其有效性。

Conclusion: MACL框架能在few-shot和zero-shot条件下提升在不同视觉域的精度，取得1-5%的提升。

Abstract: This paper introduces a novel Multi-Agent Cooperative Learning (MACL) framework to address cross-modal alignment collapse in vision-language models when handling out-of-distribution (OOD) concepts. Four core agents, including image, text, name, and coordination agents, collaboratively mitigate modality imbalance through structured message passing. The proposed framework enables multi-agent feature space name learning, incorporates a context exchange enhanced few-shot learning algorithm, and adopts an adaptive dynamic balancing mechanism to regulate inter-agent contributions. Experiments on the VISTA-Beyond dataset demonstrate that MACL significantly improves performance in both few-shot and zero-shot settings, achieving 1-5% precision gains across diverse visual domains.

</details>


### [46] [When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making](https://arxiv.org/abs/2601.10102)
*Viswonathan Manoranjan,Snehalkumar `Neil' S. Gaikwad*

Main category: cs.MA

TL;DR: 论文系统测试了不同LLM在多智能体博弈任务下，角色设定与激励信息对推理表现的影响。结果显示，角色与显性激励共同决定了系统是战略推理者还是身份驱动者，且模型间敏感性不同。本研究为现实系统设计提供重要治理启示。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对多智能体系统中，设计选择（如角色设定与激励信息可见性）对大型语言模型推理机制影响的系统性认知，尤其是在实际决策和激励冲突场景下。

Method: 在四种LLM模型（Qwen-7B、Qwen-32B、Llama-8B、Mistral-7B）上，通过让多智能体参与复杂环境决策博弈实验，并有系统变动角色设定和激励显性化，利用纳什均衡达成率诊断战略性推理能力。

Result: 角色设定会显著改变LLM的战略推理表现，移除角色设定且明确显示激励可促使Qwen模型实现较高纳什均衡达成率；添加角色设定时，模型偏向社会期望结果（如Green Transition），而放弃理论最优（如悲剧合作）均衡。不同模型对角色设定与激励显性的敏感性存在显著差别，Qwen系列更易受影响，Llama与Mistral推理则稳定。设计选项本质上即为治理决策，直接关系多智能体系统行为生成机制。

Conclusion: 不同的角色设定和信息披露对大型语言模型（LLMs）在多智能体系统中的推理方式有本质影响，系统设定决定了模型更偏向战略性推理还是角色驱动行为。

Abstract: Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.

</details>


### [47] [TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems](https://arxiv.org/abs/2601.10120)
*Rui Sun,Jie Ding,Chenghua Gong,Tianjun Gu,Yihang Jiang,Juyuan Zhang,Liming Pan,Linyuan Lü*

Main category: cs.MA

TL;DR: TopoDIM框架通过自主、多样式一次性组网，优化了LLM多智能体系统的通信结构，大幅节约token并略提升性能。


<details>
  <summary>Details</summary>
Motivation: 为解决现有LLM多智能体系统通信延迟高、计算代价大和多轮交互冗余的问题，结合评估与辩论机制以提升多智能体协同推理能力。

Method: 采用去中心化的一次性多样化通信模式，让智能体自主构建异质拓扑，结合评价与讨论机制以提升协同问题解决能力。通过实验对比验证框架在token消耗与任务性能上的领先优势。

Result: TopoDIM提高了LLM驱动多智能体系统的通信拓扑优化效率。在消除多轮对话冗余的同时，实现了异质智能体的一次性、多样式自主组网，显著提升了任务表现和token利用率。实验数据显示，TopoDIM比主流方法减少了46.41%的token消耗，并提升了平均性能1.5%。

Conclusion: TopoDIM实现了无需迭代通信协调的高效拓扑生成，有效增强了系统可适应性与隐私保护能力，在token效率和任务表现上显著优于现有方法。

Abstract: Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/

</details>


### [48] [Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems](https://arxiv.org/abs/2601.10560)
*Xi Shi,Mengxin Zheng,Qian Lou*

Main category: cs.MA

TL;DR: LAMaS框架通过并行执行与关键路径显式优化，有效降低多智能体系统的推理延迟，并取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统往往假设顺序执行，仅优化任务性能与资源消耗，未针对并行场景下的推理延迟进行专门控制，导致无法满足时敏任务需求。该研究针对这一不足，创新性提出延迟显式优化方案。

Method: 提出Latency-Aware Multi-agent System（LAMaS）编排框架，在并行执行背景下引入延迟监督信号，通过优化关键执行路径生成低延迟的执行拓扑，实现高效并行系统。

Result: 该论文提出了一种面向并行执行和显式延迟优化的多智能体系统编排框架（LAMaS），专注于通过显式优化关键执行路径降低推理延迟。实验表明，相较于最新方法，该方法在多个基准测试中将关键路径长度缩短38-46%，且任务表现不降反升，显著提升了多智能体系统在时延敏感场景下的可用性和扩展性。

Conclusion: 明确考虑并行执行下的推理延迟优化对于多智能体系统的高效设计至关重要，LAMaS在降低延迟的同时保持或提升了任务性能，展示了其实用性和优越性。

Abstract: Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS

</details>


### [49] [Procedural Fairness in Multi-Agent Bandits](https://arxiv.org/abs/2601.10600)
*Joshua Caiata,Carter Blair,Kate Larson*

Main category: cs.MA

TL;DR: 本文提出了一种新的公平目标——程序公平，强调在多智能体多臂老虎机（MA-MAB）问题中应给予所有智能体平等的决策权力，而不仅仅优化结果公平。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体博弈和决策问题中过多强调结果公平，而忽视了决策过程中的参与权与程序正当性，既有心理学、经济学与罗尔斯理论证据支持程序公平的必要性。

Method: 在MA-MAB框架下，作者提出并量化了“程序公平”目标，并通过实证研究及理论证明比较了不同公平准则对结果及决策过程的影响。

Result: 实证结果显示，纯结果导向的公平方法牺牲了平等语音和代表性，而程序公平在兼顾结果和过程公平时，损害结果公平程度很小，展现出更优的公平属性和实践意义。

Conclusion: 结果表明，现有结果导向型公平方法会牺牲各智能体平等表达与代表性，而程序公平政策在保证各方话语权的同时，对结果公平（如平等和效用最大化）的损失极小。不同公平概念侧重的价值不可兼容，程序正当性应获得更多关注。

Abstract: In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcomes. Empirical results confirm that fairness notions based on optimizing for outcomes sacrifice equal voice and representation, while the sacrifice in outcome-based fairness objectives (like equality and utilitarianism) is minimal under procedurally fair policies. We further prove that different fairness notions prioritize fundamentally different and incompatible values, highlighting that fairness requires explicit normative choices. This paper argues that procedural legitimacy deserves greater focus as a fairness objective, and provides a framework for putting procedural fairness into practice.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [50] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: Multiverse STM 同时支持无版本与多版本事务，针对不同负载均表现优异，特别是在长时间读取场景下大幅提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统 STM 不支持高效长时间（大批量）读取的问题，并避免多版本方案对不需版本管理的事务带来的性能损耗，提升 STM 在多种负载场景下的适应性和整体性能。

Method: 设计混合型 STM Multiverse，实现无版本与多版本事务的并行执行；并与多种现有 STM 系统进行性能对比实验。

Result: 论文提出了一种新型软件事务内存（STM）系统——Multiverse。Multiverse 能够同时支持无版本和多版本事务，并允许其并发执行。实验结果显示，在无长时间读取负载下，Multiverse 性能与现有主流无版本 STM 相当甚至更优；在长时间读取和频繁更新并存的工作负载下，Multiverse 显著优于其他 STM，部分场景下系统吞吐量提升达数量级。

Conclusion: Multiverse 在无需多版本支持的典型负载中保持高性能，在需要长时间读取和频繁更新的负载下显著超过现有 STM，兼顾灵活性与高效性。

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [51] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 本论文提出了两个工具——Babel和ORION，解决科学数据实际互操作性难题，推动FAIR原则的落地。


<details>
  <summary>Details</summary>
Motivation: 尽管FAIR原则已广泛推动标准化，但科学数据资源在实际互操作中，由于标识符和数据模型多样性，生态系统间仍难以实现高效互通。

Method: Babel工具通过高性能API公开同义标识符聚类，解决不同标识符方案的问题；ORION工具通过将不同知识库转换为统一、社区维护的数据模型，解决数据模型多样化的问题。

Result: 成功创建了可供下载和使用的完全互操作知识库库，验证了Babel和ORION工具在支持数据互操作方面的有效性。

Conclusion: 通过Babel和ORION，能够有效提升科学数据资源间的互操作性，生成可下载、可用的完全互操作知识库。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [52] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: 本文提出SDP算法，能高效发现最有价值的前k个数据库功能依赖，并显著减少计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 传统FD发现算法计算量和结果集过大，不适合大规模、高维数据库，亟需更高效且能聚焦有价值FD的方法。

Method: SDP按冗余度排名发现前k个功能依赖。利用冗余度单调性和上界，剪枝不必要的搜索分支。通过属性排序、统计矩阵和全局调度等优化进一步提速。

Result: 提出SDP算法用于高效发现数据库中的前k个冗余度最高的功能依赖（FD），并通过冗余度上界剪枝显著提升发现效率。实验证明SDP在速度和内存消耗上均优于传统方法。

Conclusion: SDP算法能在保证发现高价值FD的前提下大幅提升效率，适用于大规模高维数据，优于穷举发现方法。

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [53] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 本文提出了通过合并结构相似的SQL语句或事务，在应用层提升事务处理性能的机制，并设计了TransactionMerger中间件。


<details>
  <summary>Details</summary>
Motivation: 事务处理中存在大量结构相似和冗余操作，合并这些操作有望显著提升数据库系统吞吐量，但需保证对隔离级别的正确性要求。

Method: 采用三种优化方式：1）用SQL语义合并相似语句；2）去除冗余读取操作；3）通过预计算，合并跨事务存在争用的语句。核心架构是TransactionMerger中间件，并辅以静态分析工具辅助识别可合并机会，同时对典型基准和实际应用进行事务重写实践和性能评估。

Result: 在标准TPC-C基准测试和实际Spree电商应用上，应用事务合并方案均获得大幅吞吐量提升，最大分别达2.65倍和3.52倍。

Conclusion: 事务合并可大幅提升事务吞吐量，在TPC-C与Spree应用中分别提升2.65倍与3.52倍。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [54] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 本文提出一个高效完整的算法，将数学数据模型方案自动转为关系型数据结构及其非关系约束，并在家谱树建模中验证有效性，且提供了实际代码示例与开发建议。


<details>
  <summary>Details</summary>
Motivation: 需要将数学化的数据模型与实际数据库关系型模型进行桥接，同时保证部分数据约束无法仅用关系型建模实现，故需有配套算法及代码规范以实现完整性和一致性。

Method: 提出一种伪代码算法，将Elementary Mathematical Data Model方案转换为关系型数据模型，同时生成一组非关系型约束集。算法用于MatBase智能数据库管理系统原型，并针对家谱树子宇宙建模实例验证。还提供了用于约束的SQL与VBA代码示例及开发指南。

Result: 算法经证明非常快速、坚实、完整，并且达到最优。在实际家谱树建模子领域成功应用，并辅以可实施的SQL与VBA约束代码示例。

Conclusion: 所提算法及方法能够有效将数学数据模型映射到关系型数据库，实现非关系约束的表达与执行，并能促进智能数据库系统的发展。

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>
