{"id": "2601.09434", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.09434", "abs": "https://arxiv.org/abs/2601.09434", "authors": ["Di Zhao", "Longhui Ma", "Siwei Wang", "Miao Wang", "Yi Kong"], "title": "SC-MAS: Constructing Cost-Efficient Multi-Agent Systems with Edge-Level Heterogeneous Collaboration", "comment": null, "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) enhance complex problem solving through multi-agent collaboration, but often incur substantially higher costs than single-agent systems. Recent MAS routing methods aim to balance performance and overhead by dynamically selecting agent roles and language models. However, these approaches typically rely on a homogeneous collaboration mode, where all agents follow the same interaction pattern, limiting collaboration flexibility across different roles. Motivated by Social Capital Theory, which emphasizes that different roles benefit from distinct forms of collaboration, we propose SC-MAS, a framework for constructing heterogeneous and cost-efficient multi-agent systems. SC-MAS models MAS as directed graphs, where edges explicitly represent pairwise collaboration strategies, allowing different agent pairs to interact through tailored communication patterns. Given an input query, a unified controller progressively constructs an executable MAS by selecting task-relevant agent roles, assigning edge-level collaboration strategies, and allocating appropriate LLM backbones to individual agents. Experiments on multiple benchmarks demonstrate the effectiveness of SC-MAS. In particular, SC-MAS improves accuracy by 3.35% on MMLU while reducing inference cost by 15.38%, and achieves a 3.53% accuracy gain with a 12.13% cost reduction on MBPP. These results validate the feasibility of SC-MAS and highlight the effectiveness of heterogeneous collaboration in multi-agent systems.", "AI": {"tldr": "SC-MAS\u901a\u8fc7\u5f02\u6784\u534f\u4f5c\u5b9e\u73b0\u66f4\u4f18\u51c6\u786e\u7387\u548c\u63a8\u7406\u6210\u672c\uff0c\u5728\u591a\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u4f5c\u6a21\u5f0f\u5355\u4e00\uff0c\u9650\u5236\u4e86\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u89d2\u8272\u7684\u7075\u6d3b\u534f\u4f5c\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u3001\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSC-MAS\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5efa\u6a21\u4e3a\u6709\u5411\u56fe\uff0c\u8fb9\u660e\u786e\u8868\u793a\u534f\u4f5c\u7b56\u7565\uff1b\u7cfb\u7edf\u901a\u8fc7\u7edf\u4e00\u63a7\u5236\u5668\u52a8\u6001\u5206\u914d\u89d2\u8272\u3001\u534f\u4f5c\u6a21\u5f0f\u4e0e\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u5f02\u6784\u3001\u4f4e\u6210\u672c\u534f\u4f5c\u3002", "result": "SC-MAS\u5728MMLU\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u53473.35%\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e15.38%\uff1b\u5728MBPP\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u53473.53%\uff0c\u6210\u672c\u964d\u4f4e12.13%\u3002", "conclusion": "\u5f02\u6784\u534f\u4f5c\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd\u4e0e\u6027\u4ef7\u6bd4\uff0cSC-MAS\u6846\u67b6\u4e3a\u9ad8\u6548\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.09404", "categories": ["cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09404", "abs": "https://arxiv.org/abs/2601.09404", "authors": ["Jun-Peng Zhu", "Boyan Niu", "Peng Cai", "Zheming Ni", "Kai Xu", "Jiajun Huang", "Shengbo Ma", "Bing Wang", "Xuan Zhou", "Guanglei Bao", "Donghui Zhang", "Liu Tang", "Qi Liu"], "title": "TiInsight: A SQL-based Automated Exploratory Data Analysis System through Large Language Models", "comment": "4 pages, 5 figures", "summary": "The SQL-based exploratory data analysis has garnered significant attention within the data analysis community. The emergence of large language models (LLMs) has facilitated the paradigm shift from manual to automated data exploration. However, existing methods generally lack the ability for cross-domain analysis, and the exploration of LLMs capabilities remains insufficient. This paper presents TiInsight, an SQL-based automated cross-domain exploratory data analysis system. First, TiInsight offers a user-friendly GUI enabling users to explore data using natural language queries. Second, TiInsight offers a robust cross-domain exploratory data analysis pipeline: hierarchical data context (i.e., HDC) generation, question clarification and decomposition, text-to-SQL (i.e., TiSQL), and data visualization (i.e., TiChart). Third, we have implemented and deployed TiInsight in the production environment of PingCAP and demonstrated its capabilities using representative datasets. The demo video is available at https://youtu.be/JzYFyYd-emI.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u9762\u5411SQL\u7684\u81ea\u52a8\u5316\u8de8\u9886\u57df\u6570\u636e\u63a2\u7d22\u7cfb\u7edfTiInsight\uff0c\u5177\u5907\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u6570\u636e\u53ef\u89c6\u5316\u80fd\u529b\uff0c\u5e76\u5df2\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684SQL\u6570\u636e\u5206\u6790\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u52a8\u5316\u548c\u8de8\u9886\u57df\u5206\u6790\u80fd\u529b\uff0c\u4e14\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u63a2\u7d22\u9886\u57df\u7684\u4f5c\u7528\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408GUI\u754c\u9762\u3001HDC\u5c42\u7ea7\u6570\u636e\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u95ee\u9898\u6f84\u6e05\u4e0e\u5206\u89e3\u3001\u81ea\u7136\u8bed\u8a00\u8f6cSQL\uff08TiSQL\uff09\u4ee5\u53ca\u6570\u636e\u53ef\u89c6\u5316\uff08TiChart\uff09\u6784\u6210\u5b8c\u6574\u7684\u6570\u636e\u63a2\u7d22\u6d41\u7a0b\u3002", "result": "\u63d0\u51fa\u4e86TiInsight\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86SQL\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u8de8\u9886\u57df\u6570\u636e\u63a2\u7d22\uff0c\u5e76\u5df2\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u90e8\u7f72\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u7cfb\u7edf\u5728\u4ee3\u8868\u6027\u6570\u636e\u96c6\u4e0a\u7684\u80fd\u529b\u3002", "conclusion": "TiInsight\u80fd\u6709\u6548\u63d0\u5347\u6570\u636e\u5206\u6790\u7684\u81ea\u52a8\u5316\u548c\u8de8\u9886\u57df\u80fd\u529b\uff0c\u63a8\u52a8\u6570\u636e\u63a2\u7d22\u4ece\u4eba\u5de5\u5411\u667a\u80fd\u8f6c\u53d8\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u7ea7\u5e94\u7528\u3002"}}
{"id": "2601.08950", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08950", "abs": "https://arxiv.org/abs/2601.08950", "authors": ["Mayank Sharma", "Roy Pea", "Hari Subramonyam"], "title": "ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue", "comment": null, "summary": "In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.", "AI": {"tldr": "\u6784\u5efa\u5e76\u516c\u5f00\u4e86\u9762\u5411\u77e5\u8bc6\u5efa\u6784\u7684\u6559\u5b66\u5bf9\u8bdd\u6570\u636e\u96c6ConvoLearn\uff0c\u5fae\u8c03\u540e\u6a21\u578b\u5728\u6559\u5b66\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u5b58\u5728\u8fc7\u5ea6\u76f4\u63a5\u7ed9\u51fa\u7b54\u6848\u3001\u96be\u4ee5\u652f\u6301\u5bf9\u8bdd\u5f0f\u548c\u5efa\u6784\u4e3b\u4e49\u5b66\u4e60\u7b49\u6839\u672c\u6027\u7f3a\u9677\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u8bbe\u8ba1\u5177\u6709\u6559\u80b2\u7406\u8bba\u652f\u6491\u7684\u6570\u636e\u96c6\u4e0e\u8bad\u7ec3\u65b9\u5f0f\uff0c\u63d0\u5347\u5176\u6559\u5b66\u6709\u6548\u6027\u3002", "method": "\u6570\u636e\u96c6\u91c7\u96c6\u65b9\u9762\uff0c\u901a\u8fc7\u4eba\u7c7b\u6559\u5e08\u4e0e\u6a21\u62df\u5b66\u751f\u5f00\u5c55\u53d7\u63a7\u7684\u79d1\u5b66\u5bf9\u8bdd\u751f\u6210\u534a\u5408\u6210\u6570\u636e\u96c6\u3002\u4f7f\u7528QLoRA\u6280\u672f\u5bf9Mistral 7B\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5e76\u901a\u8fc731\u540d\u6559\u5e08\u7684\u4eba\u5de5\u8bc4\u4f30\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConvoLearn\u7684\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u901a\u8fc7\u77e5\u8bc6\u5efa\u6784\u7406\u8bba\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6559\u5b66\u5bf9\u8bdd\u80fd\u529b\uff0c\u5177\u4f53\u6db5\u76d6\u516d\u5927\u6838\u5fc3\u6559\u80b2\u7ef4\u5ea6\uff0c\u5e76\u5728\u6a21\u62df\u573a\u666f\u4e0b\u6784\u5efa\u4e861250\u6bb5\u6559\u4e0e\u5b66\u7684\u5bf9\u8bdd\u3002\u901a\u8fc7\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u91c7\u7528QLoRA\u8fdb\u884c\u8bad\u7ec3\uff0c\u6a21\u578b\u5c55\u793a\u51fa\u77e5\u8bc6\u5efa\u6784\u5bfc\u5411\u7684\u884c\u4e3a\u8f6c\u53d8\u3002\u4eba\u7c7b\u6559\u5e08\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5fae\u8c03\u540e\u7684Mistral 7B\u5728\u8868\u73b0\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548cClaude Sonnet 4.5\u3002", "conclusion": "\u6784\u5efaConvoLearn\u6570\u636e\u96c6\u5e76\u57fa\u4e8e\u5176\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u4fc3\u8fdbLLMs\u5728\u6559\u80b2\u573a\u666f\u4e2d\u7684\u77e5\u8bc6\u5efa\u6784\u578b\u5bf9\u8bdd\u884c\u4e3a\uff0c\u4e3a\u540e\u7eed\u6784\u5efa\u5efa\u6784\u4e3b\u4e49AI\u5bfc\u5e08\u53ca\u5176\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u6846\u67b6\u3002"}}
{"id": "2601.09032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09032", "abs": "https://arxiv.org/abs/2601.09032", "authors": ["Logan Ritchie", "Sushant Mehta", "Nick Heiner", "Mason Yu", "Edwin Chen"], "title": "The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments", "comment": null, "summary": "The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \\emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.", "AI": {"tldr": "\u672c\u6587\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u524d\u6cbf\u5927\u6a21\u578b\u5728\u771f\u5b9e\u7535\u5546\u5de5\u4f5c\u4efb\u52a1\u4e0a\u7684\u591a\u6b65\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5de5\u5177\u4f7f\u7528\u3001\u89c4\u5212\u3001\u9002\u5e94\u6027\u3001\u624e\u6839\u6027\u4e0e\u5e38\u8bc6\u63a8\u7406\u7684\u80fd\u529b\u5c42\u7ea7\uff0c\u5e76\u53d1\u73b0\u5373\u4f7f\u6700\u5f3a\u6a21\u578b\u4efb\u52a1\u5931\u8d25\u7387\u4ecd\u9ad8\u8fbe40%\u3002\u63d0\u51fa\u4e86\u591a\u6837\u5316\u4efb\u52a1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5931\u8d25\u7279\u70b9\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u8ddd\u79bb\u4eba\u7c7b\u6c34\u5e73\u8fd8\u6709\u660e\u663e\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u57fa\u7840\u7684\u667a\u80fd\u4f53\u5feb\u901f\u53d1\u5c55\uff0cAI\u8bc4\u4f30\u65b9\u5f0f\u7531\u5355\u8f6e\u56de\u590d\u8f6c\u5411\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u591a\u6b65\u4efb\u52a1\u5b8c\u6210\u3002\u73b0\u5b9e\u5de5\u4f5c\u573a\u666f\u4e0b\u6a21\u578b\u90e8\u7f72\u9700\u6c42\u63a8\u52a8\u4e86\u5bf9\u6b64\u7c7b\u6a21\u578b\u591a\u5c42\u6b21\u80fd\u529b\u7684\u6df1\u5165\u68c0\u9a8c\u3002", "method": "\u5728Surge\u7684\u771f\u5b9e\u7535\u5546\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u9488\u5bf9150\u4e2a\u5de5\u4f5c\u573a\u666f\u4efb\u52a1\uff0c\u5bf9\u524d\u6cbfAI\u6a21\u578b\u8fdb\u884c\u4e86\u7ecf\u9a8c\u6027\u8bc4\u4f30\u3002\u5206\u6790\u6a21\u578b\u5728\u4efb\u52a1\u5b8c\u6210\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f52\u7eb3\u51faagent\u80fd\u529b\u5c42\u7ea7\u3002\u540c\u65f6\u91c7\u7528\u591a\u6837\u5316\u4efb\u52a1\u8bbe\u8ba1\u5e76\u7ed3\u5408\u9886\u57df\u4e13\u5bb6\u53c2\u4e0e\uff0c\u6df1\u5165\u5931\u8d25\u6848\u4f8b\u5206\u6790\u3002", "result": "\u5373\u4f7f\u5f53\u524d\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\uff0c\u5728\u7ea640%\u7684\u4efb\u52a1\u4e0a\u4ecd\u7136\u5931\u8d25\uff0c\u4e14\u8fd9\u4e9b\u5931\u8d25\u96c6\u4e2d\u5728\u80fd\u529b\u5c42\u7ea7\u7684\u76f8\u5e94\u4f4d\u7f6e\u3002\u5f31\u6a21\u578b\u4e3b\u8981\u5728\u5de5\u5177\u4f7f\u7528\u548c\u89c4\u5212\u65b9\u9762\u6709\u7f3a\u9677\uff0c\u5f3a\u6a21\u578b\u5219\u5728\u590d\u6742\u60c5\u5883\u63a8\u7406\u548c\u9690\u542b\u4fe1\u606f\u7406\u89e3\u4e0a\u5931\u8d25\u3002\u8be6\u7ec6\u5206\u6790\u4e86\u5931\u8d25\u539f\u56e0\u53ca\u5176\u5206\u5e03\u89c4\u5f8b\u3002", "conclusion": "\u5c3d\u7ba1\u524d\u6cbf\u6a21\u578b\u5df2\u80fd\u5b9e\u73b0\u591a\u6b65\u8fde\u8d2f\u884c\u4e3a\uff0c\u4f46\u5728\u4eba\u7c7b\u6c34\u5e73\u7684\u5b9e\u9645\u5de5\u4f5c\u573a\u666f\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u80fd\u529b\u7f3a\u53e3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4efb\u52a1\u4e2d\u5fc3\u7684RL\u73af\u5883\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u6574\u4f53\u4efb\u52a1\u591a\u6837\u6027\u4e0e\u4e13\u5bb6\u8d21\u732e\uff0c\u5bf9agent\u540e\u7eed\u53d1\u5c55\u5177\u6709\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2601.09097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09097", "abs": "https://arxiv.org/abs/2601.09097", "authors": ["Derrick Goh Xin Deik", "Quanyu Long", "Zhengyuan Liu", "Nancy F. Chen", "Wenya Wang"], "title": "Programming over Thinking: Efficient and Robust Multi-Constraint Planning", "comment": "8 pages of main text, 2 pages of references and and limitations, 37 pages of appendices", "summary": "Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.", "code_url": "https://github.com/DerrickGXD/SCOPE", "code_stars": 0, "code_last_update": "2026-01-13", "AI": {"tldr": "\u63d0\u51faSCOPE\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u63a8\u7406\u4e0e\u4ee3\u7801\u6267\u884c\uff0c\u5728\u591a\u7ea6\u675f\u4efb\u52a1\u4e2d\u5927\u5e45\u63d0\u5347\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u6210\u672c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u591a\u7ea6\u675f\u89c4\u5212\u5b58\u5728\uff1a1\uff09\u7eaf\u63a8\u7406\u6cd5\u63a8\u7406\u94fe\u957f\uff0c\u5bb9\u6613\u51fa\u9519\u4e14\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u9ad8\uff1b2\uff09LLM\u7ed3\u5408\u4ee3\u7801\u6216\u6c42\u89e3\u5668\u867d\u5177\u5907\u6267\u884c\u529b\uff0c\u4f46\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u95ee\u9898\u3002\u4e9f\u9700\u4e00\u79cd\u517c\u5177\u7075\u6d3b\u6027\u3001\u4e00\u81f4\u6027\u53ca\u9ad8\u6548\u6027\u7684\u901a\u7528\u89c4\u5212\u6846\u67b6\u3002", "method": "\u63d0\u51faSCOPE\uff08Scalable COde Planning Engine\uff09\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u7279\u5b9a\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e0e\u901a\u7528\u4ee3\u7801\u6267\u884c\u89e3\u8026\u3002\u901a\u8fc7\u5148\u7528LLM\u8fdb\u884c\u903b\u8f91\u63a8\u7406\uff0c\u518d\u7528\u7b97\u6cd5\u4ee3\u7801\u6267\u884c\u7ea6\u675f\u6c42\u89e3\uff0c\u4ece\u800c\u63d0\u5347\u4e00\u81f4\u6027\u548c\u901a\u7528\u6027\u3002", "result": "SCOPE\u5229\u7528\u6a21\u5757\u5316\u601d\u60f3\u5b9e\u73b0\u63a8\u7406\u4e0e\u6267\u884c\u7684\u5206\u79bb\uff0c\u4f7f\u6c42\u89e3\u5668\u53ef\u91cd\u7528\u3001\u786e\u5b9a\u3001\u4e00\u81f4\u3002\u57fa\u4e8eGPT-4o\u5728TravelPlanner\u4efb\u52a1\u4e0a\u8fbe\u523093.1%\u6210\u529f\u7387\uff0c\u6bd4\u6700\u4f73\u5bf9\u7167\u65b9\u6cd5\uff08CoT\uff09\u63d0\u534761.6%\uff0c\u63a8\u7406\u6210\u672c\u964d1.4\u500d\uff0c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed4.67\u500d\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "SCOPE\u6709\u6548\u7834\u89e3\u4e86LLM\u5728\u591a\u7ea6\u675f\u4efb\u52a1\u4e2d\u7684\u4e00\u81f4\u6027\u53ca\u901a\u7528\u6027\u96be\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u89c4\u5212\u5f15\u64ce\uff0c\u5bf9\u591a\u9886\u57df\u590d\u6742\u4efb\u52a1\u5177\u8f83\u5f3a\u9002\u5e94\u6027\u3002"}}
{"id": "2601.09100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09100", "abs": "https://arxiv.org/abs/2601.09100", "authors": ["Lixiang Zhang", "Chenggong Zhao", "Qing Gao", "Xiaoke Zhao", "Gengyi Bai", "Jinhu Lv"], "title": "DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model", "comment": "14 pages, 6 figures", "summary": "Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.", "AI": {"tldr": "\u63d0\u51faDScheLLM\u65b9\u6cd5\uff0c\u5229\u7528\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u7cfb\u7edf\uff08\u5feb-\u6162\uff09\u63a8\u7406\u67b6\u6784\uff0c\u9996\u6b21\u9ad8\u6548\u89e3\u51b3\u52a8\u6001\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff0c\u5b9e\u9a8c\u6548\u679c\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u8c03\u5ea6\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u6270\u52a8\uff08\u5982\u52a0\u5de5\u65f6\u95f4\u53d8\u5316\u3001\u673a\u5668\u6545\u969c\u3001\u4efb\u52a1\u63d2\u5165\uff09\uff0c\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u57fa\u4e8e\u95ee\u9898\u6c42\u89e3\u5668\u751f\u6210\u7684\u7cbe\u786e\u8c03\u5ea6\u6570\u636e\u5bf9\u5927\u6a21\u578b\u8fdb\u884cLoRA\u5fae\u8c03\uff0c\u5728\u5feb-\u6162\u63a8\u7406\u6a21\u5f0f\u4e0b\u5206\u522b\u5904\u7406\u4e0d\u540c\u89c4\u6a21\u6270\u52a8\uff0c\u5b9e\u73b0\u52a8\u6001\u4e8b\u4ef6\u4e0b\u7684\u9ad8\u6548\u667a\u80fd\u8c03\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5feb\u601d\u6a21\u5f0f\u80fd\u9ad8\u6548\u5730\u4ea7\u751f\u9ad8\u8d28\u91cf\u8c03\u5ea6\u65b9\u6848\uff0c\u6162\u601d\u6a21\u5f0f\u751f\u6210\u7684\u51b3\u7b56\u8f93\u5165\u4e0e\u4e13\u4e1a\u6c42\u89e3\u5668\u517c\u5bb9\u4e14\u683c\u5f0f\u89c4\u8303\u3002\u4e24\u79cd\u6a21\u5f0f\u5747\u663e\u793a\u51fa\u9002\u5e94\u5404\u7c7b\u52a8\u6001\u6270\u52a8\u7684\u4f18\u826f\u80fd\u529b\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u8f66\u95f4\u8c03\u5ea6\u9886\u57df\u5c55\u73b0\u51fa\u667a\u80fd\u3001\u81ea\u9002\u5e94\u4f18\u5316\u7684\u5de8\u5927\u6f5c\u529b\uff0cDScheLLM\u4e3a\u5de5\u4e1a\u8c03\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.09105", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09105", "abs": "https://arxiv.org/abs/2601.09105", "authors": ["Wenbin Li", "Jingling Wu", "Xiaoyong Lin. Jing Chen", "Cong Chen"], "title": "AviationLMM: A Large Multimodal Foundation Model for Civil Aviation", "comment": "Accepted by 2025 7th International Conference on Interdisciplinary Computer Science and Engineering (ICICSE 2025) conference, Chongqing, China; 9 pages,1 figure,5 tables", "summary": "Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAviationLMM\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u91cd\u70b9\u89e3\u51b3\u6c11\u822aAI\u788e\u7247\u5316\u3001\u6570\u636e\u5f02\u6784\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u6c11\u822a\u667a\u80fd\u5316\u8fdb\u7a0b\u3002", "motivation": "\u73b0\u6709\u6c11\u822aAI\u7cfb\u7edf\u788e\u7247\u5316\u3001\u6570\u636e\u5272\u88c2\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u4fe1\u606f\u6574\u5408\u4e0e\u667a\u80fd\u63a8\u7406\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u590d\u6742\u5e94\u7528\u9700\u6c42\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u586b\u8865AI\u57fa\u7840\u6a21\u578b\u4e0e\u6c11\u822a\u9700\u6c42\u4e4b\u95f4\u7684\u6280\u672f\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u67b6\u6784\uff0c\u6574\u5408\u7a7a\u5730\u8bed\u97f3\u3001\u76d1\u63a7\u96f7\u8fbe\u3001\u673a\u8f7d\u9065\u6d4b\u3001\u89c6\u9891\u4e0e\u7ed3\u6784\u5316\u6587\u672c\u7b49\u591a\u6a21\u6001\u8f93\u5165\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0e\u878d\u5408\u6280\u672f\uff0c\u5b9e\u73b0\u7075\u6d3b\u8f93\u51fa\u3002\u5f3a\u8c03\u573a\u666f\u751f\u6210\u3001\u9884\u8bad\u7ec3\u3001\u9690\u79c1\u3001\u4fe1\u4efb\u4e0e\u9c81\u68d2\u6027\u7b49\u5173\u952e\u6280\u672f\u73af\u8282\u3002", "result": "\u63d0\u51faAviationLMM\u5927\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u7edf\u5408\u6c11\u822a\u591a\u6e90\u5f02\u6784\u6570\u636e\uff0c\u5b9e\u73b0\u8bed\u97f3\u3001\u96f7\u8fbe\u3001\u4f20\u611f\u3001\u6587\u672c\u7b49\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\uff0c\u4e3a\u6c11\u822a\u5b89\u5168\u3001\u6548\u7387\u3001\u5ba2\u6237\u6ee1\u610f\u5ea6\u7b49\u63d0\u4f9b\u667a\u80fd\u652f\u6301\u3002\u6a21\u578b\u53ef\u611f\u77e5\u591a\u6a21\u6001\u8f93\u5165\uff0c\u5e76\u8f93\u51fa\u60c5\u5883\u6458\u8981\u3001\u98ce\u9669\u9884\u8b66\u3001\u9884\u6d4b\u8bca\u65ad\u3001\u4e8b\u6545\u91cd\u5efa\u7b49\u591a\u5143\u7ed3\u679c\u3002", "conclusion": "AviationLMM\u6709\u671b\u6210\u4e3a\u6c11\u822a\u9886\u57df\u591a\u6a21\u6001\u6570\u636e\u667a\u80fd\u5904\u7406\u7684\u57fa\u7840\u5de5\u5177\uff0c\u4fc3\u8fdb\u822a\u7a7a\u5b89\u5168\u4e0e\u667a\u80fd\u5316\u53d1\u5c55\uff0c\u5e76\u63a8\u52a8\u76f8\u5173\u57fa\u7840\u7814\u7a76\u7684\u534f\u540c\u8fdb\u5c55\u3002"}}
{"id": "2601.09152", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09152", "abs": "https://arxiv.org/abs/2601.09152", "authors": ["Yiwen Tu", "Xuan Liu", "Lianhui Qin", "Haojian Jin"], "title": "PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?", "comment": null, "summary": "This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's \"privacy mind\", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \\PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684PRA\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u548c\u8bc4\u4f30\u4e2a\u6027\u5316\u9690\u79c1\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u7528\u6237\u9690\u79c1\u5173\u6ce8\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u9886\u57df\u8fc1\u79fb\u6027\u3002", "motivation": "\u73b0\u6709\u9690\u79c1\u5173\u6ce8\u5206\u6790\u5927\u591a\u505c\u7559\u5728\u7fa4\u4f53\u5c42\u9762\uff0c\u7f3a\u4e4f\u5bf9\u7528\u6237\u4e2a\u4f53\u5316\u9690\u79c1\u63a8\u7406\u8fc7\u7a0b\u7684\u5efa\u6a21\uff0c\u96be\u4ee5\u7cbe\u51c6\u53cd\u6620\u6bcf\u4e2a\u7528\u6237\u5bf9\u4e0d\u540c\u9690\u79c1\u60c5\u5883\u7684\u771f\u5b9e\u53cd\u5e94\u3002", "method": "\u57fa\u4e8e\u4e2a\u4f53\u8bc4\u8bba\u5386\u53f2\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u7ed3\u5408\u9690\u79c1\u8ba4\u77e5\u7406\u8bba\uff0c\u6784\u5efaAI-agent\u6a21\u62df\u4e2a\u4f53\u2018\u9690\u79c1\u5fc3\u667a\u2019\uff0c\u901a\u8fc7\u60c5\u5883\u8fc7\u6ee4\u89e6\u53d1\u76f8\u5173\u9690\u79c1\u8bb0\u5fc6\uff0c\u751f\u6210\u6a21\u62df\u8bc4\u8bba\u3002\u53e6\u6709\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u5668\uff0c\u53c2\u8003\u6743\u5a01\u9690\u79c1\u5173\u6ce8\u5206\u7c7b\u6cd5\uff0c\u9a8c\u8bc1\u751f\u6210\u63a8\u7406\u7684\u53ef\u4fe1\u5ea6\u3002", "result": "PRA\u5728Hacker News\u771f\u5b9e\u8ba8\u8bba\u5b9e\u9a8c\u4e2d\uff0c\u8f83\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e86\u9690\u79c1\u5173\u6ce8\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u8de8\u9886\u57df\u8fc1\u79fb\u9690\u79c1\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "PRA\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7528\u6237\u5bf9\u9690\u79c1\u7684\u5173\u6ce8\uff0c\u5e76\u5728AI\u3001\u7535\u5546\u53ca\u533b\u7597\u9886\u57df\u5c55\u73b0\u51fa\u53ef\u8fc1\u79fb\u7684\u9690\u79c1\u63a8\u7406\u6a21\u5f0f\u3002"}}
{"id": "2601.09182", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09182", "abs": "https://arxiv.org/abs/2601.09182", "authors": ["JungMin Yun", "JuneHyoung Kwon", "MiHyeon Kim", "YoungBin Kim"], "title": "Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback", "comment": "Accepted to AAAI 2026 Workshop on AI for Scientific Research (AI4Research)", "summary": "The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.", "AI": {"tldr": "\u8bba\u6587\u6279\u8bc4\u4e86\u5f53\u524dLLM\u81ea\u52a8\u751f\u6210\u8bc4\u5ba1\u7684\u65b9\u6848\uff0c\u63d0\u51fa\u5e94\u4ee5LLM\u8f85\u52a9\u548c\u57f9\u8bad\u4eba\u7c7b\u5ba1\u7a3f\u4eba\u4e3a\u6838\u5fc3\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5ba1\u7a3f\u4eba\u5bfc\u5e08\u548c\u53cd\u9988\u7cfb\u7edf\uff0c\u63d0\u9ad8\u8bc4\u5ba1\u8d28\u91cf\uff0c\u5b9e\u73b0\u5b66\u672f\u751f\u6001\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "motivation": "AI\u7814\u7a76\u9886\u57df\u7684\u8fc5\u731b\u53d1\u5c55\u52a0\u5267\u4e86\u5ba1\u7a3f\u4eba\u7f3a\u53e3\uff0c\u5a01\u80c1\u540c\u884c\u8bc4\u8bae\u7684\u53ef\u6301\u7eed\u6027\uff0c\u5e76\u5bfc\u81f4\u8bc4\u5ba1\u8d28\u91cf\u4e0b\u6ed1\uff0c\u4e9f\u9700\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u6790\u73b0\u6709LLM\u81ea\u52a8\u751f\u6210\u8bc4\u5ba1\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4ee5\u9ad8\u8d28\u91cf\u8bc4\u5ba1\u6807\u51c6\u4e3a\u57fa\u7840\uff0c\u8bbe\u8ba1LLM\u8f85\u52a9\u7684\u5ba1\u7a3f\u4eba\u57f9\u8bad\u4e0e\u53cd\u9988\u7cfb\u7edf\u3002", "result": "\u63d0\u51faLLM\u8f85\u52a9\u7684\u5bfc\u5e08\u7cfb\u7edf\u548c\u53cd\u9988\u7cfb\u7edf\uff0c\u5206\u522b\u7528\u4e8e\u63d0\u5347\u5ba1\u7a3f\u4eba\u7684\u957f\u671f\u80fd\u529b\u548c\u5b9e\u65f6\u4f18\u5316\u5ba1\u7a3f\u8d28\u91cf\uff0c\u4e3a\u66f4\u52a0\u53ef\u6301\u7eed\u7684\u5b66\u672f\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u5e94\u4ee5\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u548c\u57f9\u8bad\u4eba\u7c7b\u5ba1\u7a3f\u4eba\u4e3a\u6838\u5fc3\uff0c\u63d0\u5347\u5ba1\u7a3f\u8d28\u91cf\u4e0e\u751f\u6001\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2601.09260", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09260", "abs": "https://arxiv.org/abs/2601.09260", "authors": ["Yan Liu", "Feng Zhang", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Han Liu", "Yangdong Deng"], "title": "Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models", "comment": null, "summary": "High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.", "AI": {"tldr": "CoT-Flow\u5c06\u94fe\u5f0f\u601d\u8003\u53d8\u4e3a\u6982\u7387\u6d41\uff0c\u4ee5\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u5730\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u94fe\u5f0f\u601d\u8003\u65b9\u6cd5\u5c06\u63a8\u7406\u8fc7\u7a0b\u89c6\u4e3a\u4e0d\u53ef\u5206\u5272\u7684\u6574\u4f53\uff0c\u65e0\u6cd5\u5ea6\u91cf\u5404\u6b65\u9aa4\u5bf9\u7ed3\u679c\u7684\u5177\u4f53\u8d21\u732e\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u3001\u4f18\u5316\u96be\u5ea6\u5927\u3002\u8bba\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u7ec6\u7c92\u5ea6\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86CoT-Flow\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u5404\u6b65\u9aa4\u89c6\u4e3a\u6982\u7387\u6d41\u3002\u57fa\u4e8e\u6b64\uff0c\u5206\u522b\u8bbe\u8ba1\u4e86\u6d41\u5f15\u5bfc\u89e3\u7801\uff08greedy flow-based decoding\uff09\u4ee5\u751f\u6210\u4fe1\u606f\u9ad8\u6548\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4ee5\u53ca\u6d41\u5f3a\u5316\u5b66\u4e60\u4ee5\u6784\u5efa\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "CoT-Flow\u6846\u67b6\u901a\u8fc7\u5c06\u79bb\u6563\u63a8\u7406\u6b65\u9aa4\u6982\u5ff5\u5316\u4e3a\u8fde\u7eed\u6982\u7387\u6d41\uff0c\u5b9a\u91cf\u8861\u91cf\u6bcf\u4e00\u6b65\u5bf9\u6700\u7ec8\u7b54\u6848\u7684\u8d21\u732e\uff0c\u4ece\u800c\u4f18\u5316\u5927\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u8fbe\u5230\u4e86\u66f4\u4f18\u5e73\u8861\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\u5ea6\u91cf\u548c\u4f18\u5316\uff0cCoT-Flow\u5b9e\u73b0\u4e86\u63a8\u7406\u6548\u7387\u4e0e\u63a8\u7406\u6027\u80fd\u7684\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u989d\u5916\u76d1\u7763\u6216\u5916\u90e8\u9a8c\u8bc1\u5668\u3002"}}
{"id": "2601.09264", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09264", "abs": "https://arxiv.org/abs/2601.09264", "authors": ["Ziyi Shi", "Xusen Guo", "Hongliang Lu", "Mingxing Peng", "Haotian Wang", "Zheng Zhu", "Zhenning Li", "Yuxuan Liang", "Xinhu Zheng", "Hai Yang"], "title": "Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants", "comment": "20pages, 6 figures, a 60-page supporting material pdf file", "summary": "Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u653f\u7b56\u5236\u5b9a\u6846\u67b6\uff0c\u5b9e\u73b0\u8de8\u884c\u653f\u533a\u75ab\u60c5\u7ba1\u7406\u534f\u540c\u3002\u5728\u7f8e\u56fdCOVID-19\u6570\u636e\u5b9e\u8bc1\u4e2d\uff0c\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u611f\u67d3\u548c\u6b7b\u4ea1\u6570\uff0c\u9a8c\u8bc1\u4e86\u5176\u75ab\u60c5\u7ba1\u63a7\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u75ab\u60c5\u9632\u63a7\u4e2d\uff0c\u4e0d\u540c\u884c\u653f\u533a\u4e4b\u95f4\u7684\u653f\u7b56\u5e38\u5e38\u5404\u81ea\u4e3a\u6218\u3001\u4e8b\u540e\u8c03\u6574\uff0c\u7f3a\u4e4f\u524d\u77bb\u6027\u548c\u533a\u57df\u534f\u540c\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a7\u5236\u75ab\u60c5\u8513\u5ef6\u3002\u8be5\u6587\u7ae0\u65e8\u5728\u89e3\u51b3\u8de8\u533a\u57df\u653f\u7b56\u534f\u540c\u548c\u4e3b\u52a8\u5e72\u9884\u8fd9\u4e00\u6838\u5fc3\u96be\u9898\u3002", "method": "\u4e3a\u6bcf\u4e2a\u884c\u653f\u533a\u5206\u914d\u4e00\u4e2aLLM\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u672c\u533a\u75ab\u60c5\u6f14\u5316\u3001\u8de8\u533a\u901a\u8baf\u3001\u771f\u5b9e\u6d41\u52a8\u4e0e\u653f\u7b56\u6570\u636e\uff0c\u901a\u8fc7\u75ab\u60c5\u6a21\u62df\u5668\u548c\u95ed\u73af\u4eff\u771f\uff0c\u667a\u80fd\u4f53\u95f4\u534f\u4f5c\u63a2\u7d22\u5e72\u9884\u65b9\u6848\u5e76\u5171\u8bc6\u534f\u8c03\u653f\u7b56\uff1b\u75282020\u5e74\u7f8e\u56fdCOVID-19\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u6548\u80fd\u9a8c\u8bc1\u3002", "result": "\u5728\u7f8e\u56fd\u5dde\u7ea7COVID-19\u771f\u5b9e\u6570\u636e\u6d4b\u8bd5\u4e0b\uff0c\u8be5\u6846\u67b6\u53ef\u5c06\u5355\u5dde\u7d2f\u8ba1\u611f\u67d3\u6570\u548c\u6b7b\u4ea1\u6570\u964d\u5e45\u6700\u9ad8\u8fbe63.7%\u548c40.1%\uff1b\u8de8\u5dde\u603b\u964d\u5e45\u5206\u522b\u8fbe39.0%\u548c27.0%\u3002\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u75ab\u60c5\u534f\u540c\u7ba1\u63a7\u4e2d\u7684\u663e\u8457\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u6709\u6548\u5b9e\u73b0\u8de8\u533a\u57df\u75ab\u60c5\u653f\u7b56\u7684\u534f\u8c03\u4e0e\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u75ab\u60c5\u9632\u63a7\u6548\u679c\u3002\u4e3a\u672a\u6765\u5e94\u5bf9\u7c7b\u4f3c\u516c\u5171\u536b\u751f\u5371\u673a\u63d0\u4f9b\u4e86\u667a\u80fd\u5316\u3001\u534f\u540c\u5316\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.09269", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09269", "abs": "https://arxiv.org/abs/2601.09269", "authors": ["Wencheng Ye", "Liang Peng", "Xiaoyang Yuan", "Yi Bin", "Pengpeng Zeng", "Hengyu Jin", "Heng Tao Shen"], "title": "RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering", "comment": null, "summary": "Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51faRISER\uff0c\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3001\u53ef\u63d2\u62d4\u7684\u63a8\u7406\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u7531\u673a\u5236\u52a8\u6001\u7ec4\u5408\u63a8\u7406\u5411\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u3001\u7cbe\u786e\u7684\u5927\u6a21\u578b\u63a8\u7406\u63a7\u5236\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u7279\u5b9a\u63a8\u7406\u4efb\u52a1\u4e2d\u901a\u5e38\u4f9d\u8d56\u8bad\u7ec3\u5bc6\u96c6\u578b\u7684\u53c2\u6570\u66f4\u65b0\u65b9\u6cd5\uff0c\u867d\u7136\u6fc0\u6d3b\u64cd\u63a7\uff08activation steering\uff09\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u51fa\u73b0\uff0c\u4f46\u76ee\u524d\u65b9\u6cd5\u624b\u52a8\u5e72\u9884\u4e14\u9759\u6001\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u63a8\u7406\u7684\u52a8\u6001\u6027\u3002", "method": "RISER\u6784\u5efa\u63a8\u7406\u5411\u91cf\u5e93\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u6839\u636e\u6bcf\u4e2a\u8f93\u5165\u52a8\u6001\u7ec4\u5408\u4e0e\u6fc0\u6d3b\u5411\u91cf\uff0c\u901a\u8fc7\u4efb\u52a1\u7ea7\u522b\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8def\u7531\u5668\uff0c\u4ee5\u5b9e\u73b0\u53ef\u7ec4\u5408\u6027\u548c\u81ea\u9002\u5e94\u6027\u7684\u63a8\u7406\u6fc0\u6d3b\u3002", "result": "RISER\u5728\u4e03\u9879\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u5347\u96f6\u6837\u672c\u63a8\u7406\u51c6\u786e\u73873.4%-6.5%\uff0c\u8d85\u8fc7CoT\uff08Chain-of-Thought\uff09\u63a8\u7406\uff0c\u540c\u65f6\u5177\u67092-3\u500d\u7684token\u6548\u7387\u548c\u66f4\u7a33\u5065\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "RISER\u80fd\u591f\u81ea\u9002\u5e94\u5730\u7ec4\u5408\u548c\u6fc0\u6d3b\u591a\u4e2a\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u5411\u91cf\uff0c\u5f62\u6210\u7cbe\u786e\u7684\u63a7\u5236\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u53ef\u63a7\u6027\u548c\u6548\u7387\uff0c\u62d3\u5c55\u4e86\u53c2\u6570\u9ad8\u6548\u63a8\u7406\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.09274", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09274", "abs": "https://arxiv.org/abs/2601.09274", "authors": ["Jian Zhang", "Yu He", "Zhiyuan Wang", "Zhangqi Wang", "Kai He", "Fangzhi Xu", "Qika Lin", "Jun Liu"], "title": "$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "comment": null, "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \\textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "code_url": "https://a3-bench.github.io", "AI": {"tldr": "A^3-Bench\u662f\u805a\u7126\u53cc\u5c3a\u5ea6\u8bb0\u5fc6\u6fc0\u6d3b\u7684\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\uff0c\u5229\u7528\u951a\u70b9\u548c\u5438\u5f15\u5b50\u673a\u5236\u5bf9\u6a21\u578b\u8fdb\u884c\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u8bc4\u6d4b\uff0c\u5e76\u5f15\u5165AAUI\u6307\u6807\u91cf\u5316\u8bb0\u5fc6\u5229\u7528\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bb0\u5fc6\u6fc0\u6d3b\u5bf9\u63a8\u7406\u8868\u73b0\u7684\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6216\u63a8\u7406\u8fde\u8d2f\u6027\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u63a8\u7406\u4e2d\u8bb0\u5fc6\u6fc0\u6d3b\u4e0e\u518d\u5229\u7528\u7684\u5173\u952e\u673a\u5236\u3002\u8be5\u673a\u5236\u6709\u52a9\u4e8e\u63d0\u5347\u63a8\u7406\u4e00\u81f4\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u65b0\u7684\u8bc4\u6d4b\u6846\u67b6\u63ed\u793a\u5e76\u5ea6\u91cf\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86A^3-Bench\u57fa\u51c6\uff0c\u901a\u8fc7\u951a\u70b9\uff08Anchor\uff09\u548c\u5438\u5f15\u5b50\uff08Attractor\uff09\u7684\u8bb0\u5fc6\u6fc0\u6d3b\u673a\u5236\uff0c\u8bc4\u4f30\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u8bb0\u5fc6\u9a71\u52a8\u8fc7\u7a0b\u3002\u91c7\u7528SAPM\u6d41\u7a0b\u7cfb\u7edf\u6027\u6ce8\u91ca\u8de8\u9886\u57df2,198\u9053\u79d1\u5b66\u63a8\u7406\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86AAUI\uff08\u951a\u70b9-\u5438\u5f15\u5b50\u5229\u7528\u6307\u6570\uff09\u6307\u6807\uff0c\u4ee5\u91cf\u5316\u8bb0\u5fc6\u6fc0\u6d3b\u7387\u3002\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u6a21\u578b\u53ca\u8303\u5f0f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6784\u5efa\u4e86A^3-Bench\uff0c\u5e76\u5728\u5404\u7c7b\u57fa\u5ea7\u6a21\u578b\u548c\u63a8\u7406\u8303\u5f0f\u4e0a\uff0c\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u8bb0\u5fc6\u6fc0\u6d3b\uff08\u951a\u70b9\u4e0e\u5438\u5f15\u5b50\u5229\u7528\uff09\u4e0e\u63a8\u7406\u8868\u73b0\u7684\u76f8\u5173\u6027\uff1bAAUI\u6307\u6807\u6e05\u6670\u53cd\u6620\u4e86\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8bb0\u5fc6\u6fc0\u6d3b\u80fd\u529b\u7684\u5dee\u5f02\u3002", "conclusion": "A^3-Bench\u6709\u6548\u5f25\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u8bb0\u5fc6\u9a71\u52a8\u673a\u5236\u8bc4\u6d4b\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u7814\u7a76\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u8bb0\u5fc6\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u65b0\u89c6\u89d2\uff0c\u672a\u6765\u53ef\u7528\u4e8e\u8fdb\u4e00\u6b65\u63a8\u52a8\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u4e0e\u77e5\u8bc6\u518d\u5229\u7528\u80fd\u529b\u4e0a\u7684\u63d0\u5347\u3002"}}
{"id": "2601.09278", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09278", "abs": "https://arxiv.org/abs/2601.09278", "authors": ["Xiaohan Yu", "Chao Feng", "Lang Mei", "Chong Chen"], "title": "M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning", "comment": null, "summary": "Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e86\u89e3\u51b3\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\u6cdb\u5316\u53ca\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u7684\u65b0\u6a21\u578bM$^3$Searcher\uff0c\u5e76\u6784\u5efa\u6570\u636e\u96c6\u3002\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u590d\u6742\u4efb\u52a1\u4e2d\u6548\u679c\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709DeepResearch\u98ce\u683c\u7684\u667a\u80fd\u4f53\u5728\u81ea\u52a8\u4fe1\u606f\u83b7\u53d6\u548c\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ec5\u9650\u4e8e\u6587\u672c\u6a21\u6001\uff0c\u5bf9\u4e8e\u6269\u5c55\u5230\u591a\u6a21\u6001\u73af\u5883\u9762\u4e34\u7740\u6a21\u6001\u5de5\u5177\u6cdb\u5316\u548c\u590d\u6742\u591a\u6a21\u6001\u68c0\u7d22\u8def\u5f84\u6570\u636e\u7a00\u7f3a\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faM$^3$Searcher\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5c06\u4fe1\u606f\u83b7\u53d6\u4e0e\u7b54\u6848\u63a8\u5bfc\u89e3\u8026\uff0c\u91c7\u7528\u9762\u5411\u68c0\u7d22\u7684\u591a\u76ee\u6807\u5956\u52b1\u4f18\u5316\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u591a\u8df3\u6570\u636e\u96c6MMSearchVQA\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "M$^3$Searcher\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u8f83\u5f3a\u7684\u8fc1\u79fb\u9002\u5e94\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u591a\u76ee\u6807\u5956\u52b1\u4f18\u5316\uff0cM$^3$Searcher\u80fd\u66f4\u597d\u5730\u517c\u987e\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u63a8\u7406\u5408\u7406\u6027\u548c\u68c0\u7d22\u80fd\u529b\uff0c\u9002\u5408\u590d\u6742\u591a\u6a21\u6001\u641c\u7d22\u4efb\u52a1\u3002"}}
{"id": "2601.09281", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09281", "abs": "https://arxiv.org/abs/2601.09281", "authors": ["Jingjing Zhou", "Gaoxiang Cong", "Li Su", "Liang Li"], "title": "STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.", "AI": {"tldr": "\u672c\u6587\u5173\u6ce8\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u751f\u6210\u590d\u6742\u63a8\u7406\u94fe\u65f6\u5f15\u53d1\u7684\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u65e0\u9700\u53c2\u6570\u8c03\u6574\u3001\u63a8\u7406\u65f6\u5c31\u80fd\u5b9e\u65bd\u7684\u654f\u611f\u8f68\u8ff9\u8c03\u63a7\uff08STaR\uff09\u65b9\u6cd5\uff0c\u80fd\u5728\u63a8\u7406\u5168\u6d41\u7a0b\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u7684\u201c\u9057\u5fd8\u201d\u65b9\u6cd5\u4e3b\u8981\u53ea\u9488\u5bf9\u6700\u7ec8\u7b54\u6848\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u94fe\u4e2d\u95f4\u8fc7\u7a0b\u7684\u9690\u85cf\u654f\u611f\u6cc4\u9732\uff0c\u5bfc\u81f4\u4e0d\u80fd\u771f\u6b63\u4fdd\u62a4\u9690\u79c1\uff0c\u9700\u6784\u5efa\u80fd\u5168\u94fe\u8def\u611f\u77e5\u548c\u6d88\u9664\u654f\u611f\u5185\u5bb9\u7684\u65b0\u673a\u5236\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u56db\u6b65\uff1a1\uff09\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u68c0\u6d4b\u5b9a\u4f4d\u654f\u611f\u5185\u5bb9\uff1b2\uff09\u7528\u5b89\u5168\u524d\u7f00\u63d0\u793a\u6ce8\u5165\u6574\u4f53\u7ea6\u675f\uff1b3\uff09\u5728\u63a8\u7406\u94fe\u5404\u9636\u6bb5\u52a8\u6001\u963b\u65ad\u654f\u611f\u4fe1\u606f\u8f93\u51fa\uff1b4\uff09\u7528\u81ea\u9002\u5e94\u5206\u8bcd\u8fc7\u6ee4\u9632\u6b62\u590d\u73b0\u4e0e\u53d8\u4f53\u6cc4\u9732\u3002\u540c\u65f6\u63d0\u51faMCS\u4e0eMIA\u4e24\u9879\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728R-TOFU\u6570\u636e\u96c6\u5b9e\u9a8c\u663e\u793a\uff0cSTaR\u663e\u8457\u63d0\u5347\u4e86\u5305\u62ec\u4e2d\u95f4\u6b65\u9aa4\u5728\u5185\u7684\u5168\u94fe\u8def\u654f\u611f\u4fe1\u606f\u5220\u9664\u7684\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\uff0c\u5e76\u4ee5\u5f88\u5c0f\u7684\u6027\u80fd\u635f\u5931\u8fbe\u6210\u66f4\u5b8c\u6574\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u786e\u7acb\u4e86\u9886\u57df\u65b0\u6807\u51c6\u3002", "conclusion": "STaR\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u654f\u611f\u5185\u5bb9\u5168\u7a0b\u6291\u5236\uff0c\u89e3\u51b3\u4e86\u4ec5\u53bb\u9664\u6700\u7ec8\u7b54\u6848\u654f\u611f\u4fe1\u606f\u5bfc\u81f4\u7684\u9690\u79c1\u6cc4\u9732\u4e0e\u5b89\u5168\u9690\u60a3\uff0c\u5728R-TOFU\u57fa\u51c6\u4e0a\u8bc1\u660e\u80fd\u4ee5\u6781\u4f4e\u7684\u6548\u7528\u635f\u5931\u5b9e\u73b0\u7a33\u5b9a\u3001\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2601.09282", "categories": ["cs.AI", "cs.DC", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09282", "abs": "https://arxiv.org/abs/2601.09282", "authors": ["Leszek Sliwko", "Jolanta Mizeria-Pietraszko"], "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing", "comment": null, "summary": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u3001\u610f\u56fe\u9a71\u52a8\u7684\u96c6\u7fa4\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u89e3\u6790\u7528\u6237\u8f6f\u504f\u597d\u63d0\u793a\uff0c\u5b9e\u73b0\u4e86\u66f4\u4fbf\u6377\u4e14\u51c6\u786e\u7684\u8c03\u5ea6\u914d\u7f6e\uff0c\u5e76\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u96c6\u7fa4\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u9700\u8981\u590d\u6742\u914d\u7f6e\uff0c\u7528\u6237\u4f53\u9a8c\u548c\u53ef\u7528\u6027\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2aKubernetes\u8c03\u5ea6\u5668\u6269\u5c55\u5668\uff0c\u96c6\u6210LLM\uff08\u901a\u8fc7AWS Bedrock\uff09\uff0c\u89e3\u6790\u5e26\u6709\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\u7684\u8c03\u5ea6\u8bf7\u6c42\uff0c\u901a\u8fc7\u96c6\u7fa4\u72b6\u6001\u7f13\u5b58\u53ca\u610f\u56fe\u5206\u6790\u5668\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u8fdb\u884c\u5b9a\u91cf\u4e0e\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u5728LLM\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u5206\u914d\u63d0\u793a\u4e0a\u53d6\u5f97\u4e86\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u5728\u516d\u5927\u8c03\u5ea6\u573a\u666f\u4e2d\uff0c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u6807\u51c6Kubernetes\u914d\u7f6e\uff0c\u5c24\u5176\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u4f7f\u7528LLM\u8fdb\u884c\u8bed\u4e49\u8f6f\u4eb2\u548c\u8c03\u5ea6\u53ef\u6709\u6548\u63d0\u5347\u5de5\u4f5c\u8d1f\u8f7d\u7f16\u6392\u7684\u6613\u7528\u6027\uff0c\u4f46\u73b0\u9636\u6bb5\u540c\u6b65LLM\u8c03\u7528\u5b58\u5728\u5ef6\u8fdf\uff0c\u9700\u5f02\u6b65\u5904\u7406\u4ee5\u6ee1\u8db3\u751f\u4ea7\u7ea7\u9700\u6c42\u3002"}}
{"id": "2601.09382", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09382", "abs": "https://arxiv.org/abs/2601.09382", "authors": ["Qinglong Shi", "Donghai Wang", "Hantao Zhou", "Jiguo Li", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He"], "title": "Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments", "comment": "8 pages, 2 figures", "summary": "Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e3b\u52a8\u578b\u4efb\u52a1\u5bfc\u5411\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u65b0\u8303\u5f0f\u4e0e\u7efc\u5408\u8bc4\u6d4b\u6807\u51c6\uff0c\u663e\u8457\u63d0\u5347\u957f\u5468\u671f\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u4ec5\u5177\u5907\u88ab\u52a8\u54cd\u5e94\uff0c\u5bf9\u957f\u671f\u610f\u56fe\u7ef4\u62a4\u53ca\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u4e9f\u9700\u7a81\u7834\u9759\u6001\u7528\u6237\u9700\u6c42\u4e0e\u52a8\u6001\u73af\u5883\u4e4b\u95f4\u7684\u80fd\u529b\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e3b\u52a8\u578b\u4efb\u52a1\u5bfc\u5411\u667a\u80fd\u4f53\u7684\u65b0\u4ea4\u4e92\u8303\u5f0f\uff0c\u5305\u62ec\u57fa\u4e8e\u610f\u56fe\u7684\u76d1\u63a7\u548c\u4e8b\u4ef6\u89e6\u53d1\u7684\u8ddf\u8fdb\uff0c\u5e76\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u5408\u6210\u7ba1\u9053\u4e0e\u65b0\u57fa\u51c6ChronosBench\u5bf9\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\uff08\u5305\u62ec\u7528\u6237\u610f\u56fe\u53d8\u5316\uff09\u4e0a\u4efb\u52a1\u5b8c\u6210\u7387\u8fbe\u523085.19%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u4e86\u6570\u636e\u9a71\u52a8\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e3b\u52a8\u578b\u4ea4\u4e92\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u751f\u6210\u7ed3\u5408\uff0c\u6709\u6548\u52a0\u5f3a\u4e86\u667a\u80fd\u4f53\u8de8\u8d8a\u9759\u6001\u7528\u6237\u610f\u56fe\u4e0e\u52a8\u6001\u73af\u5883\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u5bf9\u957f\u5468\u671f\u667a\u80fd\u4f53\u4ea4\u4e92\u6709\u91cd\u8981\u63a8\u52a8\u610f\u4e49\u3002"}}
{"id": "2601.09536", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09536", "abs": "https://arxiv.org/abs/2601.09536", "authors": ["Dongjie Cheng", "Yongqi Li", "Zhixin Ma", "Hongru Cai", "Yupeng Hu", "Wenjie Wang", "Liqiang Nie", "Wenjie Li"], "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u591a\u6a21\u6001\u63a8\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u751f\u6210\u4e2d\u95f4\u56fe\u50cf\uff0c\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u63a8\u7406\u7684\u5e7f\u6cdb\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u7406\u65b9\u6cd5\u4e00\u822c\u91c7\u7528\u5355\u4e00\u4efb\u52a1\u6a21\u5f0f\uff0c\u96be\u4ee5\u63a8\u5e7f\u5230\u591a\u53d8\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\u7684\u666e\u9002\u6027\u652f\u6301\u3002\u4f5c\u8005\u65e8\u5728\u501f\u52a9\u751f\u6210\u5f0f\u673a\u5236\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u4e14\u7edf\u4e00\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Omni-R1\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9636\u6bb5SFT+RL\u8bad\u7ec3\uff0c\u7ed3\u5408\u611f\u77e5\u5bf9\u9f50\u635f\u5931\u548c\u611f\u77e5\u5956\u52b1\u4ee5\u5b9e\u73b0\u529f\u80fd\u6027\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u63a8\u51faOmni-R1-Zero\uff0c\u901a\u8fc7\u4ece\u6587\u672c\u63a8\u7406\u6570\u636e\u81ea\u4e3e\u751f\u6210\u89c6\u89c9\u6b65\u9aa4\uff0c\u65e0\u9700\u591a\u6a21\u6001\u6807\u6ce8\u3002", "result": "\u5b9e\u8bc1\u663e\u793a\uff0cOmni-R1\u53ef\u5728\u591a\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u5b9e\u73b0\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u63a8\u7406\uff0cOmni-R1-Zero\u65e0\u9700\u6807\u6ce8\u5374\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7Omni-R1\u5e73\u5747\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u6cdb\u5316\u6f5c\u80fd\u3002", "conclusion": "Omni-R1\u548cOmni-R1-Zero\u5747\u80fd\u5b9e\u73b0\u7edf\u4e00\u7684\u591a\u6a21\u6001\u751f\u6210\u5f0f\u63a8\u7406\uff0cOmni-R1-Zero\u8868\u73b0\u751a\u81f3\u53ef\u8fbe\u5230\u6216\u8d85\u8d8aOmni-R1\uff0c\u663e\u793a\u4e86\u65e0\u6807\u6ce8\u63a8\u7406\u7684\u53ef\u884c\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2601.09667", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09667", "abs": "https://arxiv.org/abs/2601.09667", "authors": ["Zhiyuan Hu", "Yunhai Hu", "Juncheng Liu", "Shuyue Stella Li", "Yucheng Wang", "Zhen Xu", "See-Kiong Ng", "Anh Tuan Luu", "Xinxing Xu", "Bryan Hooi", "Cynthia Breazeal", "Hae Won Park"], "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "comment": "Work in Progress", "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "AI": {"tldr": "MATTRL\u5728\u4e0d\u8c03\u6574\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7ecf\u9a8c\u6ce8\u5165\u548c\u591a\u4e13\u5bb6\u534f\u4f5c\uff0c\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u591a\u9886\u57df\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u7a33\u5b9a\u6027\u5dee\uff0c\u4e3b\u8981\u53d7\u961f\u53cb\u534f\u540c\u5bfc\u81f4\u7684\u975e\u5e73\u7a33\u6027\uff0c\u4ee5\u53ca\u7a00\u758f\u9ad8\u65b9\u5dee\u5956\u52b1\u5f71\u54cd\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u4e14\u7a33\u5065\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86MATTRL\uff08Multi-Agent Test-Time Reinforcement Learning\uff09\u6846\u67b6\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u5c06\u7ed3\u6784\u5316\u6587\u672c\u7ecf\u9a8c\u6ce8\u5165\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u56e2\u961f\uff0c\u901a\u8fc7\u4e13\u5bb6\u56e2\u961f\u591a\u8f6e\u8ba8\u8bba\u3001\u68c0\u7d22\u4e0e\u6574\u5408\u6d4b\u8bd5\u65f6\u7ecf\u9a8c\u3001\u5171\u8bc6\u51b3\u7b56\uff0c\u5e76\u7814\u7a76\u4e86\u8f6e\u6b21\u7ea7\u7ecf\u9a8c\u6c60\u7684\u5956\u52b1\u5f52\u56e0\u65b9\u6848\u53ca\u5176\u53cd\u9988\u6ce8\u5165\u3002", "result": "\u5728\u533b\u5b66\u3001\u6570\u5b66\u548c\u6559\u80b2\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMATTRL\u76f8\u8f83\u591a\u667a\u80fd\u4f53\u57fa\u7840\u7ebf\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.67%\u3001\u76f8\u8f83\u5355\u667a\u80fd\u4f53\u57fa\u7840\u7ebf\u63d0\u53478.67%\uff1b\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u5956\u52b1\u5f52\u56e0\u673a\u5236\u5bf9\u8bad\u7ec3\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "MATTRL\u6846\u67b6\u80fd\u591f\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7a33\u5065\u6027\u548c\u51c6\u786e\u7387\u3002"}}
{"id": "2512.07890", "categories": ["cs.MA", "cs.AI", "cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.07890", "abs": "https://arxiv.org/abs/2512.07890", "authors": ["Ryan Feng Lin", "Keyu Tian", "Hanming Zheng", "Congjing Zhang", "Li Zeng", "Shuai Huang"], "title": "CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models", "comment": null, "summary": "The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\u7684\u65b0\u6846\u67b6CrowdLLM\uff0c\u80fd\u591f\u66f4\u7cbe\u51c6\u5730\u6a21\u62df\u771f\u5b9e\u4eba\u7fa4\u884c\u4e3a\uff0c\u63d0\u5347\u6570\u5b57\u7fa4\u4f53\u7684\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b57\u4eba\u7fa4\u65e0\u6cd5\u5145\u5206\u6a21\u62df\u771f\u5b9e\u4eba\u7fa4\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u4e9f\u9700\u6539\u8fdb\u65b9\u6cd5\uff1b\u63d0\u5347\u6570\u5b57\u4eba\u7fa4\u8d28\u91cf\u6709\u52a9\u4e8e\u964d\u4f4e\u5b9e\u9a8c\u6210\u672c\u5e76\u89c4\u907f\u4f26\u7406\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86CrowdLLM\uff0c\u5c06\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u751f\u6210\u6a21\u578b\u7ed3\u5408\u4ee5\u63d0\u5347\u6570\u5b57\u4eba\u7fa4\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u8de8\u591a\u9886\u57df\u5b9e\u9a8c\uff0c\u5bf9CrowdLLM\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "CrowdLLM\u5728\u4f17\u5305\u3001\u6295\u7968\u3001\u7528\u6237\u8bc4\u5206\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\u90fd\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u62df\u771f\u6027\u548c\u5206\u5e03\u4e0a\u7684\u4fdd\u771f\u5ea6\uff0c\u76f8\u6bd4\u5355\u4e00LLM\u65b9\u6cd5\u66f4\u63a5\u8fd1\u771f\u5b9e\u4eba\u7fa4\u6570\u636e\u5206\u5e03\u3002", "conclusion": "CrowdLLM\u662f\u4e00\u79cd\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u4e14\u5177\u4ee3\u8868\u6027\u7684\u6570\u5b57\u4eba\u7fa4\u6a21\u62df\u65b9\u6848\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u793e\u4f1a\u6a21\u62df\u3001\u4f17\u5305\u3001\u63a8\u8350\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u6709\u6548\u964d\u4f4e\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u7684\u6210\u672c\u4e0e\u4f26\u7406\u98ce\u9669\u3002"}}
