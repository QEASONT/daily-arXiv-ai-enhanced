<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 34]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: 本文提出了Gated Sparse Attention (GSA)结构，将稀疏注意力与门控注意力融合，兼顾高效性与训练稳定性，显著提升长上下文语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型中注意力计算成本高，已有稀疏注意力和门控注意力各自解决不同问题，但未统一融合，本文旨在兼顾两者优点，提升训练稳定性、缓解注意力下沉并维持高效率。

Method: 提出GSA结构：包括带sigmoid激活的门控Lightning Indexer用于可解释选择打分、自适应稀疏控制器根据局部不确定性调节关注token数、双重门控机制分别应用于value和output阶段；进行了理论分析（复杂度、表达能力、收敛性）和大量大规模实验验证。

Result: 实验表明，在1.7B参数、400B token训练下，GSA达到稀疏基线12-16倍速度提升（128K context），困惑度由6.03降至5.70，RULER分数近翻倍，首token注意力比例由47%降至4%，训练损失尖峰减少98%。

Conclusion: GSA能在维持稀疏注意力高效率的基础上，显著改善训练稳定性、注意力分布和模型精度表现，尤其是在大模型与极长上下文下。

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [2] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: 本文提出MiRAGE框架，通过多智能体协作自动生成专业领域高复杂度、多模态的问答评测数据集，有效提升RAG系统评测的真实性和难度，为行业落地应用提供数据基础。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评测数据集依赖通用领域或纯文本，难以反映实际企业多模态、高复杂度场景。本工作旨在填补高要求专业领域RAG系统有效评测的空白。

Method: 提出并实现了基于多智能体协作的RAG系统评测框架MiRAGE。该系统包括递归上下文优化、对抗式事实验证、专家画像识别等多智能体流程，并对不同行业多领域数据进行实证测试及消融实验。

Result: MiRAGE在法规、金融、定量生物、新闻等四大领域生成的数据集表现出更高的推理复杂度（平均多跳数>2.3）和事实精准性。消融实验表明，若图片有文本描述，则LLM能够参与支持数据集构建。

Conclusion: MiRAGE能够生成高度符合领域特性、复杂推理需求和真实性更高的多模态、多跳问答评测数据集，为评估下一代RAG系统提供了可靠基础设施。视觉内容的真实落地仍是挑战。

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [3] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: 现有自动综述生成评价基准存在“学术价值”补维问题，本文提出了新基准DeepSurvey-Bench，并证明其评估结果可靠且与人工评审高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有评价基准忽视了生成综述文献的学术深度，仅从结构和引用等表层维度评估，难以准确衡量其真实学术价值，因此开发更科学的评测基准势在必行。

Method: 本文设计了一套涵盖信息价值、学术交流价值、研究指导价值的学术价值评价标准，构建了相应标注数据集，并通过实验验证了基准的一致性与有效性。

Result: 论文提出了DeepSurvey-Bench，一种用于综合评估自动化生成综述论文学术价值的新基准。该基准提出包含信息价值、学术交流价值、研究指导价值三大维度的评价标准，并基于此构建了带有学术价值标注的数据集，实现了对生成综述的深层学术价值评估。实验显示，DeepSurvey-Bench在人类评审中表现出高度一致性。

Conclusion: DeepSurvey-Bench 能有效、全面地评估自动生成综述的深层学术价值，弥补了现有基准只关注表层质量的不足。

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [4] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: Aeon以神经-符号方式组织记忆，突破传统RAG模糊、无结构检索，上述方法可在LLM中高效持久地管理和检索长期且有结构的记忆。


<details>
  <summary>Details</summary>
Motivation: 现有RAG结构难以捕捉交互的层级与时序性，导致信息检索断裂（Vector Haze），难以支撑自主智能体的长期推理与记忆管理；本研究旨在结合神经与符号方法重塑LLM记忆结构，并优化检索效率。

Method: 1. 固化Memory Palace结构：实现空间索引（Atlas），结合小世界图和B+树，提升索引和磁盘局部性。2. 构建Trace神经-符号情节图，实现连续性与语义联结。3. 采用Semantic Lookaside Buffer（SLB）语义预测缓存，实现子毫秒级检索延迟。4. 基于C++/Python零拷贝桥接保证一致性。5. 在工作负载和基准测试上验证系统性能和稳定性。

Result: 提出了Aeon系统，利用空间索引和神经-符号方法重塑LLM记忆，突破传统平面RAG系统的限制，实现高效且结构化的记忆管理。论文验证了Aeon在会话任务上可实现小于1毫秒的检索延迟与状态一致性。

Conclusion: Aeon显著改善了大型语言模型的记忆检索效率和结构化程度，为自主智能体的长期、结构化记忆管理提供了有效解决方案。

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [5] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: 这篇文章系统回顾了大规模视觉-语言模型（LVLMs）对跨模态虚假新闻检测的变革作用，梳理了方法发展脉络、模型架构、数据集、性能评测，并分析了技术挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态虚假新闻检测中带来显著进步，但尚无系统综述分析该领域变革及现状，亟需整合最新进展，为研究者提供全面参考。

Method: 本文采用文献综述方法，回顾了MFND领域从传统管道到基座模型驱动的转变，建立了模型架构、数据集、性能评测的结构化分类，分析现存难题并展望发展方向。

Result: 论文首次系统整理了LVLMs推动下的MFND研究进展，提出了分类框架，归纳了技术挑战和未来研究方向，对相关方法进行了详细对比和总结。

Conclusion: LVLMs极大提升了文本与图像联合建模能力，推动了跨模态虚假新闻检测从特征工程向端到端推理框架转型，但仍然面临可解释性、时序推理和领域泛化等挑战，未来需不断完善相关技术。

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [6] [Prometheus Mind: Retrofitting Memory to Frozen Language Models](https://arxiv.org/abs/2601.15324)
*Mark Wind*

Main category: cs.AI

TL;DR: 提出Prometheus Mind，通过11个可逆适配器模块为冻结的大模型添加记忆能力，解决关键技术挑战，取得高准确率，但对输入格式敏感。


<details>
  <summary>Details</summary>
Motivation: 为冻结的大型语言模型Qwen3-4B增加易于移除的记忆能力，无需修改模型权重或结构，实现在不破坏原模型的前提下，外接模块实现记忆增强。

Method: 通过11个模块化适配器扩展冻结的Qwen3-4B模型，总参数量增加7%；包括四个技术关键：1）对比方向发现法（CDD）在无标注数据下挖掘语义方向；2）分阶段适配器训练代替端到端优化防止训练崩溃；3）利用模型lm_head权重直接完成映射，无需训练；4）训练投影，避免Transformer表征层语义混淆。

Result: 在PrometheusExtract-132测试集上干净输入的检索准确率为94.4%，但在包含省略、语气词及隐含主语的非规范输入上准确率降至19.4%；关系分类准确率为47.3%，为主要性能瓶颈。

Conclusion: 适配器可为预训练语言模型引入记忆能力，效果显著且具有可逆性；但处理非规范输入及提升关系分类能力仍是待解决的问题。

Abstract: Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.

</details>


### [7] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: LOGIC方法提升了语音大模型识别新实体的准确率，效率高且几乎不增加误报。


<details>
  <summary>Details</summary>
Motivation: 现有语音大模型因训练知识静态，难以识别不断涌现的新实体。现有Prompting方法扩展性差，GEC后处理方法易造成过度修正。亟需高效稳健、且适应新兴领域实体的方法。

Method: LOGIC（Logit-Space Integration for Contextual Biasing）在解码层直接操作，通过解耦上下文注入与输入处理，实现相对实体列表长度的常数复杂度，避免了Prompting的窗口限制、推理延迟和信息丢失问题。

Result: 本文提出了一种名为LOGIC的新方法，用于提升语音大模型（Speech LLM）识别新兴实体（如联系人名称、歌单、技术术语）的能力，通过与现有技术（如Prompting、GEC）对比，LOGIC框架在11种多语言环境下平均将实体词错误率降低9%，且仅带来了0.3%的假警报率增加。

Conclusion: LOGIC能在保证推理效率的前提下，显著提升多语言环境下新实体的识别能力，优于传统Prompting和GEC。

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [8] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: 提出了一种排除偏见的谄媚性评测方法，发现大模型普遍有谄媚和新近偏见，且二者相互增强。不同模型对谄媚导致他人受损场景有不同道德补偿反应。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估LLM谄媚倾向时，往往受到设计缺陷如偏见、噪音、操控语言等问题影响，难以获得客观结果。因此，本文希望设计一种可以排除这些变量、直接且中立地测量谄媚行为的方案。

Method: 提出了一种新的评估大语言模型（LLMs）谄媚倾向的直接且中立的方法，通过使用LLM作为评审，并将谄媚行为在下注场景下视为零和博弈来进行评估。该方法旨在消除先前研究中引入的各种偏差、噪音或操控性语言。

Result: 实验比较了四种主流模型（Gemini 2.5 Pro、ChatGPT-4o、Mistral-Large-Instruct-2411、Claude Sonnet 3.7），发现所有模型在无成本场景下都有明显的谄媚倾向。当谄媚行为对第三方造成损害时，Claude与Mistral表现出“道德悔意”，并出现过度补偿的现象。此外，所有模型都存在对最后提出答案有偏好的“新近偏见”，且谄媚与新近偏见之间存在相互作用，导致在最后表达用户观点时，谄媚倾向被进一步放大。

Conclusion: 本文方法能够更客观地评估大模型谄媚行为，并揭示谄媚与时序偏见的增强效应，为未来模型设计提供新的测评思路和道德约束参考。

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [9] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: 论文提出提升法律AI可靠性的新体系，对比三类系统后发现，经过高级优化的检索增强方法能大幅减少内容虚构，适用于高风险专业场景。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在提升大型语言模型在高风险法律任务中的可靠性，特别是通过减少模型幻觉（生成虚假内容）的问题，解决现有模型在专业法律应用中的准确性与可信度不足。

Method: 作者区分了三种AI范式：（1）独立生成模型（创意神谕）、（2）基础的检索增强系统（专家档案员）、（3）高级端到端优化的检索增强系统（严密档案员）。引入两项可靠性指标：虚假引用率（FCR）和虚构事实率（FFR），对来自12个大模型、75项法律任务共2700份司法型回答进行专家双盲评审。

Result: 独立生成模型FCR超过30%，专业适用性差。基础检索增强系统虽大幅降低了错误，但仍存扎根错误。采用嵌入微调、重排与自纠正的高级检索增强系统可将虚构率降至可忽略水平（FFR小于0.2%）。

Conclusion: 要实现可信的法律AI，需采用聚焦严谨性的检索类体系结构，特别强调证据核查与可追溯性。研究还提供了一套可推广到其他高风险领域的评估框架。

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [10] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: TRACK基准展示LLM对冲突知识推理能力有限，知识更新有时反而加重推理错误，揭示现有知识更新方法在多步推理中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM知识更新方法（如上下文补充与知识编辑）在覆盖参数知识时常导致知识冲突，影响推理，现有基准仅关注单一事实和记忆而忽略对推理效果的评估。需要新方法来评估知识冲突对复杂推理任务的影响。

Method: 提出TRACK基准，测试当知识更新与模型参数知识冲突时，LLM在多步推理中的表现。基准覆盖包含Wiki、Code、Math三种推理密集场景，并引入多种现实冲突。通过对比无新事实与提供新事实的状况，评估LLM的推理能力。

Result: 在TRACK基准上，发现向模型提供更新知识反而可能降低推理性能，且提供信息越多，性能恶化越明显。性能下降既因模型未能有效整合新知识，也因整合后仍产生推理错误。

Conclusion: TRACK为LLM在知识冲突下多步推理设立了新的严格评估标准，揭示知识更新方法的局限，为未来提升LLM推理能力和知识整合方法指明方向。

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [11] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: Transformer提升了情感分析准确率，但存在牺牲中性情感识别和导致情感极化的问题，影响行业可靠性。


<details>
  <summary>Details</summary>
Motivation: 提高情感分析准确率，推动Applied AI Analytics在复杂计算问题中的进步，满足行业需求。

Method: 转移学习与Transformer架构在情感分析中的应用，通过深度学习模型提升情感分类准确率。

Result: Transformer的准确率提升带来了新的副作用，即对于某一类别的情感分类提高后，很可能使另一类别的情感极端化，并导致中性情感类的失效（无法准确识别中性情感）。

Conclusion: 在Applied NLP领域，Transformer虽然带来显著准确率提升，但必须关注其导致的情感极化与中性失效问题，建议未来工作优化模型以维持结果的中立性和多样性，确保应用可靠性。

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [12] [TransportAgents: a multi-agents LLM framework for traffic accident severity prediction](https://arxiv.org/abs/2601.15519)
*Zhichao Yang,Jiashu He,Jinxuan Fan,Cirillo Cinzia*

Main category: cs.AI

TL;DR: TransportAgents通过多智能体协同，各自关注特定信息，融合输出，实现了交通事故严重程度的高精度、稳健预测，并优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 单一LLM在处理异质、领域特定的交通事故数据时易受偏见、预测不稳定。需要提升准确性、鲁棒性与解释性，以更好地支持应急响应和公共安全规划。

Method: 提出TransportAgents：一种混合多智能体框架，将类别特定的大语言模型（LLM）推理与多层感知机（MLP）集成模块结合。每个专用智能体关注特定交通信息子集，输出中间评估，最后通过MLP融合为统一预测。

Result: 在CPSRMS和NEISS两大美国数据集上，TransportAgents在三种主流底座模型（GPT-3.5、GPT-4o、LLaMA-3.3）上全面优于传统机器学习和先进LLM基线，展现出更强稳健性、可扩展性和跨数据集泛化能力。分布分析显示预测更均衡、校准更好。

Conclusion: TransportAgents提升了交通事故严重性预测的准确性、稳定性和解释性，对安全关键型决策支持具有实际应用价值，并为多智能体LLM集成提供新思路。

Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.

</details>


### [13] [From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models](https://arxiv.org/abs/2601.15533)
*Zhikang Chen,Tingting Zhu*

Main category: cs.AI

TL;DR: 高保真世界模型并不一定理解物理或因果动态，结构化、约束感知和因果推理能力比视觉逼真更重要。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在视觉生成方面的进步掩盖了其在物理、因果理解及安全关键场景中的不足，需要建立更可靠、更具实际应用价值的世界模型。

Method: 采用综述和压力测试（以医学决策为例），结合实际应用分析世界模型的不足与需求，提出以结构化、约束感知和因果推理为核心的新框架。

Result: 论文指出当前世界模型（World Model）虽然在高保真视频生成上表现优异，但对物理和因果动态的理解不足，经常在不变性约束、干预及安全关键决策条件下失效。视觉逼真度并不能代表真实理解世界，只有能表达因果结构、尊重约束、并且在长时序下保持稳定性的模型才是有效的世界模型。作者以医学决策为压力测试场景，展示了模型必须能够支持反事实推断和干预规划，而不只是在视觉效果上"逼真"。

Conclusion: 有效的世界模型应作为可操作模拟器，具备结构化因果推理、约束感知能力和长时序稳定性。视觉真实感不能等同于真正理解世界。

Abstract: A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.

</details>


### [14] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: 该论文提出了覆盖46种认知范式的新基准CogToM，并系统评估了22个大语言模型，揭示了模型认知能力的异质性和与人类认知机制的不同，为进一步探索LLM认知边界提供了重要工具。


<details>
  <summary>Details</summary>
Motivation: 现有LLM认知能力评测仅限于狭窄范式（如错信任务），无法全面反映人类理论认知机制。因此，亟需更广泛、更系统的评测工具以揭示与人类认知能力的异同及模型瓶颈。

Method: 提出了包含46种理论范式、8000余双语实例的综合新基准CogToM，并用49名人工标注进行验证；系统评估了22种代表性模型在该基准上的表现，并结合人类认知模式进行了深入分析。

Result: 前沿LLM（如GPT-5.1、Qwen3-Max）在不同认知任务上的表现存在重大差异，部分维度仍有显著瓶颈。人类认知结构与LLM在某些模式下存在潜在分歧，验证了CogToM的理论广度和实用性。

Conclusion: CogToM不仅揭示了LLM在认知理论任务上的能力异质性和瓶颈，还表明LLM的认知机制可能与人类存在差异，为后续研究LLM的认知极限提供了工具和视角。

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [15] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: 本研究设计出结合神经科学信号与机器学习的小型、可解释幻觉检测器，比大规模LLM法官达到更高数据效率和解释性，性能提升显著，适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 当前大模型幻觉检测方法或需极大计算资源（检索外环），或依赖不透明黑盒LLM法官。作者旨在研发小体量、数据效率高、推理极快、解释性强的幻觉检测技术，打破高风险领域应用障碍。

Method: 提出混合检测框架：结合预测编码（量化与内部先验的惊讶度）与信息瓶颈理论（扰动下信号保留），设计可解释特征。经过系统消融，加入实体聚焦、上下文遵循和可证伪分数，利用监督学习进行训练和性能验证。

Result: 使用HaluBench评测，理论基线AUC为0.8017，基础监督模型AUC 0.8274，增强特征下AUC提升至0.8669，且训练数据和推理成本远低于主流方法。模型解释性好，但发现“合理化”信号无法区分幻觉，指向LLM自洽但可能错误的推理。

Conclusion: 以神经科学启发的信号架构引导的混合检测框架能在极小模型（<1M参数）下高效且可解释地检测大模型幻觉，表现优于仅靠大规模LLM裁判的扩展路径。基于信号的领域知识设计比单纯模型扩展具有更高数据效率、加速推理速度并适合生产部署。

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [16] [Improving Methodologies for LLM Evaluations Across Global Languages](https://arxiv.org/abs/2601.15706)
*Akriti Vij,Benjamin Chua,Darshini Ramiah,En Qi Ng,Mahran Morsidi,Naga Nikshith Gangarapu,Sharmini Johnson,Vanessa Wilfred,Vikneswaran Kumaran,Wan Sie Lee,Wenzhuo Yang,Yongsen Zheng,Bill Black,Boming Xia,Frank Sun,Hao Zhang,Qinghua Lu,Suyu Ma,Yue Liu,Chi-kiu Lo,Fatemeh Azadi,Isar Nejadgholi,Sowmya Vajjala,Agnes Delaborde,Nicolas Rolin,Tom Seimandi,Akiko Murakami,Haruto Ishi,Satoshi Sekine,Takayuki Semitsu,Tasuku Sasaki,Angela Kinuthia,Jean Wangari,Michael Michie,Stephanie Kasaon,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim,Daisy Newbold-Harrop,Jessica Wang,Mahmoud Ghanem,Vy Hong*

Main category: cs.AI

TL;DR: 该论文研究了前沿AI模型在全球多语言和多文化环境下的安全性与可靠性，通过多国联合的多语言安全评估发现，AI模型的安全表现因语言、伤害类别以及评估者（AI或人类）而异，提出改进多语种安全测试的建议，为建立全球统一AI安全测试框架奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着全球AI模型的广泛部署，如何确保其在不同语言和文化语境下保持安全与可靠成为重大挑战。鉴于现有安全机制主要以英文及主流语言为主，作者希望评估和提升模型在多语言环境下的安全保障能力，推动AI跨文化、多语言安全标准化和评估方法发展。

Method: 作者联合多国团队，选取两个开放权重大模型，在包括粤语、英语、法语、日语、韩语、斯瓦希里语、马来语、普通话、泰卢固语、波斯语等十种语言中，设计并翻译6000多个全新测试提示，涵盖五大危害类别（隐私、非暴力犯罪、暴力犯罪、知识产权和越狱能力），通过模型自评和人工标注两种方式对安全性表现进行评估比对。

Result: 实验发现AI模型在多语言、多类别伤害下表现出安全性差异，部分语言或类别下防护方案更弱，评估者可靠性也有显著差异。工作还总结了多语种安全评价的具体方法改进建议，如优化翻译的文化背景适应性、增强测试提示的压力测试效果和细化人工标注指导等。

Conclusion: 论文得出结论：AI模型的安全性在不同语言和伤害类别下存在显著差异，评估者之间的可靠性也有所不同。作者建议针对文化和语言背景优化翻译和标注流程，推动建立全球多语言AI安全测试框架，并呼吁业界和学术界持续合作。

Abstract: As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.
  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

</details>


### [17] [Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity](https://arxiv.org/abs/2601.15728)
*Hangle Hu,Chenyu Hou,Bin Cao,Ruizhe Li*

Main category: cs.AI

TL;DR: 本文提出了BIRD-Python基准，厘清了Text-to-SQL与Text-to-Python之间的本质差异，并通过引入领域知识补全机制，使Text-to-Python在数据检索任务上可达到与Text-to-SQL媲美的表现。


<details>
  <summary>Details</summary>
Motivation: 数据库交互主要依赖Text-to-SQL，但实际分析需求越来越强调Python、Pandas等通用编程语言来处理文件数据和复杂分析流程，然而Text-to-Python的可靠性研究远不及SQL。

Method: 优化数据集标注和执行语义，构建BIRD-Python基准，并提出逻辑补全框架（LCF）以提升代码生成的领域知识适配能力，通过实验分析不同范式下的表现差异。

Result: 提出了BIRD-Python基准，系统性优化了原始数据集以减少标注噪声，并确保语义一致性，实现了范式间的公平对比。实验证明Text-to-Python在补足领域知识后可与Text-to-SQL性能匹配。

Conclusion: Python可成为分析型智能体的有效基础，前提是系统能够充分将模糊自然语言输入映射为可执行的逻辑规范。

Abstract: While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.

</details>


### [18] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 首次扩展形式证明系统到物理领域，提出PhysLeanData数据集和PhysProver模型，通过强化学习验证实现卓越性能，显著提升物理及数学定理证明效果。


<details>
  <summary>Details</summary>
Motivation: 在形式化语言和LLM结合推动数学定理证明领域取得进展的背景下，物理领域的形式化推理却鲜有关注，物理问题同样依赖类似的形式化和定理证明框架，急需针对性的突破。

Method: 首创针对物理定理证明的正式推理方法，包括专属数据集（PhysLeanData）的构建和结合RLVR训练方法开发的PhysProver模型。

Result: PhysProver仅用约5,000样本训练，在多个物理子领域提升了2.4%，在MiniF2F-Test数学基准上也提升了1.3%，显示物理外推和对数学证明能力的提升。

Conclusion: 方法不仅显著提升物理领域定理证明效率和效果，还拓展到数学领域，展示了扩展形式证明系统的潜力和泛化能力。作者将开放数据集和模型，推动研究发展。

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [19] [Tabular Incremental Inference](https://arxiv.org/abs/2601.15751)
*Xinda Chen,Xing Zhen,Hanyu Zhang,Weimin Tan,Bo Yan*

Main category: cs.AI

TL;DR: 针对现实中表格列动态变化的问题，本文提出表格增量推理（TabII）任务与方法，理论与实验均证明该方向的有效性和前沿性，为AI高效处理动态表格结构提供了新思路和工具。


<details>
  <summary>Details</summary>
Motivation: 表格数据是基础的数据结构，但随着技术发展和需求变化，表格结构（如列）可能动态变化。传统AI表格模型假设列结构固定，不能直接适应这种动态变化，限制了AI在实际复杂场景下的应用。针对这一痛点，亟需开发新方法支持动态表格的无监督建模与推理。

Method: 提出了“表格增量推理Tabular Incremental Inference（TabII）”新任务，允许模型在推理阶段动态引入新列。理论上将TabII建模为信息瓶颈优化问题，即最小化输入表和表征的信息互信息，同时最大化表征与任务标签间的互信息。方法上结合大语言模型（LLM）填充、预训练TabAdapter引入外部知识，并通过增量示例凝练模块压缩与新列相关的任务信息。

Result: 在八个公开数据集上实验证明，TabII可以有效利用增量属性（新列），在动态表格推理任务上取得了最优（state-of-the-art）表现。

Conclusion: TabII为研究动态结构表格的AI模型开辟了新方向。本文的理论框架与方法不仅改善了动态表格推理能力，也为相关领域提供了算法和策略参考。

Abstract: Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.

</details>


### [20] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC 用单条专家轨迹即可高效学习，比前人方法更快、更稳定，且现实机器人实验表现良好，显示低成本部署潜力。


<details>
  <summary>Details</summary>
Motivation: 现有现实世界强化学习存在样本效率低、奖励稀疏、视觉观测噪声大等问题。已有方法依赖大量数据、人类反馈或大规模预训练，成本高、稳定性差。作者希望开发一种低成本、数据需求少的现实世界强化学习方法。

Method: 提出 SigEnt-SAC，一种带有 sigmoid 有界熵项的离线-在线融合的 actor-critic 强化学习方法，利用单条专家轨迹进行学习。通过在熵项上使用 sigmoid 限制，避免了优化过程中负熵导致离群动作，并减少了 Q 函数震荡。

Result: 在 D4RL 基准任务上，相比主流方法，SigEnt-SAC 显著缓解 Q 函数震荡并更快达成 100% 成功率。在四种包含稀疏奖励和原始图像输入的真实机器人任务上，SigEnt-SAC 用极少的现实世界交互成功学习出策略，验证了其实用性。

Conclusion: SigEnt-SAC 在非常有限的数据条件下实现了有效的强化学习，明显减小了 Q 函数震荡，并极大提高了学习速度和成功率，在现实机器人任务中展现出实际应用前景，为低成本强化学习部署提供了新路径。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [21] [Creativity in the Age of AI: Rethinking the Role of Intentional Agency](https://arxiv.org/abs/2601.15797)
*James S. Pearson,Matthew J. Dennis,Marc Cheong*

Main category: cs.AI

TL;DR: 生成式AI的发展削弱了“创造力需有意向性主体”的观点，作者建议以“可靠生成新奇有价值产出”为新标准，但某些领域仍需IAC。


<details>
  <summary>Details</summary>
Motivation: 生成式AI被人们赋予创造力称谓，挑战了传统上创造力需依赖主体意向的观念，促使作者重新审视并修正相关条件。

Method: 文本分析（语料库证据）、概念工程（对定义及其社会功能的分析）结合论证。

Result: 本文主张在一般情况下应拒绝“意向性主体条件（IAC）”，即创造力必须基于有意向的主体这一观点，理由包括生成式AI的发展已使IAC描述和功能上都变得不适用。作者提出新的“相容性要求”，即创造力应与可靠地产生新颖且有价值产出相关联。但作者也承认在特定领域IAC仍有价值。

Conclusion: IAC不应再作为创造力的普遍条件，而应以产出新颖且有价值产品的能力取而代之，在特定领域可适用IAC。

Abstract: Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.

</details>


### [22] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: 该文提出了融合可穿戴设备与大语言模型的慢性病管理系统VitalDiagnosis，通过智能分析与个性化互动，实现更主动、高效的疾病管理，有望提升患者依从性并缓解医疗工作压力。


<details>
  <summary>Details</summary>
Motivation: 慢性病已成为全球死亡主因，医疗资源紧张与人口老龄化加剧了管理压力，患者个人难以判断病情变化或持续执行护理计划，亟需高效、主动的数字化管理方案。

Method: 通过将可穿戴设备连续健康数据与大语言模型的推理能力整合，系统实时分析健康异常和依从性，采用上下文感知问询，生成临时见解，并在患者-医生合作流程中提供个性化指导。

Result: 验证了VitalDiagnosis能显著推动慢性病管理从被动监测向主动、交互式参与转变，提高患者自我管理能力并减少不必要的临床负担。

Conclusion: 提出并验证了一种利用大模型（LLM）和可穿戴设备数据的慢性病管理生态系统VitalDiagnosis，有望提升患者自我管理和优化医患协作流程。

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [23] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: 提出了一种推理时自我进化的新范式，通过输出验证和细致评分标准实现智能体动态自我提升，DeepVerifier模块有效提升自动知识发现与问题求解智能体的表现。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要依赖后训练方法提升策略智能体能力，缺乏推理时能动态自我优化机制，因此提出基于输出验证迭代改进的新范式以实现自我进化。

Method: 提出了一种自进化范式，让智能体在推理时通过基于细致评价标准的输出验证，实现自我改进。设计了DRA失败分类体系并据此生成评分标准，开发了DeepVerifier判别模块，它可在推理时作为模块插件实现反馈与改进，无需额外训练。

Result: DeepVerifier在meta-evaluation F1分数上优于传统agent-as-judge与LLM-judge基线12%-48%；实际推理集成后，在GAIA与XBench-DeepResearch困难子集上实现8%-11%的准确率提升。

Conclusion: 该方法为自动知识发现与问题求解领域提供了新的进化路径，验证机制及开源DeepVerifier-4K数据集促进了开源模型的反思与自我批判能力，有益于构建更强健的智能体。

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [24] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: 本文提出ErrorMap与ErrorAtlas，通过分析大语言模型（LLMs）出错的具体原因，提供更精细的评测和模型调优参考。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs基准测试只能判定模型错误发生的情况，无法解释具体出错原因，此类评测难以为模型优化提供明确方向，因此需要新的方法剖析错误来源。

Method: 作者设计了ErrorMap方法，可对任何模型或数据集应用，系统地追踪和分类LLMs错误根源，并在应用到35个数据集与83个模型后总结出ErrorAtlas——详尽的错误类型本体。

Result: ErrorAtlas系统性总结了多种模型常见但不被充分关注的错误类型（如遗漏细节、问题误解），为学界提供了丰富的模型局限性参考，同时公开了相关代码与本体，计划持续更新。

Conclusion: ErrorMap和ErrorAtlas为模型开发者提供了一个全新且深入的模型错误分析工具，有助于发现并修复模型的隐藏弱点，推动LLMs更有针对性的改进。

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [25] [EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience](https://arxiv.org/abs/2601.15876)
*Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu*

Main category: cs.AI

TL;DR: EvoCUA创新性将数据自生成与策略进化结合，在原生计算机任务智能体领域取得开源新SOTA，显著优于现有模型，展现出优秀的泛化与扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用智能体发展受限于静态数据扩展，依赖被动模仿，难以捕捉长时序复杂因果动态。因此需要突破静态模仿的瓶颈，提升智能体对复杂任务的适应与泛化能力。

Method: 提出EvoCUA模型，将数据生成与策略优化结合，形成自驱进化循环。设计可验证的合成引擎自动生成多样任务与可执行验证器，构建支持大规模异步沙箱任务收集的基础设施。基于丰富的交互轨迹，迭代进化学习策略，通过能力边界判定，强化成功策略、利用失败经验进行错误分析和自我修正。

Result: EvoCUA在OSWorld基准测试中取得56.7%成功率，成为新的开源SOTA，超过OpenCUA-72B（45.0%）及闭源模型UI-TARS-2（53.1%）。多种基础模型在该方法驱动下性能均有提升，验证了该进化范式的泛化能力。

Conclusion: EvoCUA通过自进化学习机制，实现对原生计算机任务能力的大幅提升，并显示鲁棒及可扩展的智能体进化路径。开创性工作推动了多模态AI中原生智能体的发展。

Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.

</details>


### [26] [ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search](https://arxiv.org/abs/2601.15931)
*Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.

</details>


### [27] [Natural Language-Driven Global Mapping of Martian Landforms](https://arxiv.org/abs/2601.15949)
*Yiran Wang,Shuoyuan Wang,Zhaoran Wei,Jiannan Zhao,Zhonghua Yao,Zejian Xie,Songxin Zhang,Jun Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: MarScope通过视觉-语言对齐实现火星地貌的高效、灵活检索，极大提升大规模行星影像探索的开放性和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 当前行星表面分析多依赖自然语言语义描述，而现有轨道成像数据库仅按像素级组织，导致大规模地形探索受限。

Method: 提出了MarScope视觉-语言框架，将行星图像和自然语言文本对齐在统一语义空间，基于20多万组高质量图文对训练，实现无标签、基于自然语言的火星地表快速检索与分析。

Result: MarScope支持全火星范围内任意自然语言查询，5秒响应，F1可达0.978，兼顾形态学分类、过程分析及相似性检索，突破传统僵化分类体系。

Conclusion: MarScope奠定了以自然语言直接驱动大尺度地学发现的新范式，为后续海量空间数据分析和科学任务提供通用、高效工具。

Abstract: Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.

</details>


### [28] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: DT中输入完整RTG序列有冗余，仅需用最新RTG即可。提出的DDT方法简化了输入，提高了性能和效率，且多任务上优于原方法。


<details>
  <summary>Details</summary>
Motivation: 当前的Decision Transformer (DT)在离线强化学习中，利用Return-to-Go (RTG)序列进行条件化建模，但作者发现实际仅最新RTG影响动作预测，因此喂入完整RTG序列存在冗余。

Method: 作者提出DDT，只将观测序列、动作序列送入Transformer，最新RTG单独参与动作预测判断，结构更简单，减少无效信息输入。

Result: 简化后的Decoupled DT（DDT）不仅提升了性能，还降低了计算成本，并在多个离线强化学习任务上显著优于DT，与最新SOTA方法表现相当。

Conclusion: 实验证明，DDT在提高效率的同时，在准确性、性能上均超过了原DT及部分SOTA变体，是离线RL领域值得关注的新方法。

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [29] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: 提出CS-VAR，结合小模型和LLM，用于直播风控，能跨会话高效捕捉风险，结果显著优于现有方法，辅助内容审核。


<details>
  <summary>Details</summary>
Motivation: 直播互动活跃，易滋生诈骗及恶意行为，且风险行为多累积、复现于多个会话，传统检测难以高效识别。需开发兼具实时性和跨会话模式发现能力的风险检测方法。

Method: 提出CS-VAR（跨会话证据增强型风险检测器），采用轻量领域小模型进行快速会话风险推断，通过大语言模型引导训练，结合跨会话行为证据，从而将局部洞察力迁移至小模型。

Result: CS-VAR在业界大规模数据集上离线实验及在线验证均表现优异，能高效识别跨会话风险模式，支持实时部署，并为内容审核提供解读支持。

Conclusion: CS-VAR在直播风控中实现了业界领先的检测性能，并为实际内容审核提供了可解释且局部化的风险信号。

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [30] [Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://arxiv.org/abs/2601.16038)
*Olga Bunkova,Lorenzo Di Fruscia,Sophia Rupprecht,Artur M. Schweidtmann,Marcel J. T. Reinders,Jana M. Weber*

Main category: cs.AI

TL;DR: 本文研究大型语言模型（LLM）在化学合成规划中的反应路径检索，提出将自然语言转为图数据库查询（Text2Cypher）的方法，并评估不同提示策略及自校验机制的有效性。一次性提示且与实例高度相符效果最佳，Checklist式自校正能提升零样本可执行性但对有优良样本时收效有限。


<details>
  <summary>Details</summary>
Motivation: 传统LLM提示方式在化学规划任务上易出现幻觉或用旧建议，需提高检索过程对知识图谱的精准性和实用性，优化自然语言到结构化查询的转换能力。

Method: 以化学反应知识图为基础，将反应路径检索建模为Text2Cypher（自然语言转图查询）任务，设定单步与多步检索，比较零样本与一例提示（静态、随机、嵌入对齐选例），并引入基于清单的自校验环路，评估查询有效性与检索准确率。

Result: 一例提示且实例与当前查询高度相关时准确率最高，自校验主要在无实例情况下提升查询可执行性。一旦有合适实例，自校验带来的检索增益有限。

Conclusion: 针对化学合成规划的反应路径检索，最佳方法为一例提示且实例与问题高度相关，自校验机制主要在零样本下提升可执行性，但对准确检索贡献有限。

Abstract: Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.

</details>


### [31] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 本文提出将视觉-语言模型与外部信息检索结合，用于更有效地检测气候相关的视觉虚假信息，解决模型无法判断最新事件的难题，提升应对气候虚假信息的能力。


<details>
  <summary>Details</summary>
Motivation: 当前气候虚假信息泛滥，尤其是在社交媒体上的误导性图片和视频，导致应对气候变化的行动受阻。视觉-语言模型（VLMs）虽然能检测视觉虚假信息，但仅依赖训练时的数据，难以跟踪最新动态。

Method: 系统集成VLM与外部信息检索（例如反向图片搜索、在线事实核查、专家权威内容），使模型能利用最新信息推断图像和声明的真实性。

Result: 将VLM与外部知识结合，显著提升了模型判断图像及其相关声明是否真实、误导、虚假或无法验证的能力，更好应对实时的气候虚假信息。

Conclusion: 结合VLM与外部知识检索的方法能改善对气候虚假信息的检测效果，对保护公众科学认知和促进真实信息传播有积极作用。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


### [32] [LLM Prompt Evaluation for Educational Applications](https://arxiv.org/abs/2601.16134)
*Langdon Holmes,Adam Coscia,Scott Crossley,Joon Suh Choi,Wesley Morris*

Main category: cs.AI

TL;DR: 本文提出一种可扩展的、系统性的LLM prompt评估方法，通过对六种不同prompt模板的比较，验证其在教育对话中的效果。结果发现结合persona与context manager模式、支持元认知学习的prompt效果最优。


<details>
  <summary>Details</summary>
Motivation: 现有教育类LLM prompt多靠经验，缺乏系统、循证的优化与评估体系，亟需通用、标准化设计与评估流程以提升个性化和教学契合度。

Method: 设计六种注重不同教学策略的prompt模板，采用锦标赛框架和Glicko2评分系统，在三项教育应用中对120组真实数据进行评判，邀请八位专家从格式、对话支持及适应性维度对prompt对比评分。

Result: 综合比较显示，强调策略阅读并融合persona/context manager设计的prompt在所有评分标准下均明显优胜，两两对比胜率达81%-100%；验证了基于证据的系统评估流程的有效性。

Conclusion: 结合策略阅读、persona与context manager模式的prompt设计在教育对话中表现最佳，显著优于其他设计，方法为教育类LLM prompt系统性优化提供了模板。

Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

</details>


### [33] [Structured Hints for Sample-Efficient Lean Theorem Proving](https://arxiv.org/abs/2601.16172)
*Zachary Burton*

Main category: cs.AI

TL;DR: 轻量固定策略调度显著提升RL训练的定理证明器推理表现，说明推理阶段结构引导仍有补益价值。


<details>
  <summary>Details</summary>
Motivation: 探究经过复杂训练的高性能模型在推理阶段是否还会从简单的结构性指导（即策略调度）中受益。

Method: 结合了大型语言模型与强化学习的神经定理证明器（如DeepSeek-Prover-V1.5），并在推理阶段通过15种常见策略骨架的固定提示调度进行轻量化干预。该干预用于miniF2F基准测试对比模型常规采样表现。

Result: 在相同采样数量（k=16）及生成长度（1024 tokens）下，策略调度干预的通过率（pass@16）为21.7%，高于标准采样的15.2%，即相对提升了43%。

Conclusion: 即使高性能的RL训练定理证明器，推理阶段简单的结构性引导（如策略骨架安排）依然能够有效利用策略语言中的结构先验，实现廉价而显著的性能提升。

Abstract: State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.

</details>


### [34] [Scalable Board Expansion within a General Game System](https://arxiv.org/abs/2601.16216)
*Clémentine Sacré*

Main category: cs.AI

TL;DR: 本文通过基于GGS的动态扩展机制，解决了传统无棋盘游戏过度预设棋盘的问题，提高了开发效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决传统无棋盘类游戏需要预先设定过大静态棋盘，导致浪费资源和不必要复杂度的问题。

Method: 提出并实现了一种基于通用游戏系统（GGS）的自动棋盘扩展机制，使棋盘能够在游戏过程中根据需要动态增长。

Result: 实现了动态扩展棋盘的机制，简化了无棋盘游戏的实现，并优化了资源利用率和系统复杂度。

Conclusion: 通过采用GGS支持的动态棋盘扩展方法，有效提升了无棋盘游戏的实现效率和灵活性。

Abstract: This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [35] [Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals](https://arxiv.org/abs/2601.16091)
*Saar Cohen*

Main category: cs.MA

TL;DR: 本文提出了一种在线非中心聚类算法框架，允许分配决策延迟但需付出延迟代价，优化总距离和延迟损失。在随机到达模型下，给出常数竞争比算法，突破了经典最坏情形的下界。


<details>
  <summary>Details</summary>
Motivation: 经典在线聚类常假设到达需立即决策，且最坏情形下性能难以突破亚对数下界。本文希望通过引入延迟决策和随机到达假设，改善理论性能界并更贴近实际应用。

Method: 提出了一种在线分配算法，允许点的聚类决策延迟但需付出延迟成本。分析了非中心聚类中的延迟权衡，并针对点独立同分布随机到达情形，设计出在期望成本上具常数竞争比的策略。

Result: 在点的空间分布由未知固定概率分布独立抽样、随机到达的案例下，实现了一个在总损失（聚类距离加延迟成本）上常数竞争比的在线非中心聚类算法。

Conclusion: 在随机到达模型下，所提算法实现了常数竞争比，即实际期望成本与最优离线方案的比值被限制为常数。

Abstract: Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [36] [NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases](https://arxiv.org/abs/2601.15758)
*Xieyang Wang,Mengyi Liu,Weijia Yi,Jianqiu Xu,Raymond Chi-Wing Wong*

Main category: cs.DB

TL;DR: 本文介绍了一种名为NL4ST的交互式工具，使非专业用户能够通过自然语言在时空数据库中进行查询。该系统具有三层架构，包括知识准备、实体链接以及物理计划生成，并在真实和合成数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前移动设备和定位技术的发展导致时空数据爆炸增长，但传统查询方式复杂，需要专业知识，限制了非专家用户的访问需求。因此亟需自然语言查询支持以提升数据库易用性。

Method: NL4ST采用三层架构：1）知识库与语料库用于知识准备；2）自然语言理解进行实体链接；3）生成数据库的物理查询计划。系统通过与用户交互，将自然语言输入转化为数据库可执行的查询。

Result: 在四个真实与合成数据集上的实验展示，NL4ST能够高效支持时空查询的自然语言交互，并顺利生成可执行的物理计划。

Conclusion: NL4ST能够有效地将自然语言查询转换为时空数据库的物理执行计划，降低了用户门槛，提高了系统易用性。

Abstract: The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.

</details>


### [37] [EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery](https://arxiv.org/abs/2601.16025)
*Yajuan Xu,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: 本文提出了EAIFD算法，显著提升了关系数据库在增量式函数依赖发现中的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于现有静态算法需全量重执行、增量算法则有明显性能与内存瓶颈，亟需更高效且可扩展的方法满足数据库增量更新下的函数依赖发现需求。

Method: EAIFD算法核心包括：1）维护差异集的部分超图，并将增量FD发现转化为对超图的最小击中集枚举，避免全量重计算；2）多属性哈希表（MHT）用于高频有效FD键值映射，内存消耗与数据集大小无关；3）两步验证策略，利用MHT先缩减候选验证空间，再分批加载数据块实现高效验证，减少重复I/O。

Result: EAIFD显著提升增量FD发现效率，实验显示比现有算法运行时间提高至数量级，内存消耗下降超两数量级，展示了方法的优越性能与应用前景。

Conclusion: EAIFD在真实数据集上的实验结果表现优异，运行速度较现有算法提高至数量级水平，同时内存使用量降低两数量级，证明其高效且易扩展。

Abstract: Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.

</details>
