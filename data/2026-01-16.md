<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 本文构建了针对AI存在性风险的通用分析框架，将风险归因为两条前提，并设计生存故事分类法，通过分析前提失效场景探讨人类幸存条件，给出了生存概率估算。


<details>
  <summary>Details</summary>
Motivation: 回应关于AI存在性风险的争论，厘清AI带来的潜在生存威胁，并以系统性的理论分析帮助更好理解风险应对路径。

Method: 构建AI生存风险的通用框架，并基于两条前提论证AI的生存威胁，进一步提出生存故事分类法并进行情境分析。

Result: 提出了AI生存风险的几类生存故事，每类故事中至少有一个关键前提失效，从而人类幸存；详细分析了各类生存故事所面临的不同挑战和激励的应对方式，并据此框架给出人类被AI毁灭概率的粗略估计。

Conclusion: AI存在性风险可归纳为两个关键前提，若至少一个前提不成立，则人类可幸存。不同生存故事要求不同的技术、政策或价值观应对。本文框架为评估和应对AI风险提供了理论基础和决策参考。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [2] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 参与者会因同伴使用LLM提升工作效率而实施经济惩罚，说明LLM使用虽提升效率，但需付出社会惩罚代价。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，人们开始关注其引发的社会反应。尽管已有研究记录了人们对AI用户的负面态度，但尚不清楚这种不满是否会转化为实际的付诸行动。

Method: 采用两阶段在线实验：第一阶段参与者作为目标，完成任务时选择是否用LLM协助；第二阶段491名参与者花费自身奖金来减少他人奖金，检验对LLM使用者的惩罚行为。

Result: 实验表明，参与者会自掏腰包惩罚依赖LLM完成任务的同伴，被惩罚的程度与实际LLM使用量单调相关。通过自报告“未用LLM”但实际上未用者比实际未用者受到更严重惩罚，反映出披露存在可信度缺口。而高强度真实使用者相较自报者会受到更严重惩罚。

Conclusion: LLMs带来的效率提升伴随社会惩罚成本，这是首次通过行为实验证据确认这种代价的存在。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [3] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 本文提出一种端到端、非交互式的逻辑推理框架，通过引入结构信息和设计AAI方法显著提升了大语言模型的逻辑推理能力，无需依赖外部资源。


<details>
  <summary>Details</summary>
Motivation: 当前使用大语言模型（LLMs）进行逻辑推理主要依赖复杂的交互式框架或混合方法，需要分解推理流程或调用外部资源（如符号求解器），这些方式带来了效率和可扩展性的问题。因此，提出一种无需额外资源的端到端推理框架，以提升模型泛化能力和可解析性。

Method: 通过few-shot prompt引入结构信息，在推理过程中利用注意力头的逻辑模式设计AAI（推理时干预，重权部分注意力头），引导模型重点利用逻辑相关注意力，提升推理效果。

Result: 提出的Attention-Aware Intervention（AAI）方法能有效提升逻辑推理性能，适用于多种基准任务和模型架构，计算开销极小。

Conclusion: 引入结构化的提示和AAI方法能够激活与逻辑运算相关的注意力头，有效引导模型利用先验知识，提升逻辑推理性能且开销较低，具备良好的通用性和可扩展性。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [4] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek在不增加训练的情况下，优化了模型在多轮推理时的稳定性和准确率，且计算与存储效率高。


<details>
  <summary>Details</summary>
Motivation: 受现有序贯推理缩放方法启发，试图解决随着推理步数增加，模型准确率下降与不稳定现象，并减少对推理长度的依赖。

Method: Min-Seek引入了在推理过程中KV缓存（仅多保存一个thought的KV对），结合自定义KV缓存（去除位置信息），动态编码实现超长推理，确保线性计算复杂度。

Result: 提出Min-Seek方法，大幅提升大模型推理准确性，实现推理过程的稳定性并免去推理长度微调需求。

Conclusion: Min-Seek能有效改善序贯推理中准确率下降和模型不稳定的问题，对多种推理任务均有提升。同时该方法高效，用较低资源超越原限长，且复杂度线性。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [5] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 该文反思了人机互补性的理论挑战，提出通过知识论与可靠性视角重新定义其作用，将其价值定位于支持可靠决策而非单纯提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 当前人机协作领域主张'互补性'，即人类加AI的团队决策优于单一主体。然而，互补性理论缺乏精确定义、仅作为事后指标、忽视其他人机交互需求并未考虑‘性能收益的成本’等重要挑战，导致其实证应用存在困难。

Method: 作者采用理论分析，结合知识论（特别是计算可依赖性理论），以案例举证与理论阐释论证其对互补性理解的重构。

Result: 作者以知识论为基础，提出将互补性纳入'证成性AI'讨论框架，主张将其作为一种团队预测任务可靠性的证据，并结合其他可靠性指标来共同校准人机团队决策过程的可靠性。

Conclusion: 互补性的核心意义不在于简单的准确性提升，而在于帮助人们校准和信任由AI支持的决策流程，提高其在实际社会中的可采纳性和可靠性。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [6] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 本文提出了一种无需预定义工作流的信息流协调多智能体范式，显著提升了任务表现（准确率提升8.49%），在复杂任务与异常场景中表现更灵活与鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统主要依赖预定义工作流，但这种设计本质上是基于规则的决策树，存在高度人工成本且无法覆盖复杂任务全部状态空间。

Method: 提出了一种以信息流为核心的多智能体系统范式，采用CORAL框架下的Agent-to-Agent (A2A) 通信，通过信息流协调器动态监控任务进度并以自然语言协调智能体，无需依赖预定义工作流。

Result: 在通用基准GAIA上，提出方法的准确率达到63.64%，比基准OWL的55.15%高8.49个百分点，令牌消耗基本相当。案例分析表明该范式在任务监控灵活性和边界情况处理上更具鲁棒性。

Conclusion: 信息流驱动的多智能体系统能够减少人工工作流设计成本，提高在复杂任务环境中的通用性和鲁棒性，比传统基于规则方法表现更优。

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [7] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: 本文提出了 MolGen 框架，分两个阶段，通过片段级检索和强化学习优化，有效提升了多属性约束下分子生成的精度和可控性，超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 在分子设计领域，生成具备多个理化属性精准数值约束的分子是关键但极具挑战性的任务。传统LLM虽能描述分子，但难以精准实现多目标控制和数值推理。

Method: 提出了 MolGen 两阶段框架：第一阶段通过多智能体片段级检索生成接近可行域的候选分子；第二阶段采用基于 Group Relative Policy Optimization (GRPO) 的片段级强化学习，在控制编辑复杂度和与原型差异的同时，精细优化分子属性到目标值。构建了自动化大规模推理链数据集，为两阶段提供可控、可复现的监督。

Result: MolGen 框架在指定多属性约束下分子生成任务取得了比主流 LLM 和图算法更高的有效性以及多属性目标精确满足率。

Conclusion: MolGen 能更好地利用片段信息，支持对数值型分子属性目标的精准调控，并在多属性约束下实现高效分子生成，优于传统 LLM 和图生成算法。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [8] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: CMA是一种超越传统RAG记忆机制的新架构，能够动态更新和关联记忆，适合长序任务，并在多项实验中优于RAG。


<details>
  <summary>Details</summary>
Motivation: 目前主流大模型利用RAG进行信息检索，但RAG在记忆机制上存在重大缺陷，如信息只读、无时间连续性，缺乏真实“记忆”能力。

Method: 未披露具体实现，仅规定了架构需求，并通过知识更新、时间关联、联想回忆、语境消歧等任务进行行为验证。

Result: 提出并定义了Continuum Memory Architecture（CMA），一类具有持续状态更新和复杂记忆机制的系统，并通过多种任务证明其行为优势。

Conclusion: CMA是构建长程智能体不可或缺的架构原语，但仍存在延迟、漂移、可解释性等未解决挑战。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [9] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文提出，基于大语言模型（LLMs）的多智能体集体行为研究至关重要，需要新的理论与分析框架，以系统探索其社会影响，作者还提出了四个关键研究方向。


<details>
  <summary>Details</summary>
Motivation: LLM智能体集体行为具有社会影响，其预训练知识和隐性社会先验与情境互动，现有理论工具不足，因此亟需新的研究范式与方法。

Method: 论文采用理论分析和方向性讨论，提出建立互动主义范式来系统研究LLM智能体集体行为，并围绕理论、方法和跨学科对话提出具体建议。

Result: 本文提出了四个关键发展方向，旨在推动LLM集体系统的理论建设、方法创新以及跨领域合作，为其安全和有效应用奠定基础。

Conclusion: 作者强调推动大语言模型集体行为研究，需要建立适应性强的理论基础与跨学科方法，为未来LLM集体的开发和部署提供指导。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [10] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 本论文针对大模型幻觉风险，提出了基于根因闭环的综合管理框架，分层检测与缓解，有效提升生成式AI在高风险领域的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大模型在金融、法律等高风险领域具有巨大潜力，但发生幻觉现象带来严重可靠性隐患。亟需构建系统化、可扩展的幻觉管理方案，以实现生成式AI在严格监管环境下的可信应用。

Method: 提出了一个全方位的幻觉管理操作框架，基于根本原因意识驱动的持续改进周期。通过将幻觉来源分为模型、数据和上下文因素，采用多检测方法并配合分层缓解策略，在闭环架构中进行可靠性提升。框架实际应用于金融数据抽取案例。

Result: 通过分层架构和持续反馈机制，有效提升模型的可靠性和可信度，并通过金融数据抽取实证了框架的效益；提出了系统可扩展的幻觉检测、缓解体系。

Conclusion: 本框架为高风险领域构建可信的生成式AI系统提供了方法论基础，具备系统性、可扩展性和持续优化能力，对监管环境下大模型部署具有重要现实意义。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [11] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: 本文提出了针对中国劳动法的专业法律大型语言模型LabourLawLLM，并构建了涵盖多任务的评测基准LabourLawBench，对模型进行了多维度评估，结果表明其在相关任务上优于通用和已有法律语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前通用大型语言模型在法律细分领域缺乏专业知识和复杂推理能力，难以满足实际法律应用需求，尤其是在劳动法相关场景。为此亟需开发能够应对专业法律任务的定制化语言模型。

Method: 1）针对中国劳动法定制训练大型语言模型LabourLawLLM；2）构建多任务评测基准LabourLawBench，任务包括法律条文引用、知识问答、案例分类、赔偿计算、命名实体识别及法律案例分析；3）结合客观指标与基于GPT-4的主观评分进行模型性能评估。

Result: LabourLawLLM在LabourLawBench涵盖的所有任务中表现均优于GPT-4等通用模型以及现有法律领域的大模型，实验证明其具有更高的专业性和任务适应性。

Conclusion: LabourLawLLM在多项劳动法任务中表现优异，超越了现有的通用和法律特定大型语言模型，表明其在法律AI应用中具备较高的准确性和可靠性。该方法可扩展至其他法律细分领域，有助于提升法律人工智能的应用价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [12] [SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation](https://arxiv.org/abs/2601.09974)
*Seoyeon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: SPRInG能针对用户兴趣漂移进行选择性适应和持续个性化，有效解决偏好动态变化问题，并优于业界主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖静态检索或一次性适应，假定用户兴趣不变，但真实场景用户偏好动态变化，模型需要避免灾难性遗忘并有效应对偏好漂移。标准持续学习法在噪声流下难以区分真实偏好转变与短暂上下文变化。

Method: 提出了SPRInG半参数化框架，通过漂移驱动的选择性适应与回放机制，实现高效的持续个性化。训练期间使用基于似然评分函数筛选高新颖互动，推断时则利用严格相关性门控并通过logit插值融合参数化知识与历史。

Result: 在长文本个性化生成基准上，SPRInG表现优异，明显超越现有基线模型，展现了强鲁棒性和实际适用性。

Conclusion: SPRInG通过识别高新颖互动和保留关键残差，有效避免灾难性遗忘，提升了语言模型在真实动态个性化场景中的表现和适应能力。

Abstract: Personalizing Large Language Models typically relies on static retrieval or one-time adaptation, assuming user preferences remain invariant over time. However, real-world interactions are dynamic, where user interests continuously evolve, posing a challenge for models to adapt to preference drift without catastrophic forgetting. Standard continual learning approaches often struggle in this context, as they indiscriminately update on noisy interaction streams, failing to distinguish genuine preference shifts from transient contexts. To address this, we introduce SPRInG, a novel semi-parametric framework designed for effective continual personalization. During training, SPRInG employs drift-driven selective adaptation, which utilizes a likelihood-based scoring function to identify high-novelty interactions. This allows the model to selectively update the user-specific adapter on drift signals while preserving hard-to-learn residuals in a replay buffer. During inference, we apply strict relevance gating and fuse parametric knowledge with retrieved history via logit interpolation. Experiments on the long-form personalized generation benchmark demonstrate that SPRInG outperforms existing baselines, validating its robustness for real-world continual personalization.

</details>


### [13] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL利用结构化分解与历史纠错记忆，无需训练即可显著提高NL2SQL的准确率和资源利用率，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统主要存在两个问题：一是仅依赖正确的样例进行上下文学习，未充分利用历史错误-修正对的信息；二是测试时的分解策略随意，导致SQL候选几乎一致，降低集成效果，且准确率与效率难以兼得。

Method: 提出了Memo-SQL框架，完全无需训练，采用结构化分解（实体分解、层次分解、原子顺序分解）提升推理多样性，并通过动态记忆存储成功查询与错误-修正对，在推理时利用检索增强提示，将相关历史示例融入上下文，无需微调或借助外部API。

Result: Memo-SQL在BIRD数据集上达到了68.5%的执行准确率，在无需微调的开源方法中创下新纪录，且资源消耗仅为现有TTS方法的十分之一。

Conclusion: Memo-SQL有效解决了NL2SQL系统在自我纠错和分解策略上的不足，兼顾准确率和效率，为零训练NL2SQL设定了新标杆。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [14] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: 提出首个利用多保真（MF）数据解决弹塑性大变形问题的深度学习方法FilDeep，实现数据数量与精度的兼顾，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 弹塑性大变形在制造业中极其重要，但传统数值方法难以突破精度与计算量的双重瓶颈。深度学习虽有潜力，但受限于高质量大数据的稀缺性。为兼顾数据量和精度，亟需新的数据高效利用策略，驱动了FilDeep框架的提出。

Method: 方法上，FilDeep框架集成了低保真和高保真数据源，通过注意力机制增强的跨保真模块捕捉物理交互，提高模型对复杂大变形问题的识别与泛化能力。采用联合训练策略，有效解决了数据精度和数量的矛盾。

Result: 本论文提出了一种新型深度学习框架FilDeep，用于解决弹塑性固体大变形计算问题。FilDeep利用低保真（低精度、高数量）和高保真（高精度、低数量）数据的联合训练来突破传统深度学习模型在数据量和精度上的权衡难题，特别是在拉伸弯曲这一典型制造问题中验证了其有效性。通过引入融合注意力机制的跨保真模块，FilDeep显著提升了物理长程交互的捕捉能力，并在实验证明中取得了当前最优的准确率和可用性。该方法为工程制造业中的大变形数值建模提供了高效、创新的解决方案。

Conclusion: FilDeep在有代表性的制造场景（如拉伸弯曲）中实现了准确、高效的大变形预测，是当前领域内应用多保真数据的最佳深度学习框架，并展现出良好的工程部署前景。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [15] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 该论文采用OpenRouter平台大规模数据，分析了100万亿tokens的LLM真实交互，发现开源模型、创意类和编程应用受欢迎，且早期用户保留率高，揭示了LLM实际应用的多样性和复杂性。


<details>
  <summary>Details</summary>
Motivation: 随着新一代大语言模型（LLMs）在推理和真实场景应用上取得重大突破，学界和业界对其实际使用情况缺乏系统性的理解。作者试图通过大规模实证分析，揭示LLMs的真实使用模式。

Method: 通过OpenRouter平台收集和分析了超过100万亿tokens的LLM实际交互数据，涵盖多种任务、地域和时间维度，采用统计分析和用户留存分析方法，量化了不同模型和用例的流行度及用户粘性。

Result: 实证结果表明：（1）开源权重模型被广泛采用；（2）创意角色扮演和编程辅助任务的实际受欢迎度高于传统认知；（3）分析发现早期用户的持续使用率显著高于后来的用户，作者称之为“玻璃鞋效应”；（4）用户对LLMs的实际交互复杂且多元。

Conclusion: LLM的实际应用场景远比传统预期复杂多元，对于模型开发者、应用开发者和基础设施提供者具有重要启示，强调以数据驱动的洞见来优化模型设计与部署。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [16] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: MatrixCoT利用矩阵化结构化规划和反馈驱动修正，明显提升LLM在复杂逻辑推理任务上的稳健性与可解释性，无需依赖外部求解器，性能优异。


<details>
  <summary>Details</summary>
Motivation: 随着Web上的知识和语义日益复杂，现有大型语言模型（LLMs）即使采用Chain-of-Thought（CoT）提升了推理能力，但在依赖符号表达与严格演绎规则的逻辑推理任务上仍表现不足。传统的神经符号推理方法因依赖外部求解器存在格式敏感等问题，LLM端到端方法缺失结构化表示和纠错机制。

Method: 提出MatrixCoT，一种基于矩阵结构的CoT框架。具体为：1）规范化和类型化自然语言表达、附带显式引用字段，2）引入矩阵式规划方法以维持步骤间全局关系，计划过程可验证，提升稳健性，3）通过反馈驱动重规划机制约束语义等价性，识别缺漏/缺陷并重写及压缩依赖矩阵。

Result: 在五个逻辑推理基准和五款LLM上实验证明，MatrixCoT在不依赖外部求解器的情况下提升了逻辑推理的稳健性与可解释性，同时保持了有竞争力的性能。

Conclusion: MatrixCoT通过结构化矩阵规划与反馈修正机制，有效提升LLM复杂符号逻辑推理能力，在保持性能的同时增强了输出的稳健性与可解释性。

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [17] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 本论文系统分析LLM在用户行为时序预测中的表现，发现其虽优于简单统计方法，但弱于专用机器学习模型。上下文适量有利，过多反降性能，提示未来应结合统计精度与语言灵活性优化模型设计。


<details>
  <summary>Details</summary>
Motivation: 探究大模型在推断结构化行为数据中的时间规律性——尤其是用户重复行为的时间间隔——方面的能力，这是现有研究较少涉猎的领域。

Method: 构建重复购买场景作为实验背景，将最前沿的LLM与统计模型及专用机器学习模型在零样本设置下进行系统对比，并分析不同上下文信息丰富度对于预测性能的影响。

Result: LLM在定量捕捉行为的时间结构方面劣于专用机器学习模型，但优于轻量级统计基线。适度的上下文有助提高LLM预测能力，但过多的用户层级细节反而使性能下降。

Conclusion: 现有LLM在结构化时序推断方面有限，丰富上下文不总能带来更好推理，未来模型设计需融合统计与语言能力以实现高性能的时序与语义推断。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [18] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 本文提出了一种漂移感知的数据流系统，通过自适应数据生成与管理，显著提升了金融模型在动态市场中的表现。


<details>
  <summary>Details</summary>
Motivation: 在量化金融领域，训练和真实世界表现之间由于概念漂移和分布不稳定性存在明显差距，导致历史数据训练的模型在动态市场中难以泛化，迫切需要提升系统的适应性。

Method: 提出了一种结合机器学习的漂移感知数据流系统，将参数化的数据处理模块（包括单股票变换、多股票混合和数据筛选操作）与使用梯度双层优化的自适应计划调度器相结合。该框架可微分地统一数据增强、课程学习和数据流程管理，并支持可追溯回放与持续数据质量监控。

Result: 在预测和强化学习交易任务的大量实验中，所述框架增强了模型鲁棒性并提升了风险调整后收益。

Conclusion: 该系统为金融数据的自适应管理和学习驱动的流程自动化提供了通用可推广的方法。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [19] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 该文提出利用LLM做长序列离线决策，通过模态对齐提升性能，并且实验结果显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 长期序列决策对于动态环境中的战略优化（如广告实时竞价）至关重要。传统方式多使用强化学习，近年来Transformer模型（如Decision Transformer）提供了新的建模思路。与此同时，大型语言模型（LLMs）在推理与规划任务表现出色，作者受到启发，探索LLMs在长期序列决策中的潜力。

Method: 主要方法包括将轨迹作为独立模态，设计模态对齐机制，将其与任务文本信息融合，在Transformer架构下进行自回归预测，并通过大规模数据、模型与高质量数据提升性能。

Result: 提出了DecisionLLM框架，将轨迹数据作为一种新模态，并与自然语言任务描述对齐学习，在多个离线实验与竞价场景中取得了强性能。DecisionLLM-3B在Maze2D umaze-v1任务上优于传统DT 69.4分，在AuctionNet上提升0.085分。

Conclusion: DecisionLLM框架有效提升了长序列离线决策的性能，验证了LLM在该类任务中的可行性和优势，并为在线竞价及更广泛AIGB领域探索提供了新方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [20] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源、容器化医学影像AI模型平台，统一模型接口与格式，强化可复现性与易用性，已支持多种主流模型，能直接对比使用，极大简化了人工智能在医学影像领域的实际应用流程。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能在医学影像自动分析和临床研究上具备巨大潜力，但由于AI实现方式和架构多样、文档不一致及可复现性差，限制了研究和临床应用的推广。亟需有统一标准的平台，提升AI模型的可访问性和可复现性。

Method: 本文提出MHub.ai开源容器平台，将同行评审期刊中的AI模型标准化打包，支持DICOM等格式直接处理，并嵌入结构化元数据和统一接口。同时配备公共参考数据用于验证模型。平台初步集成了多种SOTA分割、预测和特征提取模型，模块化架构方便模型适配与社区贡献，示例展示了在肺部分割模型上的比较评估。提供公开可交互仪表板和分割结果，促进结果透明和可复现。

Result: MHub.ai通过标准化容器减少模型配置与使用复杂度，实现模型间的对比评估、标准化输出和低门槛临床转化。平台的开放性和可扩展性支撑多模型场景下的透明和可复现实验。

Conclusion: MHub.ai提升了医学影像AI模型的标准化、可访问性与可复现性，为AI模型的研究、临床验证和实际应用降低了门槛。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [21] [MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning](https://arxiv.org/abs/2601.10157)
*Yusong Wang,Jialun Shen,Zhihao Wu,Yicheng Xu,Shiyin Tan,Mingkun Xu,Changshuo Wang,Zixing Song,Prayag Tiwari*

Main category: cs.AI

TL;DR: 提出多视角蛋白质图表示并融合，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅利用单一视角构建蛋白质图，捕获残基相互作用特性不完整，导致蛋白质表示信息缺失。因此需融合多角度信息以提升蛋白质表示的质量。

Method: 从物理、化学、几何三种视角构建蛋白质残基相互作用图，利用专家混合机制（MoE）动态融合各视角特征，包括个体视角表示和多视角协同信息，通过定量分析专家分工和协作。

Result: 该论文提出了一种名为MMPG的蛋白质表示学习框架，能够从物理、化学和几何多个角度构建蛋白质图并通过专家混合方法融合多元图信息。实验结果显示，MMPG在四项蛋白质相关任务上取得了先进性能，超越了现有基于单一视角的GNN方法。

Conclusion: 多视角蛋白质图融合、专家动态分流和特征协同，显著提升了蛋白质任务表现。

Abstract: Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.

</details>


### [22] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 本研究提出一种结合波形失真度量与机器学习分类的下采样评估流程，实验证明优选形状感知下采样可在确保诊断准确性的同时降低运算负担，并为高频时间序列分析提供通用指导。


<details>
  <summary>Details</summary>
Motivation: 针对针电肌图（nEMG）信号在检测神经肌肉疾病过程中，信号的高且多样化的采样率为基于特征的机器学习模型带来了计算挑战，尤其是在近实时分析中。下采样可能缓解该问题，但下采样对诊断信息和分类性能的影响尚不充分研究。

Method: 构建以形状失真度量为核心，集成特征空间分析与机器学习分类结果的评估流程，系统测定不同下采样算法与参数对波形完整性和模型预测性能的影响，并通过三分类实验验证其有效性。

Result: 提出并验证了一套系统评估高频时间序列下采样信息损失的工作流程。结果显示，具备形状感知能力的下采样算法在保留波形特征方面优于标准抽取方法，可在显著降低计算负载的同时，保持诊断信息。多类NMD分类实验展示了该流程可有效识别兼顾信息保留和计算效率的下采样配置。

Conclusion: 所述工作流程可有效指导下采样算法的选取，实现信息保留与计算效率的最佳平衡，推动nEMG和其他高频时间序列数据的实时分析和应用。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [23] [GFM4GA: Graph Foundation Model for Group Anomaly Detection](https://arxiv.org/abs/2601.10193)
*Jiujiu Chen,Weijun Zeng,Shaofeng Hu,Sihong Xie,Hui Xiong*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.

</details>


### [24] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG提出针对企业混合结构数据的检索生成新框架，显著提升了查询表现。


<details>
  <summary>Details</summary>
Motivation: 目前RAG系统处理企业数据时，往往将富结构的表格数据线性化为简单文本，但已有研究证明此做法在数学上存在不足，难以有效表达数据的空间关系。

Method: 采用双重架构：普通文本通过密集检索器处理，表格结构则用Cell-Aware Late Interaction机制，分别守护叙事文本流动性和表格空间关系。

Result: Topo-RAG框架在SEC-25企业数据集上评估，nDCG@10指标在混合查询场景下相比标准线性化方法提升了18.4%。

Conclusion: 理解并保留数据的空间拓扑结构对企业数据检索至关重要，Topo-RAG能够更好地理解信息形态，实现更优检索效果。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [25] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: 提出了TRIM方法，针对多步推理任务，将关键步骤路由至大型模型，其余由小型模型完成，实现显著的成本效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统LLM路由方法将整个问题分配给同一模型，忽略了各推理步骤难度的差异，导致高成本且容易发生级联错误。该研究旨在通过区分关键步骤，提升推理效率与准确率。

Method: TRIM在每一步推理中利用奖励模型识别高风险步骤，并据此决定是否调用大型模型，实现步级路由。路由策略包括阈值法与更复杂的长期准确率-成本权衡算法。

Result: 在MATH-500数据集上，最低门槛策略相较前沿方法提升了5倍成本效率，复杂策略在仅用20%高阶模型资源的情况下匹配了最优模型表现。在AIME等更难任务上可达6倍成本效率提升。

Conclusion: TRIM能在保证高推理准确率的同时，实现大幅成本节约，在多种数学推理任务和基准测试中均有效泛化。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [26] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo创新性地评估LLM几何本征理解能力，结果显示模型对纯几何认知存在明显短板，微调亦难以弥补。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在几何推理任务通常依赖代数方法解决，缺乏对几何本征理解的专项评估，因此亟需一种能检验模型纯空间关系与几何属性编码能力的新基准。

Method: 提出NoReGeo基准，包括2500个覆盖25类且可直接由几何理解解决的问题集，通过测试各模型原生几何认知性能，同时设计消融实验考察微调对几何理解的影响。

Result: 最先进的大语言模型（如GPT-4）在NoReGeo二分类任务上的准确率最高仅为65%；模型即使经过微调，几何理解能力依然未显著提升。

Conclusion: 当今主流LLM难以原生编码空间关系和几何属性，需专门的几何认知训练方法以推动模型能力进步。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [27] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: 本论文提出EAPO，通过引入证据增强奖励机制及奖励-策略共同进化，有效缓解了长上下文推理中证据检索的无监督瓶颈，实验验证性能明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习已提升大模型推理能力，但在长上下文场景下，由于奖励信号稀疏，难以处罚凭运气猜对等非依据证据的行为，导致关键证据检索阶段无人监管。为此，作者针对证据检索质量提出了密集且精准的过程监督方案。

Method: 提出了一种称为EAPO（证据增强策略优化）的新型强化学习优化方法，用于提升大型语言模型在长上下文推理任务中的证据检索和推理能力。EAPO包含证据增强推理范式、树结构证据采样验证、基于组相对证据的奖励函数设计，以及奖励-策略自适应协进机制。

Result: 在八项长上下文推理基准测试中，EAPO均能显著提升证据检索和推理表现，优于当前主流基线方法。

Conclusion: EAPO对长上下文推理任务提供了有效的过程奖励信号和训练监督，解决了强化学习中奖励稀疏带来的证据检索弱监管问题，实现了检索和推理能力的双提升，具有广阔应用前景。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [28] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: 本文提出了C-GRASP系统，通过八步可追溯推理流程及个体化基线权重机制，有效提升了大型语言模型解释心率变异性的可靠性，显著增强了情绪分类性能，减少了生理数据的幻觉和群体偏差。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在HRV解释方面常受生理学幻觉影响，包括RSA污染、非线性指标不稳定及群体基线偏倚，亟需增强个体化和推理过程透明度，提高临床应用安全性。

Method: 采用RAG增强管道，将HRV解释分解为八个推理步骤，并结合Z分数优先层级和RSA防护机制。通过个体化基线而非群体均值进行权重赋值，同时利用自动化RSA监测减少频域污染。用DREAMER数据集验证，并进行消融实验分析关键模块作用。

Result: C-GRASP系统在DREAMER数据集上的四分类情绪识别准确率达37.3%，临床推理一致性评分为69.6%。关键Delta Z-score模块可有效降低人口偏差，并使整个系统更适应临床需求。

Conclusion: C-GRASP系统显著改善了HRV解释的准确性和透明度，并为情感计算领域提供了更安全可靠的AI集成方案。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [29] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: 本文提出LatentRefusal方法，通过探测LLM的隐藏层激活信号，甄别不可回答查询，在保障安全的同时提升准确率，并且效率高、可无缝集成到现有text-to-SQL系统。


<details>
  <summary>Details</summary>
Motivation: 在基于大语言模型（LLM）的text-to-SQL系统中，处理无法回答或定义不明确的用户查询是安全部署的主要障碍，因为这类查询可能导致模型生成错误或具误导性的可执行SQL，甚至违反安全约束。

Method: 将text-to-SQL系统中的安全拒答问题形式化为answerability-gating，并提出LatentRefusal机制，利用Tri-Residual Gated Encoder架构检测模型中间激活，以区分可回答与不可回答的查询，进而决定是否生成SQL输出。

Result: LatentRefusal方法在四个基准数据集上将平均F1提升至88.5%，且对系统仅增加约2毫秒推理延迟，有效提升系统对于不可回答查询的安全性和表现。

Conclusion: LatentRefusal方法结合Tri-Residual Gated Encoder，有效抑制schema噪音并放大不可回答信号，实验证实该机制能够作为text-to-SQL系统的高效安全层，提升安全性且性能开销极小。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [30] [Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering](https://arxiv.org/abs/2601.10402)
*Xinyu Zhu,Yuzhu Cai,Zexi Liu,Bingyang Zheng,Cheng Wang,Rui Ye,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Yuzhi Zhang,Linfeng Zhang,Weinan E,Di Jin,Siheng Chen*

Main category: cs.AI

TL;DR: 该论文提出了ML-Master 2.0，通过层次认知缓存架构，实现了在多天实验周期内的自主科学探索，在行业标准测试中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习工程及科学发现中智能体在长周期、多维度、反馈稀疏环境下无法持续自主探索和有效策略优化的问题。

Method: 采用层次认知缓存（HCC）架构，将短时的执行痕迹动态提炼为持久的知识和跨任务智慧，实现执行细节与长期战略的有效分离，提升智能体在长周期任务中的自主能力。

Result: ML-Master 2.0是一种自主智能体，能够在机器学习工程领域实现超长周期自主探索，即在持续多天或多周的实验过程中保持战略一致性与迭代优化。通过层次化的认知缓存（Hierarchical Cognitive Caching, HCC）架构，有效管理和积累认知经验，从而突破了常规大语言模型在超长时序任务中的上下文瓶颈，在同类基准下取得了领先性能。

Conclusion: 超长周期自主性是实现AI自主科学探索的关键，架构创新可突破传统模型在长时序任务下的限制。ML-Master 2.0为AI突破人类前例复杂度提供了新范式。

Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.

</details>


### [31] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是针对自动问题生成的显式错误感知评估框架，通过两阶段诊断流程，使评估更贴近人工，更能识别和惩罚关键错误。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成(QG)评估方法多为整体、黑箱，未对具体错误显式建模，容易忽视事实错误及答案不匹配，导致评估结果与实际质量偏差。提出新框架以提升评估细致性和准确性。

Method: 提出ErrEval框架，将QG评估流程分为错误诊断和基于诊断的评分两阶段。首阶段采用轻量级错误识别器对结构、语言及内容相关错误进行分类；然后利用这些诊断信号显式引导LLM评估，提升细粒度和扎实性。

Result: 在三个基准数据集上的大量实验表明，ErrEval能提高与人工评判的一致性，显著缓解对低质量问题的高估现象。

Conclusion: ErrEval通过显式错误诊断与信息化评分显著提升了QG评估的准确性和细粒度，减少了传统方法中对低质量问题的高估，可广泛应用于自动问题生成领域的质量控制。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [32] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: 本文提出了LADFA，一个结合检索增强生成（RAG）和定制知识库的大型语言模型（LLM）隐私政策自动分析框架，可提取并分析个人数据流。经过汽车行业隐私政策案例验证，效果显著。


<details>
  <summary>Details</summary>
Motivation: 隐私政策由于法律术语复杂且行业实践不一致，普通用户难以理解。研究目的在于利用机器学习和自然语言处理技术，实现对隐私政策的自动化与大规模分析，提升个人数据流程的透明性和洞察能力。

Method: 方法包括三大环节：预处理模块对隐私政策文本进行结构化准备；LLM处理模块结合RAG和知识库提取个人数据流；后处理模块构建数据流图并分析其特征。最后通过十份汽车行业隐私政策案例进行实证验证。

Result: LADFA在十份汽车行业隐私政策数据上取得了较高准确率，成功构建并分析了个人数据流图，有效支持了隐私政策的深度理解和合规评估，同时证明了框架的通用性和可定制性。

Conclusion: LADFA框架能够高效、准确地从非结构化隐私政策文本中提取并构建个人数据流图，分析个人数据处理流程，展示了对实际隐私政策分析的有效性，同时框架具备灵活定制能力，可扩展至其他文本分析任务。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [33] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor提出了一种高效的测试时对齐方法，通过token级信号引导和小模型优化，实现了比现有方法更优的性能和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型与人类偏好对齐的方法多依赖于高昂的微调开销或在测试时对齐方案存在性能和多样性损失，亟需高效且能保持生成多样性的对齐方法。

Method: 提出了LLMdoctor框架，结合了基于患者—医生范式的token级奖励提取与token级流引导偏好优化（TFPO），通过小型doctor模型引导冻结的大型patient模型进行细粒度的token级别偏好对齐。

Result: 大量实验表明，LLMdoctor不仅显著优于现有测试时对齐方案，在性能上甚至超过了如DPO等完全微调方法。

Conclusion: LLMdoctor能高效、精确地实现大语言模型测试时对齐，同时保持生成多样性，是优于当前主流方案的创新方法。

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [34] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: 提出了一种用于工业场景的非侵入式神经-符号残差提升框架NSR-Boost，对已有GBDT模型进行补偿强化，无需整体重训练，能够捕捉传统模型遗漏的长尾风险。


<details>
  <summary>Details</summary>
Motivation: GBDT主导了工业表格数据建模，但在高并发生产环境下，传统模型的升级迭代由于高昂的重训练成本和系统性风险面临严重挑战，需要一种低成本、低风险的模型演进方法。

Method: 1）通过残差定位'困难区域'；2）利用大语言模型生成可解释的符号专家结构，并用贝叶斯优化调整参数；3）采用轻量化聚合器动态集成专家与旧模型输出，实现对难点区域的定向修正。

Result: 在金融风险控制场景真实系统上线，通过六个公开数据集和一个私有集的系统实验证明，NSR-Boost不仅全面优于SOTA基线方法，在实际线上数据中也取得了显著效果提升，特别能补足传统模型遗漏的长尾风险。

Conclusion: NSR-Boost显著提升了传统金融风控系统性能，在多个公开和私有数据集上优于现有最佳方法，并可安全、低成本地用于工业模型演进。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [35] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 现有MLLM图表理解基准数据集种类有限，本文发布了覆盖30种图表类型的ChartComplete数据集，助力多样化图表理解研究。


<details>
  <summary>Details</summary>
Motivation: 现有用于多模态大语言模型在图表理解评测的数据集普遍覆盖面狭窄，对于图表类型多样性的需求未满足。因此，亟需一个覆盖种类更广的数据集以推动相关技术进步。

Method: 作者采用编码分类法，基于可视化社区的图表分类体系，收集并整理30类图表图片，构建了ChartComplete数据集，并客观描述其内容而未加入标注或训练信号。

Result: 该论文提出了名为ChartComplete的新数据集，目的是弥补现有多模态大语言模型（MLLMs）在图表理解领域评测数据集图表类型局限的不足。ChartComplete基于可视化领域的图表分类方法，覆盖了30种不同类型图表，内容为已归类的图表图像集合，未包含监督信号。作者将该数据集无附加处理地发布，供社区拓展使用。

Conclusion: ChartComplete填补了现有数据集图表类型单一的不足，为MLLM在图表理解领域的评测和发展提供了更全面、广泛的基准资源。

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [36] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出了一种知识图谱融合任务（DKGF），使用ExeFuse方法将通用知识图谱的相关事实无缝整合到领域知识图谱中，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱信息覆盖有限，通过融合通用知识图谱相关事实丰富领域知识图谱，但面临领域相关性识别和知识粒度不匹配等挑战。

Method: 提出了Fact-as-Program范式（ExeFuse），将通用知识图谱中的事实视为潜在语义程序，通过将抽象关系映射为粒度感知算子，并以程序可执行性验证领域相关性，统一处理相关性和粒度问题。构建了两个DKGF基准和21种评测配置。

Result: 通过两个基准和大量实验，证明该任务的现实意义及所提ExeFuse模型的有效性，并为后续研究提供了标准测试平台。

Conclusion: ExeFuse方法通过统一的概率框架有效解决了领域相关性和粒度匹配问题，实验结果突出其在DKGF任务上的优越表现并提供了该领域的首个标准化基准。

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [37] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 本文提出多层诊断方法，发现LLM的泛化与架构和数据多样性强相关，部分架构（如Mistral）表现出更强泛化能力，为模型失败诊断和可靠AI系统设计提供实证依据。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）微调在特定任务上表现优异，但关于模型脆弱性及泛化失败的诊断仍是难题。本文致力于探究模型泛化失败的成因。

Method: 提出多层次诊断框架，结合SHAP分析和机械解释性方法，对Llama 3.1 8B、Gemma 2 9B、Mistral模型在钓鱼检测任务上的泛化性能进行交叉架构研究。

Result: 1）架构与数据多样性协同促进泛化，Gemma 2 9B在多样化数据集上可达SOTA性能（F1>91%）；2）泛化高度依赖模型架构，Llama 3.1 8B在窄域数据上优异，但不适应多样化数据，表现骤降；3）部分架构（如Mistral）天生更具泛化性，在多种训练范式下表现一致且稳健。

Conclusion: 研究揭示了LLM泛化失败的架构和数据根因，提供具体诊断方法，强调AI系统可靠性需深度验证架构、数据和训练策略的协同作用。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [38] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 多模态大模型能力提升未必带来同步的安全进展，不同模型在多项安全指标下表现差异明显，模型安全性评估需标准化以反映实际风险。


<details>
  <summary>Details</summary>
Motivation: 当前LLM和MLLM在能力上有显著提升，但安全性进展尚未明确，主要由于评测手段碎片化且局限于单一模态或威胁模型。本文旨在提供多模态、系统性的安全评估，揭示当前前沿模型的安全表现和薄弱环节。

Method: 对7个最新模型（GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5）在语言、视觉-语言和图像生成三种设定下，采用统一协议进行安全性评估，包括基准测试、对抗性测试、多语言测试和合规性测试。

Result: GPT-5.2在各项安全评估中表现稳定且均衡；其他模型在基准安全、对抗性、跨语种泛化及合规性间存在明显权衡。所有模型对抗性测试下均显著脆弱，尽管基准测试表现优异。文本-图像模型在监管要求下的视觉风险类别对齐较好，但对对抗性或语义模糊提示敏感，鲁棒性有限。

Conclusion: 安全性是多维且复杂的，受模态、语言、评估手段影响。现有前沿模型在安全性上存在显著不足，尤其是在对抗性和跨语言情境下，要求开发统一的标准化安全评估体系促进负责任的模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [39] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 该工作发现LLM在生成过程中，尽管被攻破，仍存在潜在安全信号；通过表露并利用这些信号实现早期检测，有效提升模型安全性且几乎不损失正常性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在现实应用中的安全性面临挑战，尽管已有安全对齐，但仍易遭受jailbreak攻击，现有防御机制效果有限或影响模型效用。

Method: 分析模型解码过程，发现潜在安全信号，设计机制在解码时显式激活、利用信号，以实现对不安全内容的早期检测。

Result: 提出方法可显著提升模型对jailbreak攻击的安全防护，降低对正常输入的过度拒绝，并保持响应质量。

Conclusion: 在解码过程中激发模型内在的安全识别能力，为防御攻击提供了新思路，与现有方法形成互补。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [40] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent利用多智能体机制在基因组问答任务中优于GeneGPT，并具有更强的灵活性和适应性。


<details>
  <summary>Details</summary>
Motivation: 为克服LLMs在基因组信息问答中对专有数据库访问受限、API依赖死板等瓶颈，推动更高效、灵活的生物医学知识抽取。

Method: 复制GeneGPT后，设计多智能体协调机制，在GeneTuring九项任务上评估表现，通过智能体分工实现高效信息抽取。

Result: GenomAgent框架通过协调多智能体，提升LLMs在复杂基因组信息问答中的能力，在GeneTuring基准任务上平均超过GeneGPT 12%。此外，该框架具备领域迁移能力，适用于其它需要专家知识抽取的科学领域。

Conclusion: GenomAgent多智能体架构有效提升了LLMs在基因组方向复杂问题上的表现，且具备很强的跨领域扩展性。

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [41] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 针对无法同时满足所有 LTLf 性质的情况，本文提出以符号方式高效确定最大可实现目标集的算法，性能明显优越。


<details>
  <summary>Details</summary>
Motivation: 在多性质 LTLf 综合场景下，往往无法全部满足所有目标，因此需要一种高效方法确定可实现的最大目标集。

Method: 采用一次固定点计算方法，通过引入布尔目标变量及利用单调性，将指数级多的目标组合以符号方式紧凑表示，实现多个 LTLf 性质综合。

Result: 提出的符号算法显著优于基于枚举的方法，实际中取得最高达百倍的运行速度提升。

Conclusion: 文中方法有效提升了多性质 LTLf 综合的效率与可扩展性，为实际应用中复杂目标组合的综合问题提供了可靠解决方案。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [42] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: 该论文对分层推理模型（HRM）进行机制分析，发现其推理行为存在“猜测”特征，提出三种策略提升表现，最终显著提升数独任务准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然HRM在多种推理任务上优于大语言模型推理器，但其内部推理机制尚不清晰。研究动机是深入理解HRM的强项及失效模式，同时探索提升其推理能力的有效策略。

Method: 作者通过对HRM在推理任务中的行为进行系统性分析，揭示其推理过程中的失败模式和“猜测”机制，进而提出三种扩展猜测能力的方法：数据增强、输入扰动和模型自举，并在难度较高的数独任务上进行实证验证。

Result: 通过三种策略组合获得增强型HRM，在Sudoku-Extreme任务上准确率从54.5%提升至96.9% ，分析还揭示了推理模型内部由“猜测”主导的本质。

Conclusion: 通过数据增强、输入扰动和模型自举三种策略改进HRM，大幅提升推理准确率，并为推理模型的工作机理提供新见解。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [43] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 本文提出结构化且多样受限的上下文构建办法，相比传统top-k显著减少冗余、更好覆盖多阶信息，全面提升模型問答质量。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型（LLM）采用的基于检索增强生成（RAG）的方法在构建上下文时常面临信息碎片化、内容冗余、信息覆盖不足等问题，特别是在文档的2阶、3阶结构特征无法有效利用。本文动机在于提出更精细、结构感知且多样化受限的上下文构建方法，以提升上下文质量和模型表现。

Method: 本文提出了一种结构感知、多样性受约束的Context Bubble构建框架：基于高相关锚定段落，在限定token预算下，通过多粒度结构（如章节、行）组合片段，结合任务条件下的结构先验，平衡相关性、边际覆盖和冗余，显式约束多样性和长度，获得连贯、可引用的上下文包。引入全检索回溯机制使检索与选择过程可审计、可调优。

Result: 在企业文档实验证明该方法较传统top-k检索显著减少冗余，更好覆盖二级特征，提升答案质量和引用可信度。消融实验显示结构先验和多样性约束缺一不可，移除任何一项都会降低覆盖性、提升冗余或遗漏。

Conclusion: 结构感知、多样性约束的Context Bubble方法能够在token有限的情况下组建更高效、全面且可追溯的上下文，优于传统top-k方法。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [44] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: GenAI能提升新手建筑设计者的表现，但会降低创造性自我效能感，对整体认知负荷无影响，提示方式影响认知负荷表现。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索生成式人工智能（GenAI）在建筑概念设计任务中对表现、创造性自我效能感和认知负荷的影响。

Method: 采集36名建筑及相关专业学生，设置两阶段设计任务：独立完成与外部工具辅助（GenAI或建筑项目资料库），由专家评估成果，自我报告效能感及认知负荷，并利用差异中的差异分析组间效果。

Result: 整体来说，GenAI未提升所有参与者的设计表现，但对新手设计者有显著提升作用。使用GenAI的学生的创造性自我效能感下降，认知负荷在各条件间无显著差异。提示迭代和视觉反馈可有效降低认知负荷。

Conclusion: GenAI的应用效果依赖于用户的先验专业水平及其提示交互策略。对于新手设计师更有效，但需关注自我效能感下降的问题。设计合适的交互策略可优化认知负荷。

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [45] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: Multiverse通过支持并发的版本化和非版本化事务，高效兼容不同工作负载类型，在长读取场景下胜出现有STM，而在常规负载下也无显著性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有STM在高频更新、大范围长读取场景下存在效率问题。多版本化虽能解决长读取，但通常引入高开销，影响普通事务性能。亟需兼顾两者优点的方案。

Method: 提出Multiverse，这是一种结合了无版本STM和多版本STM优点的新型软件事务存储（STM）系统，支持版本化与非版本化事务并发执行。

Result: Multiverse在无长时间读取场景下性能可与最先进的无版本STM媲美，而在长时间读取及频繁更新负载下，性能显著优于现有STM，部分场景中吞吐量提升数个数量级。

Conclusion: Multiverse有效弥补了传统STM系统在长读取场景下的性能短板，同时保证常规工作负载下不牺牲无版本STM的高性能，为STM系统设计提供了新的高效方案。

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [46] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 本研究提出Babel和ORION两种工具，分别解决标识符和数据模型异构带来的科学数据互操作性难题，已实现基于FAIR原则的知识库互操作和开放共享。


<details>
  <summary>Details</summary>
Motivation: 尽管许多科学资源采用了FAIR原则设计并标注，但因标识符模式和数据模型的多样化，实际上的互操作性仍然存在困难。作者旨在解决理论上的互操作性和实际应用中的互操作性之间的鸿沟。

Method: 本文提出了两个工具：Babel 和 ORION。Babel通过整理和映射不同的标识符方案，实现同一实体的多种ID等价聚合，并通过高性能API提供访问；ORION则通过将多个知识库转化为共同、社区管理的数据模型，解决数据模型多样性问题。

Result: Babel解决了标识符多样性，ORION统一了数据模型，两者结合可创建可完全互操作的知识库。目前成果已作为知识库库在 https://robokop.renci.org 上开放下载和使用。

Conclusion: 通过Babel和ORION工具，有效提升了科学数据资源的实际互操作性，为FAIR生态系统构建了高效的数据互联和共享解决方案。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [47] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: SDP算法通过冗余上界剪枝、特征排序、对界收紧，以及优先搜索等优化，大幅提高了FD发现效率，在40+数据集上实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统FD发现方法计算复杂度高、结果集冗余，难以应用于大规模、高维数据。需设计高效、有效遴选有价值FD的新方法。

Method: 1. 冗余计数作为FD排序准则。 2. 引入单调冗余上界剪枝无效分支。 3. 通过特征排序、Partition Cardinality Matrix收紧界限。 4. 全局调度优先遍历潜力分支。 5. 对比实验验证性能。

Result: 提出了SDP（Selective-Discovery-and-Prune）算法，有效发现冗余计数排名前k的函数依赖（FDs），极大提升了在大规模、高维数据上的发现速度与内存效率。

Conclusion: SDP能高效发现最有代表性的FD，实验中比传统方法更快、更省内存，适合实际大数据场景。

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [48] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 通过合并结构相似的事务语句，本文设计并实现了TransactionMerger中间件和静态分析工具，有效提升系统的事务处理性能，在TPC-C与Spree实验中吞吐量最高提升达2.65倍和3.52倍。


<details>
  <summary>Details</summary>
Motivation: 传统事务处理面临性能瓶颈，应用层存在大量结构相似且重复的事务语句，进一步挖掘和优化这些语句的合并潜力可提升整体吞吐效率。

Method: 提出基于结构性分析合并相似事务语句，包括消除冗余读取和合并竞态语句，通过静态分析工具自动识别合并机会，并设计了TransactionMerger中间件具体实现，结合实际场景进行实验验证。

Result: 本文提出了一种通过结构性相似语句或事务合并提升应用端事务处理性能的新方法。通过合并类似语句、消除冗余读取、以及合并事务间竞态语句并预先计算其聚合效应，实现事务重写。设计了TransactionMerger中间件，可跨客户端收集并合并事务，并提供静态分析工具用于识别合并机会且不破坏隔离性。此外，作者在TPC-C与Spree（实际应用场景）中进行了事务重写实验，实验证明合并策略能显著提升系统吞吐量。

Conclusion: 事务合并策略可以显著提高应用侧事务处理性能且不影响隔离性，是优化数据库中间件的有效手段。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [49] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 文中提出并验证了将数学模型自动转化为关系模型及配套约束的高效算法，同时给出了在实际数据库系统中实现约束的代码示例。


<details>
  <summary>Details</summary>
Motivation: 推动从数学数据模型到关系型数据库模型的自动化转换与约束管理，提高智能数据库系统的设计规范与执行效率。

Method: 提出伪代码算法并在原型数据库MatBase中实现，通过具体实例（如家谱树子宇宙）测试其有效性，并给出SQL、VBA代码示例及开发指南以支持约束实现。

Result: 提出了一个高效、健壮、完整且最优的伪代码算法，用于转换数学数据模型为关系型数据模型及其相关约束，并在Genealogical Trees子宇宙建模场景中进行了应用验证。

Conclusion: 算法能高效且完整地支持数据库模型转换与非关系约束管理，对智能数据库系统的自动化和规范化具有重要支持作用，相关编程实现便于实际推广。

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [50] [Multi-Agent Cooperative Learning for Robust Vision-Language Alignment under OOD Concepts](https://arxiv.org/abs/2601.09746)
*Philip Xu,Isabel Wagner,Eerke Boiten*

Main category: cs.MA

TL;DR: MACL多智能体框架可有效提升视觉-语言模型应对OOD概念时的对齐与泛化能力，在极小样本和零样本设置下带来1-5%的精度提升。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉-语言模型在处理分布外概念时因模态不均衡导致的跨模态对齐崩溃，提升其泛化和鲁棒性。

Method: 方法包括多智能体特征空间名称学习、上下文交换增强的极小样本学习算法，以及自适应动态平衡机制调控智能体间贡献，通过结构化消息传递实现智能体协同。

Result: 该论文提出了一种新型多智能体协同学习（MACL）框架，旨在解决视觉-语言模型在处理分布外（OOD）概念时的跨模态对齐崩塌问题。MACL框架包含图像、文本、名称和协调四类智能体，通过结构化消息传递共同缓解模态失衡。

Conclusion: MACL框架在VISTA-Beyond数据集上展现了卓越性能，能在few-shot和zero-shot场景下提升模型精准度，表明多智能体协同机制在跨模态对齐和分布外泛化中的实际有效性。

Abstract: This paper introduces a novel Multi-Agent Cooperative Learning (MACL) framework to address cross-modal alignment collapse in vision-language models when handling out-of-distribution (OOD) concepts. Four core agents, including image, text, name, and coordination agents, collaboratively mitigate modality imbalance through structured message passing. The proposed framework enables multi-agent feature space name learning, incorporates a context exchange enhanced few-shot learning algorithm, and adopts an adaptive dynamic balancing mechanism to regulate inter-agent contributions. Experiments on the VISTA-Beyond dataset demonstrate that MACL significantly improves performance in both few-shot and zero-shot settings, achieving 1-5% precision gains across diverse visual domains.

</details>


### [51] [When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making](https://arxiv.org/abs/2601.10102)
*Viswonathan Manoranjan,Snehalkumar `Neil' S. Gaikwad*

Main category: cs.MA

TL;DR: 模型的角色设置和收益信息展示会影响其在多智能体决策中的推理模式，不同大模型架构对此敏感性存在差异，相关代表性设计选择直接决定了模型行为倾向，对实际应用具有重要治理意义。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多智能体系统中的行为动因（是战略收益最大化还是角色身份一致性）及其对代表性设计（如persona设置、收益信息展示）的敏感性尚不明确。本文旨在厘清大模型在战略任务中究竟更倾向于战略理性还是身份驱动型行为，并探究代表性设计如何影响其推理表现。

Method: 在多智能体环境决策游戏中，对四种大语言模型（Qwen-7B、Qwen-32B、Llama-8B、Mistral-7B）系统性地实验并比较模型在不同角色设定（persona）和收益展示（payoff visibility）条件下的策略推理表现。通过Nash均衡达成率作为策略推理能力的度量指标。

Result: 实验发现，角色身份偏向显著干扰模型的策略推理，即使在信息完全和收益最优化的条件下也如此；移除persona并显式提供收益，Qwen系列模型可高效达到Nash均衡，反之则偏向社会偏好结局。Llama和Mistral模型在不同条件下则表现出推理行为的刚性。代表性设计影响模型是否表现为战略推理者或身份驱动行动者，且不同架构的模型对此敏感性有明显不同。

Conclusion: 大语言模型在多智能体战略任务中的推理模式受角色身份和收益信息展示强烈影响。合理设计persona和收益呈现是确保模型战略合理性的必要前提；治理多智能体系统应视代表性设计为核心环节，且不同模型架构需区别对待。

Abstract: Large language models are increasingly deployed in multi-agent systems for strategic tasks, yet how design choices such as role-based personas and payoff visibility affect reasoning remains poorly understood. We investigate whether multi-agent systems function as strategic reasoners capable of payoff optimization or as identity-driven actors that prioritize role alignment over explicit incentives. Using Nash equilibrium achievement as a diagnostic for strategic reasoning, we conduct systematic experiments across four LLM architectures (Qwen-7B, Qwen-32B, Llama-8B, Mistral-7B) in complex environmental decision-making games involving four agents. We show that role identity bias fundamentally alters strategic reasoning even when payoff-optimal equilibria exist and complete payoff information is available. Removing personas and providing explicit payoffs enables Qwen models to achieve high Nash equilibrium rates, indicating that both conditions are necessary for strategic reasoning. In contrast, personas systematically bias equilibrium selection toward socially preferred outcomes: with personas present, all of the achieved equilibria correspond to Green Transition, while models entirely fail to reach equilibrium when Tragedy of the Commons is payoff-optimal. The effect of explicit payoffs depends entirely on persona presence, revealing strong interactions between representational design choices. We also observe clear model-dependent patterns. Qwen architectures are highly sensitive to both personas and payoff visibility, whereas Llama and Mistral exhibit rigid reasoning behavior across conditions. These findings demonstrate that representational choices are substantive governance decisions that determine whether multi-agent systems act as strategic reasoners or identity-driven actors, with important implications for real-world deployment.

</details>


### [52] [TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems](https://arxiv.org/abs/2601.10120)
*Rui Sun,Jie Ding,Chenghua Gong,Tianjun Gu,Yihang Jiang,Juyuan Zhang,Liming Pan,Linyuan Lü*

Main category: cs.MA

TL;DR: TopoDIM框架通过多样交互模式实现一次性、高效、适应性强的多智能体通信拓扑，无需迭代，token利用率高，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统的通信主要依赖时空交互范式，导致对话多轮顺序执行时延和计算开销大；受评估与辩论机制可提升多智能体系统问题解决能力的启发，需求高效、灵活的通信拓扑生成方法。

Method: 提出TopoDIM框架，实现基于多样交互模式的一次性通信拓扑生成，支持去中心化执行。

Result: TopoDIM在无需迭代协调的情况下实现异构通信、提升token效率和任务表现。实验证明其token消耗减少46.41%，性能提升1.50%，在异构智能体通讯组织上具良好适应性。

Conclusion: TopoDIM有效解决了多智能体系统中通信拓扑构建过程中延迟与计算开销高的问题，提升了任务表现和通信效率，具备良好实用前景。

Abstract: Optimizing communication topology in LLM-based multi-agent system is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, where the sequential execution of multi-round dialogues incurs high latency and computation. Motivated by the recent insights that evaluation and debate mechanisms can improve problem-solving in multi-agent systems, we propose TopoDIM, a framework for one-shot Topology generation with Diverse Interaction Modes. Designed for decentralized execution to enhance adaptability and privacy, TopoDIM enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments demonstrate that TopoDIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods. Moreover, the framework exhibits strong adaptability in organizing communication among heterogeneous agents. Code is available at: https://anonymous.4open.science/r/TopoDIM-8D35/

</details>


### [53] [Fairness Driven Multi-Agent Path Finding Problem](https://arxiv.org/abs/2601.10123)
*Aditi Anand,Dildar Ali,Suman Banerjee*

Main category: cs.MA

TL;DR: 提出了针对不同智能体行为（理性与否）的MAPF问题有效解决方法。


<details>
  <summary>Details</summary>
Motivation: MAPF问题在机器人导航和无人机调度等实际场景中广泛存在，因其高计算复杂度及智能体的理性行为（如信息隐瞒），亟需公平且高效的求解机制。

Method: 结合启发式算法与机制设计方法。启发式方法用于非理性智能体的路径规划，机制设计确保理性智能体在公平及激励一致前提下如实报告信息以获得非冲突路径。

Result: 本文针对多智能体路径规划（MAPF）问题，提出了分别适用于非理性和理性智能体的解决方案。对于非理性智能体，设计了启发式方法；对于理性智能体，提出了一种机制设计模型，并证明具有激励相容性和个体理性。

Conclusion: 所开发的方法与机制在公平性、效率与有效性方面表现优异，为实际中多智能体路径规划提供了理论与实践基础。

Abstract: The Multi-Agent Path Finding (MAPF) problem aims at finding non-conflicting paths for multiple agents from their respective sources to destinations. This problem arises in multiple real-life situations, including robot motion planning and airspace assignment for unmanned aerial vehicle movement. The problem is computationally expensive, and adding to it, the agents are rational and can misreport their private information. In this paper, we study both variants of the problem under the realm of fairness. For the non-rational agents, we propose a heuristic solution for this problem. Considering the agents are rational, we develop a mechanism and demonstrate that it is a dominant strategy, incentive compatible, and individually rational. We employ various solution methodologies to highlight the effectiveness and efficiency of the proposed solution approaches.

</details>


### [54] [Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems](https://arxiv.org/abs/2601.10560)
*Xi Shi,Mengxin Zheng,Qian Lou*

Main category: cs.MA

TL;DR: LAMaS框架通过并行执行和延迟优化，大幅降低多智能体系统关键路径时延，提升效率。


<details>
  <summary>Details</summary>
Motivation: 由于多智能体推理常需多步和多次模型调用造成较高延迟，现有方法多假定顺序执行，未充分优化并行场景下的时延问题。因此，作者旨在提升多智能体系统在并行执行时的效率和可用性。

Method: 该方法设计了基于学习的多智能体编排器，通过延迟监督和拓扑图构建，优化多智能体并行执行过程中的关键路径长度，以实现低延迟高效推理。

Result: 本文提出了一种名为LAMaS的并行执行延迟优化多智能体系统编排框架，通过显式延迟监督以及对关键路径执行优化，实现更低推理时延，在多个基准任务上将关键路径长度降低了38-46%，且任务性能保持或提升。

Conclusion: 显式优化并行执行下的关键路径延迟对于提升多智能体系统的效率至关重要，LAMaS在保证甚至提升任务性能的同时显著降低了延迟。

Abstract: Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS

</details>
