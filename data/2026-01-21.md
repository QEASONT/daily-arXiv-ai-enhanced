<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 12]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Building AI Agents to Improve Job Referral Requests to Strangers](https://arxiv.org/abs/2601.10726)
*Ross Chu,Yuting Huang*

Main category: cs.AI

TL;DR: 本文提出了一套基于AI智能体的系统，旨在协助求职者优化在专业社区中的内推请求文本，并利用效果评估模型对修改结果进行质量预测。引入检索增强生成（RAG）技术后，AI能够显著提升弱内推请求的效果，并避免损害强请求的表现。


<details>
  <summary>Details</summary>
Motivation: 动机在于提升求职者通过线上社区获得内推的成功概率，尤其关注如何利用AI技术自动优化内推请求文本，从而提升内推同意率并为大规模部署提供先验有效性指标。

Method: 方法包括：1）改写型智能体（Improver Agent）使用大语言模型（LLM）对求职内推请求进行改写；2）评估型智能体（Evaluator Agent）通过一个预测实际获得内推概率的训练模型来评估改写质量；3）将LLM与检索增强生成（RAG）结合，以避免对强请求的不良改写，并强化对弱请求的优化。

Result: 结果显示，结合RAG增强的编辑能提升弱请求14%的成功预测率，且不会影响强请求的表现。模型预测结果为大规模实验前的优选特征提供了强信号。

Conclusion: 结合LLM与RAG的修改方式能在不影响强请求的基础上，使弱请求的模型预测成功率提升14%，为大规模真实实验前提供了低成本、高信号的特征选择参考。

Abstract: This paper develops AI agents that help job seekers write effective requests for job referrals in a professional online community. The basic workflow consists of an improver agent that rewrites the referral request and an evaluator agent that measures the quality of revisions using a model trained to predict the probability of receiving referrals from other users. Revisions suggested by the LLM (large language model) increase predicted success rates for weaker requests while reducing them for stronger requests. Enhancing the LLM with Retrieval-Augmented Generation (RAG) prevents edits that worsen stronger requests while it amplifies improvements for weaker requests. Overall, using LLM revisions with RAG increases the predicted success rate for weaker requests by 14\% without degrading performance on stronger requests. Although improvements in model-predicted success do not guarantee more referrals in the real world, they provide low-cost signals for promising features before running higher-stakes experiments on real users.

</details>


### [2] [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)
*Percy Jardine*

Main category: cs.AI

TL;DR: CTHA通过约束层间通信与决策空间，有效改善多时间尺度智能体层次结构的协调问题，显著提升稳定性和任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多时间尺度智能体架构虽然通过引入时间层次结构在性能上取得显著提升，但其协调稳定性受到严重损害，带来层间冲突、错误传播和可扩展性受限等问题。

Method: 通过将层间通信投影到结构化流形，并设定三类关键约束——消息合约、权限流形和仲裁解析，保障层间信息流、决策空间及决策协调的有序性与稳定性。

Result: 提出的CTHA方法显著降低了故障级联（减少47%），提升样本效率（提升2.3倍），在复杂任务上的可扩展性超过无约束层次化基线方法。

Conclusion: CTHA作为针对多时间尺度层次结构的通用方案，通过结构化约束和仲裁机制，深化了多智能体协调理论并为自治系统发展提供新方向。

Abstract: Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.

</details>


### [3] [Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration](https://arxiv.org/abs/2601.10744)
*Sen Wang,Bangwei Liu,Zhenkun Gao,Lizhuang Ma,Xuhong Wang,Yuan Xie,Xin Tan*

Main category: cs.AI

TL;DR: 本文提出了一种新的长期记忆体感探索方法与基准，构建数据集并设计了结合探索与记忆召回的评估体系，实验结果显示方法优于当前主流模型。


<details>
  <summary>Details</summary>
Motivation: 当前主流的一次性体感任务主要关注任务完成结果，忽略探索过程和记忆利用，但复杂长期任务和通用环境下操作需要体感体具备终身学习能力和长时记忆，有效优化决策过程。

Method: 设计了LMEE长期记忆体感探索方法及基准，包括多目标导航和基于记忆的问答任务，并提出MemoryExplorer，通过强化学习微调多模态大模型，引入包括动作预测、前沿选择和问答的多任务奖励机制，实现主动探索与记忆利用。

Result: 所提出的MemoryExplorer方法，在长时序体感任务中，相较于现有最新模型，在主动探索和记忆利用等方面表现出显著优势。

Conclusion: 通过统一探索认知与决策行为，增强体感体主动记忆查询能力，使其能更好实现终身学习，并在复杂长期任务中有更优表现。

Abstract: An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.

</details>


### [4] [Optimisation of complex product innovation processes based on trend models with three-valued logic](https://arxiv.org/abs/2601.10768)
*Nina Bočková,Barbora Volná,Mirko Dohnal*

Main category: cs.AI

TL;DR: 利用简单趋势启发式构建创新过程模型，用转移图展示方案间可能转移，直观表达系统演化路径，无需复杂数值化。


<details>
  <summary>Details</summary>
Motivation: 分析复杂产品创新过程，寻求低信息强度建模方法。

Method: 基于启发式趋势（增长、减少、恒定）建立模型，通过转移图描述场景及其转换。

Result: 提出基于启发式趋势的模型，并以转移图的路径描述系统可能的过去与未来行为。

Conclusion: 趋势模型与场景转移图为创新过程的表达提供简洁有效的方式，适用信息有限场景。

Abstract: This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.

</details>


### [5] [ARC Prize 2025: Technical Report](https://arxiv.org/abs/2601.10904)
*François Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers*

Main category: cs.AI

TL;DR: 本文系统性梳理了ARC-AGI-2大赛与前沿实验室方法，突出细化循环机制的崛起，并指出当前AI推理能力受限于知识覆盖与基准污染，同时预告了引入交互推理任务的ARC-AGI-3。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI-2引入了更高复杂度的任务，且作为衡量AI少样本泛化与抽象推理能力的重要标准，备受学术与工业界关注。随着全球竞赛和参与研究者激增，理解当前最优方法、技术瓶颈及未来发展方向极具价值。

Method: 调研并综述在ARC-AGI-2基准上的最优方法，重点分析细化循环（refinement loop）的创新，包括进化式程序综合和AI系统应用层细化，以及新型零预训练深度学习方法；同时评估前沿AI实验室公开表现，并展望下一版ARC-AGI-3。

Result: 在Kaggle竞赛中最佳成绩为私有测试集24%，迭代细化循环成为主流创新，出现了小规模（700万参数）且无需预训练的深度模型在任务上取得竞争性能。产业界四大前沿实验室将ARC-AGI作为公开标准。分析显示仍存在以知识覆盖为核心的性能限制与基准污染风险。

Conclusion: ARC-AGI已成为AI泛化与推理的行业标准，细化循环和零预训练小模型等创新带来新进展，但现有系统受知识范围限制，基准可信度受挑战。后续通过引入交互、规划与记忆等能力，或将推动AI推理的进一步突破。

Abstract: The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.

</details>


### [6] [What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge](https://arxiv.org/abs/2601.10922)
*Yosub Shin,Michael Buriek,Boris Sobolev,Pavel Bushuyeu,Vikas Kumar,Haoyang Xu,Samuel Watson,Igor Molybog*

Main category: cs.AI

TL;DR: 本论文在固定模型和训练协议下，发现基于难度的样本选择最能提升多模态视觉语言推理性能，扩展数据集规模仅降低结果波动，多样性与数据增强方法无益甚至有害。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解在固定模型训练协议下，数据选择和策划如何影响多模态视觉语言推理任务的性能。

Method: 通过NeurIPS 2025 DCVLR挑战，使用精心策划的紧凑型数据集并结合多组消融实验，评估不同数据策划策略（难度选择、规模扩展、多样性和合成增强）对多模态视觉语言推理的影响。

Result: 难度驱动的样本选择是性能提升的关键。扩展数据集规模不一定提升均值准确率，主要作用是降低训练结果的波动性。多样性和合成数据增强未提升，甚至可能削弱模型表现。

Conclusion: 在固定模型和训练协议下，多模态推理任务的数据集选择具有决定性作用。基于难度的样本选择在提高性能上贡献最大，而增加数据集规模主要是降低波动性，对准确率提升有限。多样性与合成增强方法未带来预期的益处。

Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.

</details>


### [7] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出了一个高效的多智能体系统（MAS）构建方法，通过在任务级别生成少量最佳工作流，并结合自我预测和小样本校准替代全量验证执行，从而显著降低大语言模型驱动的MAS的计算与资源成本，且几乎不损失性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在任务、查询级别生成工作流，但差异与成本未被系统衡量，尤其在token消耗与执行评估方面存在资源瓶颈。作者希望提升效率，降低消耗，同时保持准确性。

Method: 提出SCALE生成框架，自我预测任务级优化结果，并利用小样本校准完成评估，无需耗时耗资源的逐条执行验证；通过实验对比token消耗与性能表现。

Result: SCALE在多个数据集上相较于现有方法token消耗减少83%，性能仅下降0.61%，验证了其高效和实用性。

Conclusion: SCALE框架能够在显著减少token消耗的前提下，维持与现有方法几乎相当的性能，平均性能损失仅0.61%。

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [8] [TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech](https://arxiv.org/abs/2601.11178)
*Girish A. Koushik,Helen Treharne,Diptesh Kanojia*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as "black boxes" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.

</details>


### [9] [XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making](https://arxiv.org/abs/2601.11286)
*Weihong Qi,Fan Huang,Rasika Muralidharan,Jisun An,Haewoon Kwak*

Main category: cs.AI

TL;DR: XChoice框架能以可解释方式量化AI与人类在复杂决策中的机制性对齐，揭示不同人群与活动下失配点，并支持有针对性的改善。


<details>
  <summary>Details</summary>
Motivation: 以往用于衡量AI与人类决策一致性的指标（如accuracy、F1）仅关注结果的一致性，无法揭示机制层面的对齐细节。研究动机是开发更具解释性的评估框架，以深入理解AI和人类在受约束决策中的对齐机制。

Method: 提出XChoice框架，通过拟合机制化决策模型于人类数据与大模型生成的决策，提取可解释参数并对比这些参数向量，从而量化两者在决策因素重要性、约束敏感性及权衡偏好等方面的对齐。

Result: 在美国时间使用调查数据（ATUS）上测试表明，不同模型、活动、子群体间存在显著且异质化的对齐水平，且在黑人和已婚群体中发现明显失配。通过不变性分析验证了方法的鲁棒性，同时用RAG干预评估了定向缓解效果。

Conclusion: XChoice能提供机制层面对齐的诊断和解释，有助于针对性地改进AI模型，而不仅仅停留在结果统计的一致性层面。

Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.

</details>


### [10] [Hyperparameter Optimization of Constraint Programming Solvers](https://arxiv.org/abs/2601.11389)
*Hedieh Haddad,Thibault Falque,Pierre Talbot,Pascal Bouvry*

Main category: cs.AI

TL;DR: 论文提出了一个自动化超参数优化方法（probe and solve算法），能在有限时间内帮助约束规划求解器自动选择更优超参数配置，从而提高求解性能。


<details>
  <summary>Details</summary>
Motivation: 约束规划求解器性能极依赖其超参数配置，而手动调优复杂且耗时。迫切需要自动化、无需人工经验的调参方法，尤其适用于时间资源有限的实际应用场景。

Method: 该方法分为探测（probe）和求解（solve）两个阶段。先利用可配置的超参数优化算法（如贝叶斯优化与Hamming距离搜索）在限定时间内进行参数探索，再用找到的最优配置，在剩余时间内进行实际问题求解。

Result: 实验显示，贝叶斯优化在ACE求解器上25.4%的实例获得更优解，57.9%与默认持平；在Choco上38.6%实例表现更佳，且整体优于Hamming距离搜索。证明了基于模型的探索策略在求解器超参数优化中的有效性。

Conclusion: probe and solve算法通过在两个主流求解器（ACE和Choco）上的实验，显著提升了部分实例的解质量，并表现出对默认配置和传统搜素策略的明显优势，表明其是一种实用且通用的求解器调优方法。

Abstract: The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.
  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.
  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.

</details>


### [11] [Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs](https://arxiv.org/abs/2601.11468)
*Alessandro Padella,Massimiliano de Leoni,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文扩展了基于大型语言模型（LLM）的预测过程监控框架，不仅预测总耗时，还综合评估其在多种关键绩效指标（KPI）上的表现。实证结果显示，在数据稀缺（仅100条轨迹）情况下，LLM表现优于基线方法，并利用模型自身的知识与训练数据相关性进行推理。研究还发现LLM具备更复杂的推理能力，而非简单套用传统预测方法。


<details>
  <summary>Details</summary>
Motivation: 现有预测过程监控方法逐渐采用机器学习与深度学习，但对其泛化能力、语义理解和推理机制的系统评估不足。本文旨在探究LLM能否通过更复杂的推理和语义利用，在数据稀缺情况下提升多维流程预测准确性。

Method: 通过扩展原有的LLM框架，不再仅限于总时间预测，新增对多种关键绩效指标（如活动发生预测）的评估。实证评估基于三个不同的事件日志，并在总时间和活动发生预测两项KPI上与现有基线方法进行对比分析，同时深入研究LLM推理机制。

Result: 实验显示，在仅有100条轨迹的数据稀缺场景下，LLM在总时间和活动发生预测两个关键绩效指标上明显优于主流基线方法，并能有效利用训练数据的内在结构和模型知识，展现出复杂的推理过程。

Conclusion: LLM在数据量有限的流程监控任务中，能够高效利用内在知识和训练数据关联，优于传统基线方法，并具备更高级推理能力，为预测过程监控领域提供了更具泛化性与智能性的方案。

Abstract: Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.

</details>


### [12] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 提出LEG混合框架，用大语言模型将专家意见自然语言转化进优化过程，实现卫生设施升级优先排序的高覆盖率和专家指导融合，在埃塞俄比亚实证效果良好。


<details>
  <summary>Details</summary>
Motivation: 资源有限情况下，如何在农村卫生服务升级中兼顾理论最优和专家/利益相关方多元偏好，提高覆盖率与决策的可行性。

Method: 混合优化框架LEG，结合大语言模型与扩展贪心算法，将专家知识与理论优化结合，用于卫生站升级的优先级排序。

Result: 在埃塞俄比亚三区真实数据实验显示该框架能有效融合专家指导和优化保障，提高服务覆盖率，并且推进更公平、数据驱动的卫生系统规划。

Conclusion: 该方法兼顾专家引导与理论优化，为卫生系统规划提供具备公平性和实际可操作性的技术支撑，有较好推广潜力。

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [13] [Can Small Agent Collaboration Beat a Single Big LLM?](https://arxiv.org/abs/2601.11327)
*Agata Żywot,Xinyi Chen,Maarten de Rijke*

Main category: cs.MA

TL;DR: 小模型配合工具增强可在GAIA基准超越未增强大模型，工具增强是提升的主要来源，过度推理可能反而降低表现。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为大规模模型性能更好，本研究探索小模型通过智能策略（工具增强与推理层级控制）弥补规模劣势，为实际部署成本和灵活性提供理论依据。

Method: 作者采用4B-32B的Qwen3模型，在改进的Agentic-Reasoning框架下，通过控制变量对比模型规模、推理模式（无推理、仅规划、完全推理）、工具调用三大因素的影响，并在GAIA基准上进行系统实验评估。

Result: 该研究发现，通过工具增强，较小规模的模型（例如4B参数的Qwen3）能够在GAIA基准测试上超越未增强大型模型（如32B参数的模型）。工具增强（包括搜索、编程、思维导图）所带来的性能提升最稳定、最显著，而显式推理的效果则取决于具体配置和任务难度。部分推理（仅规划器模式）有利于任务分解与约束追踪，但“完全自由”推理反而可能导致表现下降，因为它会扰乱工具调用流程，出现跳过验证、过度调用、任务无法完成及输出格式混乱等问题。

Conclusion: 工具增强对于小模型尤其关键，使其性能大幅提升，甚至超过更大模型。推理策略需谨慎选择，过于复杂的显式推理可能损害系统整体表现。

Abstract: This report studies whether small, tool-augmented agents can match or outperform larger monolithic models on the GAIA benchmark. Using Qwen3 models (4B-32B) within an adapted Agentic-Reasoning framework, we isolate the effects of model scale, explicit thinking (no thinking, planner-only, or full), and tool use (search, code, mind-map). Tool augmentation provides the largest and most consistent gains. Using tools, 4B models can outperform 32B models without tool access on GAIA in our experimental setup. In contrast, explicit thinking is highly configuration- and difficulty-dependent: planner-only thinking can improve decomposition and constraint tracking, while unrestricted full thinking often degrades performance by destabilizing tool orchestration, leading to skipped verification steps, excessive tool calls, non-termination, and output-format drift.

</details>


### [14] [EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting](https://arxiv.org/abs/2601.05487)
*Huanxiang Lin,Qianyue Wang,Jinwu Hu,Bailin Chen,Qing Du,Mingkui Tan*

Main category: cs.MA

TL;DR: EvidFuse通过多智能体协作，实现写作时灵活插入高质量图表，有效提升报告的表达力，被验证为优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的系统在生成数据驱动报告时，文本与图表的生成往往分阶段进行，导致图表与文本不一致及见解僵化，影响分析深度和灵活性。本文旨在解决这一问题，实现更紧密的文本-图表协同生成。

Method: 提出了EvidFuse，一个无需训练的多智能体框架。该框架通过两个组件协作：Data-Augmented Analysis Agent负责探索性数据分析并访问原始表格，Real-Time Evidence Construction Writer负责规划和撰写报告，并在需要时请求细致分析，实现写作时的文本与图表交替生成。

Result: EvidFuse在图表质量、图表与文本一致性以及报告级有用性方面，均在LLM自动评价和人工评测中取得最高分。

Conclusion: EvidFuse能够显著提升数据驱动报告的文本-图表一致性与分析深度，为报告生成设定了新的标准。

Abstract: Data-driven reports communicate decision-relevant insights by tightly interleaving narrative text with charts grounded in underlying tables. However, current LLM-based systems typically generate narratives and visualizations in staged pipelines, following either a text-first-graph-second or a graph-first-text-second paradigm. These designs often lead to chart-text inconsistency and insight freezing, where the intermediate evidence space becomes fixed and the model can no longer retrieve or construct new visual evidence as the narrative evolves, resulting in shallow and predefined analysis. To address the limitations, we propose \textbf{EvidFuse}, a training-free multi-agent framework that enables writing-time text-chart interleaved generation for data-driven reports. EvidFuse decouples visualization analysis from long-form drafting via two collaborating components: a \textbf{Data-Augmented Analysis Agent}, equipped with Exploratory Data Analysis (EDA)-derived knowledge and access to raw tables, and a \textbf{Real-Time Evidence Construction Writer} that plans an outline and drafts the report while intermittently issuing fine-grained analysis requests. This design allows visual evidence to be constructed and incorporated exactly when the narrative requires it, directly constraining subsequent claims and enabling on-demand expansion of the evidence space. Experiments demonstrate that EvidFuse attains the top rank in both LLM-as-a-judge and human evaluations on chart quality, chart-text alignment, and report-level usefulness.

</details>
